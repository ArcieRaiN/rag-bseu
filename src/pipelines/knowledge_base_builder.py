from pathlib import Path
from typing import List, Optional
import json

from src.core.models import Chunk
from src.ingestion.pdf_chunker import PDFChunker
from src.enrichers.client import OllamaClient, OllamaConfig
from src.enrichers.enrichers import LLMEnricher
from src.utils.post_processor import EnrichmentPostProcessor
from src.vectorstore.vectorizer import SentenceVectorizer
from src.vectorstore.faiss_store import FAISSStore


class KnowledgeBaseBuilder:
    """
    Pipeline Ð´Ð»Ñ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ñ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ (PDF â†’ Chunk â†’ LLM â†’ FAISS).

    Ð’ÑÐµ ÑˆÐ°Ð³Ð¸ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð² Ð¾Ð´Ð½Ð¾Ð¼ Ñ„Ð°ÑÐ°Ð´Ðµ:
    1. Ð§Ð°Ð½ÐºÐ¸Ð½Ð³ PDF Ñ‡ÐµÑ€ÐµÐ· PDFChunker
    2. Enrichment Ñ‡ÐµÑ€ÐµÐ· LLMEnricher
    3. Post-processing Ñ‡ÐµÑ€ÐµÐ· EnrichmentPostProcessor
    4. ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ FAISS Ð¸Ð½Ð´ÐµÐºÑÐ° Ñ‡ÐµÑ€ÐµÐ· FAISSStore
    5. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…
    """

    def __init__(
        self,
        documents_dir: Path,
        output_dir: Path,
        llm_model: str = "llama3-chatqa:latest",
        vector_dim: int = 256,
        llm_client: Optional[OllamaClient] = None,
    ):
        self.documents_dir = documents_dir
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # LLM
        if llm_client is None:
            ollama_config = OllamaConfig(model=llm_model)
            self.llm_client = OllamaClient(config=ollama_config)
        else:
            self.llm_client = llm_client

        # ÐœÐ¾Ð´ÑƒÐ»Ð¸
        self.pdf_chunker = PDFChunker()
        self.llm_enricher = LLMEnricher(llm_client=self.llm_client)
        self.post_processor = EnrichmentPostProcessor()
        self.vectorizer = SentenceVectorizer(dimension=vector_dim)
        self.faiss_indexer = FAISSStore(vectorizer=self.vectorizer)

    def build(self) -> None:
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´: ÑÑ‚Ñ€Ð¾Ð¸Ð¼ Ð±Ð°Ð·Ñƒ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¸Ð· Ð²ÑÐµÑ… PDF Ð² Ð¿Ð°Ð¿ÐºÐµ."""
        all_chunks: List[Chunk] = []
        chunk_id_counter = 0

        pdf_files = sorted(self.documents_dir.glob("*.pdf"))
        if not pdf_files:
            print(f"âŒ ÐÐµÑ‚ PDF-Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð² {self.documents_dir}")
            return

        print(f"ðŸ“„ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ PDF-Ñ„Ð°Ð¹Ð»Ð¾Ð²: {len(pdf_files)}")
        print("ðŸ”§ Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ Ð±Ð°Ð·Ñƒ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· LlamaIndex + Ollama enrichment...")

        for pdf_path in pdf_files:
            # 1. Ð§Ð°Ð½ÐºÐ¸Ð½Ð³ PDF
            raw_chunks = self.pdf_chunker.chunk_pdf(pdf_path)

            # ÐÐ°Ð·Ð½Ð°Ñ‡Ð°ÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ id
            for ch in raw_chunks:
                ch.id = f"{pdf_path.name}::page{ch.page}::chunk{chunk_id_counter}"
                chunk_id_counter += 1

            # 2. LLM enrichment
            enriched_chunks = self.llm_enricher.enrich_chunks(
                pdf_path.name,
                raw_chunks,
                show_progress=True,
            )

            # 3. Post-processing
            processed_chunks = [self.post_processor.process_chunk(ch) for ch in enriched_chunks]

            all_chunks.extend(processed_chunks)

        # 4. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ data.json
        self._save_data_json(all_chunks)

        # 5. Ð¡Ñ‚Ñ€Ð¾Ð¸Ð¼ FAISS Ð¸Ð½Ð´ÐµÐºÑ
        index_path = self.output_dir / "index.faiss"
        self.faiss_indexer.build_and_save(all_chunks, index_path)

        # 6. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
        self._save_metadata(all_chunks)

        print("âœ… Ð‘Ð°Ð·Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð°!")
        print(f"ðŸ“ Ð˜Ð½Ð´ÐµÐºÑ Ð¸ data.json ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: {self.output_dir}")

    def _save_data_json(self, chunks: List[Chunk]) -> None:
        data_path = self.output_dir / "data.json"
        with open(data_path, "w", encoding="utf-8") as f:
            json.dump(
                [
                    {
                        "id": ch.id,
                        "context": ch.context,
                        "text": ch.text,
                        "source": ch.source,
                        "page": ch.page,
                        "geo": ch.geo,
                        "metrics": ch.metrics,
                        "years": ch.years,
                        "time_granularity": ch.time_granularity,
                        "oked": ch.oked,
                    }
                    for ch in chunks
                ],
                f,
                ensure_ascii=False,
                indent=2,
            )

    def _save_metadata(self, chunks: List[Chunk]) -> None:
        meta_path = self.output_dir / "metadata.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "vectorizer": type(self.vectorizer).__name__,
                    "dimension": self.vectorizer.dimension,
                    "chunks": len(chunks),
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
