from pathlib import Path
from typing import List, Optional
import json

from src.core.models import Chunk
from src.ingestion.pdf_chunker import PDFChunker
from src.enrichers.client import OllamaClient, OllamaConfig
from src.enrichers.enrichers import LLMEnricher
from src.utils.post_processor import EnrichmentPostProcessor
from src.vectorstore.vectorizer import SentenceVectorizer
from src.vectorstore.faiss_store import FAISSStore


class KnowledgeBaseBuilder:
    """
    Pipeline Ð´Ð»Ñ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ñ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹ (PDF â†’ Chunk â†’ LLM â†’ FAISS).

    Ð’ÑÐµ ÑˆÐ°Ð³Ð¸ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð² Ð¾Ð´Ð½Ð¾Ð¼ Ñ„Ð°ÑÐ°Ð´Ðµ:
    1. Ð§Ð°Ð½ÐºÐ¸Ð½Ð³ PDF Ñ‡ÐµÑ€ÐµÐ· PDFChunker
    2. Enrichment Ñ‡ÐµÑ€ÐµÐ· LLMEnricher
    3. Post-processing Ñ‡ÐµÑ€ÐµÐ· EnrichmentPostProcessor
    4. ÐŸÐ¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ FAISS Ð¸Ð½Ð´ÐµÐºÑÐ° Ñ‡ÐµÑ€ÐµÐ· FAISSStore
    5. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…
    """

    def __init__(
        self,
        documents_dir: Path,
        output_dir: Path,
        llm_model: str = "llama3-chatqa:latest",
        vector_dim: int = 256,
        llm_client: Optional[OllamaClient] = None,
    ):
        self.documents_dir = documents_dir
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # LLM
        if llm_client is None:
            ollama_config = OllamaConfig(model=llm_model)
            self.llm_client = OllamaClient(config=ollama_config)
        else:
            self.llm_client = llm_client

        # ÐœÐ¾Ð´ÑƒÐ»Ð¸
        self.pdf_chunker = PDFChunker()
        self.llm_enricher = LLMEnricher(llm_client=self.llm_client)
        self.post_processor = EnrichmentPostProcessor()
        self.vectorizer = SentenceVectorizer(dimension=vector_dim)
        self.faiss_indexer = FAISSStore(vectorizer=self.vectorizer)

    def _load_data_json(self) -> List[dict]:
        data_path = self.output_dir / "data.json"
        if not data_path.exists():
            return []
        with open(data_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def build(self) -> None:
        chunk_id_counter = 0
        index_path = self.output_dir / "index.faiss"

        # ÐŸÐ¾Ð´Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
        all_chunks = self._load_data_json()
        if index_path.exists():
            self.faiss_indexer.load(index_path)

        pdf_files = sorted(self.documents_dir.glob("*.pdf"))
        for pdf_path in pdf_files:
            pdf_name = pdf_path.name
            # 1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÐµÑÑ‚ÑŒ Ð»Ð¸ PDF Ð² Ð±Ð°Ð·Ðµ
            existing_chunks = [Chunk(**ch) for ch in all_chunks if ch["source"] == pdf_name]
            if existing_chunks:
                # ÑƒÐ´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ñ‡Ð°Ð½ÐºÐ¸ Ð¸Ð· FAISS Ð¸ data.json
                self.faiss_indexer.delete_chunks_by_pdf(pdf_name, existing_chunks)
                all_chunks = [ch for ch in all_chunks if ch["source"] != pdf_name]

            # 2. Ð§Ð°Ð½ÐºÐ¸Ð½Ð³
            raw_chunks = self.pdf_chunker.chunk_pdf(pdf_path)
            for ch in raw_chunks:
                ch.id = f"{pdf_name}::page{ch.page}::chunk{chunk_id_counter}"
                chunk_id_counter += 1

            # 3. LLM enrichment
            enriched_chunks = self.llm_enricher.enrich_chunks(pdf_name, raw_chunks, show_progress=True)

            # 4. Post-processing
            processed_chunks = [self.post_processor.process_chunk(ch) for ch in enriched_chunks]

            # 5. Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ Ñ‡Ð°Ð½ÐºÐ¸ Ð² FAISS Ð¸ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
            self.faiss_indexer.add_chunks(processed_chunks)
            all_chunks.extend([{
                "id": ch.id,
                "context": ch.context,
                "text": ch.text,
                "source": ch.source,
                "page": ch.page,
                "geo": ch.geo,
                "metrics": ch.metrics,
                "years": ch.years,
                "time_granularity": ch.time_granularity,
                "oked": ch.oked,
            } for ch in processed_chunks])

            # 6. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ñ€Ð¾Ð¼ÐµÐ¶ÑƒÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
            self._save_data_json([Chunk(**ch) for ch in all_chunks])
            self.faiss_indexer.save(index_path)
            print(f"âœ… ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½ PDF {pdf_name} Ð² FAISS Ð¸ data.json")

        # 7. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
        self._save_metadata([Chunk(**ch) for ch in all_chunks])
        print("ðŸŽ‰ Ð‘Ð°Ð·Ð° Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ Ð¿Ð¾ÑÑ‚Ñ€Ð¾ÐµÐ½Ð°!")

    def _save_data_json(self, chunks: List[Chunk]) -> None:
        data_path = self.output_dir / "data.json"
        with open(data_path, "w", encoding="utf-8") as f:
            json.dump(
                [
                    {
                        "id": ch.id,
                        "context": ch.context,
                        "text": ch.text,
                        "source": ch.source,
                        "page": ch.page,
                        "geo": ch.geo,
                        "metrics": ch.metrics,
                        "years": ch.years,
                        "time_granularity": ch.time_granularity,
                        "oked": ch.oked,
                    }
                    for ch in chunks
                ],
                f,
                ensure_ascii=False,
                indent=2,
            )

    def _save_metadata(self, chunks: List[Chunk]) -> None:
        meta_path = self.output_dir / "metadata.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "vectorizer": type(self.vectorizer).__name__,
                    "dimension": self.vectorizer.dimension,
                    "chunks": len(chunks),
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
