from __future__ import annotations

"""
–ú–æ–¥—É–ª—å –¥–ª—è LLM-–æ–±–æ–≥–∞—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer.

–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
- –ë–∞—Ç—á–µ–≤–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ Ollama
- Rolling context buffer (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞–Ω–∫–æ–≤ –≤–º–µ—Å—Ç–æ –≤—Å–µ—Ö)
- –ü–∞—Ä—Å–∏–Ω–≥ JSON-–æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç LLM
- –í–∞–ª–∏–¥–∞—Ü–∏—é –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
"""

from typing import List, Dict, Any, Optional, Deque
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os
import time
import codecs

from src.main.models import Chunk
from src.main.ollama_client import OllamaClient
from src.logs.logger import get_logger
from src.prepare_db.json_validator import ChunkValidator
from src.prepare_db.chunk_filter import ChunkFilter, ChunkType
from src.prepare_db.post_processor import EnrichmentPostProcessor


class RollingContextBuffer:
    """
    Rolling context buffer –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —á–∞–Ω–∫–æ–≤.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–º–µ—Å—Ç–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
    –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """

    def __init__(self, max_size: int = 10):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—É—Ñ–µ—Ä–∞.
        
        Args:
            max_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –≤ –±—É—Ñ–µ—Ä–µ
        """
        self._buffer: Deque[Chunk] = deque(maxlen=max_size)
        self.max_size = max_size

    def add(self, chunk: Chunk) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫ –≤ –±—É—Ñ–µ—Ä (—Å—Ç–∞—Ä—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è—é—Ç—Å—è)."""
        self._buffer.append(chunk)

    def add_batch(self, chunks: List[Chunk]) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –±–∞—Ç—á —á–∞–Ω–∫–æ–≤ –≤ –±—É—Ñ–µ—Ä."""
        for chunk in chunks:
            self._buffer.append(chunk)

    def get_context(self, num_chunks: int = 2) -> List[Chunk]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        Args:
            num_chunks: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —á–∞–Ω–∫–æ–≤
        """
        return list(self._buffer)[-num_chunks:]

    def clear(self) -> None:
        """–û—á–∏—â–∞–µ—Ç –±—É—Ñ–µ—Ä."""
        self._buffer.clear()

    def __len__(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞."""
        return len(self._buffer)


class LLMEnricher:
    """
    –ö–ª–∞—Å—Å –¥–ª—è LLM-–æ–±–æ–≥–∞—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞—Ç—á–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞—Ç—á–µ–π.
    """

    def __init__(
        self,
        llm_client: OllamaClient,
        batch_size: int = 5,
        batch_concurrency: int = 1,
        context_buffer_size: int = 10,
        reset_interval: int = 50,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–æ–≥–∞—Ç–∏—Ç–µ–ª—è.
        
        Args:
            llm_client: –ö–ª–∏–µ–Ω—Ç Ollama –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –≤ –æ–¥–Ω–æ–º LLM-–∑–∞–ø—Ä–æ—Å–µ)
            batch_concurrency: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –±–∞—Ç—á–µ–π
            context_buffer_size: –†–∞–∑–º–µ—Ä rolling context buffer
            reset_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–±—Ä–æ—Å–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ (–≤ —á–∞–Ω–∫–∞—Ö)
        """
        self._llm = llm_client
        self._batch_size = batch_size
        self._batch_concurrency = batch_concurrency
        self._reset_interval = reset_interval
        self._logger = get_logger()
        self._validator = ChunkValidator()
        self._chunk_filter = ChunkFilter(skip_first_pages=3)
        self._post_processor = EnrichmentPostProcessor()
        
        # Rolling context buffer
        self._context_buffer = RollingContextBuffer(max_size=context_buffer_size)
        
        # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–±—Ä–æ—Å–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self._chunks_since_reset = 0

    def enrich_chunks(
        self,
        pdf_name: str,
        chunks: List[Chunk],
        skip_first_pages: int = 3,
    ) -> List[Chunk]:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer.
        
        Args:
            pdf_name: –ò–º—è PDF —Ñ–∞–π–ª–∞
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            skip_first_pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ (—Ç–æ–ª—å–∫–æ –æ–±–ª–æ–∂–∫–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        if not chunks:
            return []

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ChunkFilter –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–∞–Ω–∫–æ–≤
        data_chunks, metadata_chunks, skip_chunks = self._chunk_filter.filter_chunks(chunks)
        
        # –î–ª—è metadata —á–∞–Ω–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ
        for chunk in metadata_chunks:
            chunk.context = chunk.text[:200] if chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
        
        # –î–ª—è skip —á–∞–Ω–∫–æ–≤ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        for chunk in skip_chunks:
            chunk.context = chunk.text[:200] if chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
        
        chunks_to_process = data_chunks
        chunks_to_skip = metadata_chunks + skip_chunks

        if not chunks_to_process:
            return chunks

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ env (—Å fallback –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞)
        batch_size = int(os.getenv("RAG_ENRICH_BATCH_SIZE", str(self._batch_size)))
        batch_concurrency = int(os.getenv("RAG_ENRICH_CONCURRENCY", str(self._batch_concurrency)))
        batch_size = max(1, batch_size)
        batch_concurrency = max(1, min(batch_concurrency, 8))

        print(
            f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(chunks_to_process)} —á–∞–Ω–∫–æ–≤ "
            f"(–ø—Ä–æ–ø—É—â–µ–Ω–æ {len(chunks_to_skip)} —Å–ª—É–∂–µ–±–Ω—ã—Ö —á–∞–Ω–∫–æ–≤)"
        )
        print(
            f"   –†–µ–∂–∏–º: 1 —á–∞–Ω–∫ = 1 –∑–∞–ø—Ä–æ—Å, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å={batch_concurrency * batch_size}"
        )

        all_enriched_chunks: List[Chunk] = chunks_to_skip.copy()

        # –ü–∞—Ä–∞–ª–ª–µ–ª–∏–º —Ç–æ–ª—å–∫–æ –±–∞—Ç—á–∏ (–∞ –Ω–µ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫)
        start_time = time.time()

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: 1 —á–∞–Ω–∫ = 1 –∑–∞–ø—Ä–æ—Å
        # –ë–∞—Ç—á–∏–Ω–≥ –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ (–Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
        with ThreadPoolExecutor(max_workers=min(batch_concurrency * batch_size, len(chunks_to_process))) as executor:
            futures = []
            for chunk in chunks_to_process:
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ rolling buffer (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —á–∞–Ω–∫–∞)
                previous_chunks = self._context_buffer.get_context(num_chunks=2)
                
                fut = executor.submit(
                    self._enrich_single_chunk,
                    pdf_name,
                    chunk,
                    previous_chunks,
                )
                fut._rag_submit_ts = time.time()  # type: ignore[attr-defined]
                futures.append((fut, chunk))

            completed_chunks = 0
            total_chunks = len(chunks_to_process)

            for fut, original_chunk in futures:
                try:
                    enriched_chunk = fut.result()
                    submit_ts = getattr(fut, "_rag_submit_ts", None)
                    chunk_time = (time.time() - submit_ts) if submit_ts else 0.0
                    
                    if enriched_chunk:
                        all_enriched_chunks.append(enriched_chunk)
                        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —á–∞–Ω–∫ –≤ rolling buffer
                        self._context_buffer.add(enriched_chunk)
                    else:
                        # Fallback: –¥–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –±–µ–∑ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
                        if not original_chunk.context:
                            original_chunk.context = original_chunk.text[:200] if original_chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                        all_enriched_chunks.append(original_chunk)
                        self._context_buffer.add(original_chunk)
                    
                    completed_chunks += 1
                    self._chunks_since_reset += 1

                    # –°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–∞–∂–¥—ã–µ N —á–∞–Ω–∫–æ–≤
                    if self._chunks_since_reset >= self._reset_interval:
                        print(
                            f"   üîÑ –°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ {completed_chunks} —á–∞–Ω–∫–æ–≤..."
                        )
                        self._llm.reset_context()
                        self._chunks_since_reset = 0
                        time.sleep(0.5)

                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (–∫–∞–∂–¥—ã–µ 10 —á–∞–Ω–∫–æ–≤ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ)
                    if completed_chunks % 10 == 0 or completed_chunks == total_chunks:
                        elapsed = time.time() - start_time
                        rate = completed_chunks / elapsed * 3600 if elapsed > 0 else 0
                        print(
                            f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: {completed_chunks}/{total_chunks} —á–∞–Ω–∫–æ–≤ | "
                            f"–°–∫–æ—Ä–æ—Å—Ç—å: {rate:.0f} —á–∞–Ω–∫–æ–≤/—á–∞—Å"
                        )

                except Exception as e:
                    submit_ts = getattr(fut, "_rag_submit_ts", None)
                    chunk_time = (time.time() - submit_ts) if submit_ts else 0.0
                    print(
                        f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞–Ω–∫–∞ {original_chunk.id} "
                        f"(–≤—Ä–µ–º—è: {chunk_time:.1f}—Å): {e}"
                    )
                    import traceback
                    traceback.print_exc()
                    # Fallback: –¥–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –±–µ–∑ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
                    if not original_chunk.context:
                        original_chunk.context = original_chunk.text[:200] if original_chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                    all_enriched_chunks.append(original_chunk)
                    completed_chunks += 1
                    self._chunks_since_reset += 1

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
        chunk_order = {ch.id: i for i, ch in enumerate(chunks)}
        all_enriched_chunks.sort(key=lambda ch: chunk_order.get(ch.id, 999999))

        return all_enriched_chunks

    def _enrich_single_chunk(
        self,
        pdf_name: str,
        chunk: Chunk,
        previous_chunks: List[Chunk] | None = None,
    ) -> Optional[Chunk]:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
        
        Args:
            pdf_name: –ò–º—è PDF —Ñ–∞–π–ª–∞
            chunk: –ß–∞–Ω–∫ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            previous_chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —á–∞–Ω–∫ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        previous_context = None
        if previous_chunks:
            prev_texts = []
            for prev_ch in previous_chunks[-2:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2 —á–∞–Ω–∫–∞
                if prev_ch.text:
                    prev_texts.append(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {prev_ch.page}: {prev_ch.text[:200]}")
            if prev_texts:
                previous_context = "\n".join(prev_texts)

        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π system prompt –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
        system_prompt = self._build_system_prompt_single()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
        prompt = self._build_prompt_single(pdf_name, chunk, previous_context)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        keep_alive = os.getenv("RAG_OLLAMA_KEEP_ALIVE", "5m")
        req_options = {
            "temperature": 0,
            "top_p": 1,
            "num_predict": 300,  # –û–¥–∏–Ω –æ–±—ä–µ–∫—Ç - –º–µ–Ω—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
        }

        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        ollama_config = {
            "model": getattr(getattr(self._llm, "config", None), "model", None),
            "base_url": getattr(getattr(self._llm, "config", None), "base_url", None),
            "timeout": getattr(getattr(self._llm, "config", None), "timeout", None),
            "format": "json",
            "options": req_options,
        }

        self._logger.log_llm_enrichment(
            event="request",
            pdf_name=pdf_name,
            chunks_count=1,
            chunk_ids=[chunk.id],
            pages=[chunk.page],
            system_prompt=system_prompt,
            prompt=prompt,
            ollama_config=ollama_config,
        )

        # –í—ã–∑—ã–≤–∞–µ–º LLM —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º retry (–æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –≤—Å–µ–≥–¥–∞ –≤–∞–ª–∏–¥–µ–Ω)
        max_retries = 2
        enriched_data = None
        raw_response = ""

        for attempt in range(max_retries):
            raw_response = self._llm.generate(
                prompt,
                system_prompt=system_prompt,
                format="json",
                keep_alive=keep_alive,
                options=req_options,
            )

            # –ü–∞—Ä—Å–∏–º JSON-–æ—Ç–≤–µ—Ç (–æ–∂–∏–¥–∞–µ–º –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç)
            enriched_data = self._parse_llm_single_enrichment(raw_response, chunk.id)
            
            if enriched_data:
                # –í–°–ï–ì–î–ê –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º chunk_id –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
                # LLM –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –∏—Å—Ç–∏–Ω—ã –¥–ª—è ID - —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
                enriched_data["chunk_id"] = chunk.id
                # –ï—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω - –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞
                break
            elif attempt < max_retries - 1:
                print(
                    f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç –¥–ª—è {chunk.id} "
                    f"(–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})"
                )
                time.sleep(0.5)

        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        parsed_successfully = enriched_data is not None
        self._logger.log_llm_enrichment(
            event="response",
            pdf_name=pdf_name,
            chunks_count=1,
            chunk_ids=[chunk.id],
            raw_response=raw_response,
            parsed_items=1 if parsed_successfully else 0,
            parsed_with_chunk_id=1 if (parsed_successfully and enriched_data and enriched_data.get("chunk_id")) else 0,
        )

        if not enriched_data:
            return None

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∫ —á–∞–Ω–∫—É
        enriched_chunk = self._apply_enrichment_data_single(chunk, enriched_data)
        
        # Post-processing: –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ LLM
        enriched_chunk = self._post_processor.process_chunk(enriched_chunk)

        return enriched_chunk

    def _build_system_prompt_single(self) -> str:
        """
        –°—Ç—Ä–æ–∏—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π system prompt –¥–ª—è –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞.
        
        Returns:
            System prompt –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞
        """
        return (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
            "1. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç {}, –Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å '{' –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏–π—Å—è '}'\n"
            "2. –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–π –º–∞—Å—Å–∏–≤ [] - —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç {}\n"
            "3. chunk_id –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ–±–∞–π—Ç–Ω–æ, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n"
            "4. –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ç–µ–∫—Å—Ç –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ JSON\n"
            "5. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π markdown code blocks (```json)\n"
            "6. –ù–ï –¥–æ–±–∞–≤–ª—è–π –ø—Ä–µ—Ñ–∏–∫—Å—ã —Ç–∏–ø–∞ 'ID:' –∫ chunk_id - –∏—Å–ø–æ–ª—å–∑—É–π –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –µ—Å—Ç—å\n\n"
            "JSON Schema –¥–ª—è –æ–±—ä–µ–∫—Ç–∞:\n"
            "{\n"
            '  "chunk_id": "string (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)",\n'
            '  "context": "string (–º–∞–∫—Å 256 —Å–∏–º–≤–æ–ª–æ–≤, —Ä—É—Å—Å–∫–∏–π)",\n'
            '  "geo": "string | null",\n'
            '  "metrics": ["string"] | null (–º–∞–∫—Å 5, —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä),\n'
            '  "years": [int] | null (–º–∞–∫—Å 5, —Ç–æ–ª—å–∫–æ –≥–æ–¥—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞),\n'
            '  "time_granularity": "year" | "quarter" | "month" | "day" | null,\n'
            '  "oked": "string | null"\n'
            "}\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è metrics:\n"
            "- –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '—É–¥–æ–π –º–æ–ª–æ–∫–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏')\n"
            "- –ù–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ù–ï –æ–±—ä–µ–∫—Ç—ã –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, –ù–ï –∑–∞–≥–æ–ª–æ–≤–∫–∏\n"
            "- –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä\n"
            "- –ú–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è years:\n"
            "- –¢–æ–ª—å–∫–æ –≥–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ\n"
            "- –ù–ï –¥–æ–±–∞–≤–ª—è–π –≥–æ–¥—ã, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ (–∏—Å–ø–æ–ª—å–∑—É–π null)\n"
            "- –ù–ï –¥–æ–±–∞–≤–ª—è–π –≥–æ–¥—ã –¥–ª—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è\n"
            "- –ú–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n"
        )

    def _build_system_prompt(
        self,
        chunks_count: int,
        format_error: bool = False,
        expected_count: Optional[int] = None,
    ) -> str:
        """
        –°—Ç—Ä–æ–∏—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π system prompt —Å JSON Schema.
        
        Args:
            chunks_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –≤ –±–∞—Ç—á–µ
            format_error: True –µ—Å–ª–∏ –±—ã–ª–∞ –æ—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ø–æ–ø—ã—Ç–∫–µ
            expected_count: –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫)
        """
        base_prompt = (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:\n"
            "1. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON-–º–∞—Å—Å–∏–≤, –Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å '[' –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏–π—Å—è ']'\n"
            "2. –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–π –æ–±—ä–µ–∫—Ç {} - —Ç–æ–ª—å–∫–æ –º–∞—Å—Å–∏–≤ []\n"
            f"3. –í –º–∞—Å—Å–∏–≤–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –†–û–í–ù–û {chunks_count} –æ–±—ä–µ–∫—Ç–æ–≤\n"
            "4. chunk_id –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ–±–∞–π—Ç–Ω–æ)\n"
            "5. –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ç–µ–∫—Å—Ç –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ JSON\n"
            "6. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π markdown code blocks (```json)\n\n"
        )
        
        if format_error:
            base_prompt += (
                "‚ö†Ô∏è –û–®–ò–ë–ö–ê –§–û–†–ú–ê–¢–ê –í –ü–†–ï–î–´–î–£–©–ï–ô –ü–û–ü–´–¢–ö–ï!\n"
                "–¢—ã –≤–µ—Ä–Ω—É–ª –æ–±—ä–µ–∫—Ç {} –≤–º–µ—Å—Ç–æ –º–∞—Å—Å–∏–≤–∞ [] –∏–ª–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.\n"
                f"–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤–µ—Ä–Ω–∏ –º–∞—Å—Å–∏–≤ –∏–∑ {expected_count or chunks_count} –æ–±—ä–µ–∫—Ç–æ–≤.\n\n"
            )
        
        base_prompt += (
            "JSON Schema –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞:\n"
            "{\n"
            '  "chunk_id": "string (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)",\n'
            '  "context": "string (–º–∞–∫—Å 256 —Å–∏–º–≤–æ–ª–æ–≤, —Ä—É—Å—Å–∫–∏–π)",\n'
            '  "geo": "string | null",\n'
            '  "metrics": ["string"] | null (–º–∞–∫—Å 5, —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏),\n'
            '  "years": [int] | null (–º–∞–∫—Å 5, —Ç–æ–ª—å–∫–æ –≥–æ–¥—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞),\n'
            '  "time_granularity": "year" | "quarter" | "month" | "day" | null,\n'
            '  "oked": "string | null"\n'
            "}\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è metrics:\n"
            "- –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '—É–¥–æ–π –º–æ–ª–æ–∫–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏')\n"
            "- –ù–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ù–ï –æ–±—ä–µ–∫—Ç—ã –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, –ù–ï –∑–∞–≥–æ–ª–æ–≤–∫–∏\n"
            "- –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä\n"
            "- –ú–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è years:\n"
            "- –¢–æ–ª—å–∫–æ –≥–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ\n"
            "- –ù–ï –¥–æ–±–∞–≤–ª—è–π –≥–æ–¥—ã, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ (–∏—Å–ø–æ–ª—å–∑—É–π null)\n"
            "- –ù–ï –¥–æ–±–∞–≤–ª—è–π –≥–æ–¥—ã –¥–ª—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è\n"
            "- –ú–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n"
        )
        
        return base_prompt

    def _check_chunk_ids_match(
        self,
        enriched_items: List[Dict[str, Any]],
        expected_chunk_ids: List[str],
    ) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ chunk_id (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π).
        
        Args:
            enriched_items: –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç LLM
            expected_chunk_ids: –û–∂–∏–¥–∞–µ–º—ã–µ chunk_id
            
        Returns:
            True –µ—Å–ª–∏ –≤—Å–µ chunk_id —Å–æ–≤–ø–∞–¥–∞—é—Ç
        """
        if len(enriched_items) != len(expected_chunk_ids):
            return False
        
        for i, (item, expected_id) in enumerate(zip(enriched_items, expected_chunk_ids)):
            actual_id = item.get("chunk_id", "")
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            normalized_actual = ChunkFilter.normalize_chunk_id(actual_id)
            normalized_expected = ChunkFilter.normalize_chunk_id(expected_id)
            
            if normalized_actual != normalized_expected:
                return False
        
        return True

    def _fix_format_error(
        self,
        enriched_data: Any,
        expected_chunk_ids: List[str],
    ) -> List[Dict[str, Any]]:
        """
        –ü—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ (–æ–±—ä–µ–∫—Ç –≤–º–µ—Å—Ç–æ –º–∞—Å—Å–∏–≤–∞).
        
        Args:
            enriched_data: –î–∞–Ω–Ω—ã–µ –æ—Ç LLM (–≤–æ–∑–º–æ–∂–Ω–æ, –æ–±—ä–µ–∫—Ç –≤–º–µ—Å—Ç–æ –º–∞—Å—Å–∏–≤–∞)
            expected_chunk_ids: –û–∂–∏–¥–∞–µ–º—ã–µ chunk_id
            
        Returns:
            –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
        """
        # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –º–∞—Å—Å–∏–≤ –∏–∑ –Ω–µ–≥–æ
        if isinstance(enriched_data, dict):
            # –ò—â–µ–º –º–∞—Å—Å–∏–≤ –≤ –∫–ª—é—á–∞—Ö
            for key in ["chunks", "data", "results", "items", "array"]:
                if key in enriched_data and isinstance(enriched_data[key], list):
                    return enriched_data[key]
            
            # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç chunk_id, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –º–∞—Å—Å–∏–≤
            if "chunk_id" in enriched_data or any(
                key in enriched_data for key in ["context", "geo", "metrics", "years"]
            ):
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º chunk_id –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if "chunk_id" not in enriched_data and expected_chunk_ids:
                    enriched_data["chunk_id"] = expected_chunk_ids[0]
                return [enriched_data]
        
        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å–ø–∏—Å–æ–∫, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        if not isinstance(enriched_data, list):
            return []
        
        return enriched_data

    def _build_prompt_single(
        self,
        pdf_name: str,
        chunk: Chunk,
        previous_context: Optional[str],
    ) -> str:
        """
        –°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞.
        
        Args:
            pdf_name: –ò–º—è PDF —Ñ–∞–π–ª–∞
            chunk: –ß–∞–Ω–∫ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            previous_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            
        Returns:
            –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
        """
        context_section = ""
        if previous_context:
            context_section = (
                f"\n–ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ò–• –°–¢–†–ê–ù–ò–¶ "
                f"(–¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞):\n"
                f"{previous_context}\n\n"
            )

        return (
            f"–î–æ–∫—É–º–µ–Ω—Ç: {pdf_name}\n\n"
            f"{context_section}"
            f"–ß–∞–Ω–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n"
            f"chunk_id: {chunk.id}\n"
            f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {chunk.page}\n"
            f"–¢–µ–∫—Å—Ç: {(chunk.text or '')[:500]}\n\n"
            "–í–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç (–ù–ï –º–∞—Å—Å–∏–≤!). –§–æ—Ä–º–∞—Ç:\n"
            '{"chunk_id":"...","context":"...","geo":null,"metrics":null,'
            '"years":null,"time_granularity":null,"oked":null}\n\n'
            "–ü–æ–ª—è:\n"
            f"- chunk_id: —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ –≤—ã—à–µ: '{chunk.id}' (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤)\n"
            "- context: –∫—Ä–∞—Ç–∫–æ–µ, —Ç–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞–Ω–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ "
            "(1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —á–∞–Ω–∫–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n"
            "- geo: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞/–≥–æ—Ä–æ–¥–∞/–æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ null\n"
            "- metrics: —Å–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–º–∞–∫—Å–∏–º—É–º 5) –∏–ª–∏ null. "
            "–¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞, –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n"
            "- years: —Å–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ (–º–∞–∫—Å–∏–º—É–º 9) –∏–ª–∏ null. –¢–æ–ª—å–∫–æ –≥–æ–¥—ã, —è–≤–Ω–æ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ —Ç–µ–∫—Å—Ç–µ\n"
            "- time_granularity: 'year'/'quarter'/'month'/'day' –∏–ª–∏ null\n"
            "- oked: –∫–æ–¥ –û–ö–≠–î –∏–ª–∏ null\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
            "1. –í–µ—Ä–Ω–∏ –û–ë–™–ï–ö–¢ {}, –ù–ï –º–∞—Å—Å–∏–≤ []\n"
            "2. chunk_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–º –∂–µ –∫–∞–∫ –≤—ã—à–µ (–±–µ–∑ 'ID:', –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n"
            "3. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (\\u0412) - –ø–∏—à–∏ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é"
        )

    def _build_prompt(
        self,
        pdf_name: str,
        chunks_data: List[Dict[str, Any]],
        previous_context: Optional[str],
    ) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM (legacy –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)."""
        # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if len(chunks_data) == 1:
            chunk_data = chunks_data[0]
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —á–∞–Ω–∫ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞
            temp_chunk = Chunk(
                id=chunk_data["chunk_id"],
                context="",
                text=chunk_data["text"],
                source=pdf_name,
                page=chunk_data["page"],
            )
            return self._build_prompt_single(pdf_name, temp_chunk, previous_context)
        else:
            # Fallback –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–Ω–µ –¥–æ–ª–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è)
            return f"–û–±—Ä–∞–±–æ—Ç–∞–π {len(chunks_data)} —á–∞–Ω–∫–æ–≤..."

    def _apply_enrichment_data(
        self,
        chunks: List[Chunk],
        enriched_data: List[Dict[str, Any]],
    ) -> List[Chunk]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∫ —á–∞–Ω–∫–∞–º."""
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏
        valid_enriched_data = [
            item for item in enriched_data if isinstance(item, dict)
        ]

        # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å chunk_id -> enriched_data (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π)
        enriched_map: Dict[str, Dict[str, Any]] = {}
        for item in valid_enriched_data:
            chunk_id = item.get("chunk_id")
            if chunk_id:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º chunk_id –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                normalized_id = ChunkFilter.normalize_chunk_id(str(chunk_id))
                enriched_map[normalized_id] = item
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π chunk_id –≤ –¥–∞–Ω–Ω—ã—Ö
                item["chunk_id"] = normalized_id

        # –ï—Å–ª–∏ –Ω–µ –≤—Å–µ —á–∞–Ω–∫–∏ –Ω–∞–π–¥–µ–Ω—ã –ø–æ ID, –ø—ã—Ç–∞–µ–º—Å—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ –ø–æ—Ä—è–¥–∫—É
        if len(enriched_map) < len(chunks) and valid_enriched_data:
            if len(valid_enriched_data) == len(chunks):
                for i, ch in enumerate(chunks):
                    if ch.id not in enriched_map and i < len(valid_enriched_data):
                        enriched_map[ch.id] = valid_enriched_data[i]
                        enriched_map[ch.id]["chunk_id"] = ch.id
            elif len(valid_enriched_data) == 1 and len(chunks) > 1:
                first_chunk = chunks[0]
                if first_chunk.id not in enriched_map:
                    enriched_map[first_chunk.id] = valid_enriched_data[0].copy()
                    enriched_map[first_chunk.id]["chunk_id"] = first_chunk.id

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫ —á–∞–Ω–∫–∞–º (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π chunk_id)
        enriched_chunks: List[Chunk] = []
        for ch in chunks:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º chunk_id —á–∞–Ω–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            normalized_chunk_id = ChunkFilter.normalize_chunk_id(ch.id)
            enriched = enriched_map.get(normalized_chunk_id, {})

            # –ï—Å–ª–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ ID, –Ω–æ –µ—Å—Ç—å –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if not enriched and len(chunks) == 1 and valid_enriched_data:
                enriched = valid_enriched_data[0]
                if "chunk_id" not in enriched:
                    enriched["chunk_id"] = ch.id

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            validation_result = self._validator.validate_chunk(enriched, check_uniqueness=False)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ metrics
            if enriched.get("metrics"):
                metrics_warnings = self._validator.validate_metrics_quality(
                    enriched["metrics"],
                    chunk_text=ch.text,
                )
                if metrics_warnings:
                    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –∫–∞—á–µ—Å—Ç–≤–µ metrics
                    for warning in metrics_warnings:
                        print(f"   ‚ö†Ô∏è  {warning}")
            
            if not validation_result.is_valid:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
                if enriched.get("context"):
                    enriched["context"] = self._validator.normalize_context(
                        str(enriched["context"])
                    )
                if enriched.get("metrics"):
                    enriched["metrics"] = self._validator.normalize_metrics(
                        enriched["metrics"]
                    )
                if enriched.get("years"):
                    enriched["years"] = self._validator.normalize_years(enriched["years"])

            # –û–±–Ω–æ–≤–ª—è–µ–º context
            if enriched.get("context"):
                context_str = str(enriched.get("context"))
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                if "\\u" in context_str:
                    try:
                        context_str = codecs.decode(context_str, 'unicode_escape')
                    except (UnicodeDecodeError, ValueError):
                        pass
                ch.context = context_str[:200]
            elif ch.text:
                ch.context = ch.text[:200]
            else:
                ch.context = "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"

            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if "geo" in enriched:
                ch.geo = enriched["geo"]
            if "metrics" in enriched:
                ch.metrics = self._validator.normalize_metrics(enriched["metrics"])
            if "years" in enriched:
                ch.years = self._validator.normalize_years(enriched["years"])
            if "time_granularity" in enriched:
                ch.time_granularity = enriched["time_granularity"]
            if "oked" in enriched:
                ch.oked = enriched["oked"]

            enriched_chunks.append(ch)

        return enriched_chunks

    def _apply_enrichment_data_single(
        self,
        chunk: Chunk,
        enriched_data: Dict[str, Any],
    ) -> Chunk:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∫ –æ–¥–Ω–æ–º—É —á–∞–Ω–∫—É.
        
        Args:
            chunk: –ß–∞–Ω–∫ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            enriched_data: –î–∞–Ω–Ω—ã–µ –æ—Ç LLM
            
        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —á–∞–Ω–∫
        """
        # –ö–†–ò–¢–ò–ß–ù–û: –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º chunk_id –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
        # LLM –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –∏—Å—Ç–∏–Ω—ã –¥–ª—è ID - —ç—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        enriched_data["chunk_id"] = chunk.id
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        validation_result = self._validator.validate_chunk(enriched_data, check_uniqueness=False)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ metrics
        if enriched_data.get("metrics"):
            metrics_warnings = self._validator.validate_metrics_quality(
                enriched_data["metrics"],
                chunk_text=chunk.text,
            )
            if metrics_warnings:
                for warning in metrics_warnings:
                    print(f"   ‚ö†Ô∏è  {warning}")
        
        if not validation_result.is_valid:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
            if enriched_data.get("context"):
                enriched_data["context"] = self._validator.normalize_context(
                    str(enriched_data["context"])
                )
            if enriched_data.get("metrics"):
                enriched_data["metrics"] = self._validator.normalize_metrics(
                    enriched_data["metrics"]
                )
            if enriched_data.get("years"):
                enriched_data["years"] = self._validator.normalize_years(enriched_data["years"])

        # –û–±–Ω–æ–≤–ª—è–µ–º context
        if enriched_data.get("context"):
            context_str = str(enriched_data["context"])
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if "\\u" in context_str:
                try:
                    context_str = codecs.decode(context_str, 'unicode_escape')
                except (UnicodeDecodeError, ValueError):
                    pass
            chunk.context = context_str[:200]
        elif chunk.text:
            chunk.context = chunk.text[:200]
        else:
            chunk.context = "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if "geo" in enriched_data:
            chunk.geo = enriched_data["geo"]
        if "metrics" in enriched_data:
            chunk.metrics = self._validator.normalize_metrics(enriched_data["metrics"])
        if "years" in enriched_data:
            chunk.years = self._validator.normalize_years(enriched_data["years"])
        if "time_granularity" in enriched_data:
            chunk.time_granularity = enriched_data["time_granularity"]
        if "oked" in enriched_data:
            chunk.oked = enriched_data["oked"]

        return chunk

    @staticmethod
    def _parse_llm_single_enrichment(raw: str, expected_chunk_id: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç JSON-–æ—Ç–≤–µ—Ç –æ—Ç LLM –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (–æ–∂–∏–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç).
        
        Args:
            raw: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM
            expected_chunk_id: –û–∂–∏–¥–∞–µ–º—ã–π chunk_id
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not raw:
            return None

        # –£–¥–∞–ª–µ–Ω–∏–µ markdown code blocks
        cleaned = raw.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()

        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        while cleaned and cleaned[0] in [' ', '\n', '\r', '\t']:
            cleaned = cleaned[1:]

        # –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–∞
        try:
            data = json.loads(cleaned)
            if isinstance(data, dict):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –æ–±—ä–µ–∫—Ç —Å –Ω—É–∂–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
                if "chunk_id" in data or any(
                    key in data for key in ["context", "geo", "metrics", "years"]
                ):
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º chunk_id –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if "chunk_id" not in data:
                        data["chunk_id"] = expected_chunk_id
                    return data
        except json.JSONDecodeError:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ–±—ä–µ–∫—Ç –≤ —Ç–µ–∫—Å—Ç–µ
            start = cleaned.find("{")
            end = cleaned.rfind("}")
            if start != -1 and end != -1 and end > start:
                snippet = cleaned[start : end + 1]
                try:
                    data = json.loads(snippet)
                    if isinstance(data, dict):
                        if "chunk_id" not in data:
                            data["chunk_id"] = expected_chunk_id
                        return data
                except json.JSONDecodeError:
                    pass

        return None

    @staticmethod
    def _parse_llm_batch_enrichment(raw: str) -> List[Dict[str, Any]]:
        """
        –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–µ—Ä JSON-–æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM –¥–ª—è –±–∞—Ç—á–µ–≤–æ–≥–æ enrichment.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤.
        """
        if not raw:
            return []

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£–¥–∞–ª–µ–Ω–∏–µ markdown code blocks
        cleaned = raw.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()

        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        while cleaned and cleaned[0] in [' ', '\n', '\r', '\t']:
            cleaned = cleaned[1:]

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        try:
            data = json.loads(cleaned)
            if isinstance(data, list):
                return LLMEnricher._validate_and_fix_enrichment_data(data)
            elif isinstance(data, dict):
                for key in ["chunks", "data", "results", "items", "array"]:
                    if key in data and isinstance(data[key], list):
                        return LLMEnricher._validate_and_fix_enrichment_data(data[key])
                if "chunk_id" in data or any(
                    key in data for key in ["context", "geo", "metrics", "years"]
                ):
                    return LLMEnricher._validate_and_fix_enrichment_data([data])
        except json.JSONDecodeError:
            try:
                decoded = codecs.decode(cleaned, 'unicode_escape')
                data = json.loads(decoded)
                if isinstance(data, list):
                    return LLMEnricher._validate_and_fix_enrichment_data(data)
                elif isinstance(data, dict):
                    if "chunk_id" in data or any(
                        key in data for key in ["context", "geo", "metrics", "years"]
                    ):
                        return LLMEnricher._validate_and_fix_enrichment_data([data])
            except (json.JSONDecodeError, UnicodeDecodeError, ValueError):
                pass

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ JSON-–º–∞—Å—Å–∏–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        start = cleaned.find("[")
        end = cleaned.rfind("]")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            try:
                data = json.loads(snippet)
                if isinstance(data, list):
                    return LLMEnricher._validate_and_fix_enrichment_data(data)
            except json.JSONDecodeError:
                pass

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö JSON –æ–±—ä–µ–∫—Ç–æ–≤ (–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        objects = []
        i = 0
        while i < len(cleaned):
            if cleaned[i] == '{':
                depth = 0
                j = i
                while j < len(cleaned):
                    if cleaned[j] == '{':
                        depth += 1
                    elif cleaned[j] == '}':
                        depth -= 1
                        if depth == 0:
                            try:
                                obj_str = cleaned[i:j+1]
                                obj = json.loads(obj_str)
                                if isinstance(obj, dict):
                                    if "chunk_id" in obj or any(
                                        key in obj
                                        for key in ["context", "geo", "metrics", "years"]
                                    ):
                                        objects.append(obj)
                            except json.JSONDecodeError:
                                pass
                            i = j + 1
                            break
                    j += 1
                else:
                    i += 1
            else:
                i += 1

        if objects:
            return LLMEnricher._validate_and_fix_enrichment_data(objects)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü–æ–∏—Å–∫ –æ–¥–Ω–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç–∞
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            try:
                data = json.loads(snippet)
                if isinstance(data, dict):
                    for key in ["chunks", "data", "results", "items", "array"]:
                        if key in data and isinstance(data[key], list):
                            return LLMEnricher._validate_and_fix_enrichment_data(
                                data[key]
                            )
                    if "chunk_id" in data or any(
                        key in data for key in ["context", "geo", "metrics", "years"]
                    ):
                        return LLMEnricher._validate_and_fix_enrichment_data([data])
            except json.JSONDecodeError:
                pass

        print(
            f"‚ö†Ô∏è  WARNING: –ù–µ –Ω–∞–π–¥–µ–Ω JSON-–º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ LLM. "
            f"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤: {raw[:500]}"
        )
        return []

    @staticmethod
    def _validate_and_fix_enrichment_data(
        data: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è."""
        if not isinstance(data, list):
            print(f"‚ö†Ô∏è  WARNING: –ü–∞—Ä—Å–µ—Ä –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫, –∞ {type(data).__name__}")
            return []

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏
        valid_items = []
        for item in data:
            if isinstance(item, dict):
                valid_items.append(item)

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –ø–æ–ª—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        required_fields = [
            "chunk_id",
            "context",
            "geo",
            "metrics",
            "years",
            "time_granularity",
            "oked",
        ]
        for item in valid_items:
            for field in required_fields:
                if field not in item:
                    item[field] = None

        return valid_items
