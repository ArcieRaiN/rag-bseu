from __future__ import annotations

"""
–ú–æ–¥—É–ª—å –¥–ª—è LLM-–æ–±–æ–≥–∞—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer.

–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
- –û–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ Ollama (1 —á–∞–Ω–∫ = 1 –∑–∞–ø—Ä–æ—Å, –±–µ–∑ –±–∞—Ç—á–∏–Ω–≥–∞)
- Rolling context buffer (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞–Ω–∫–æ–≤ –≤–º–µ—Å—Ç–æ –≤—Å–µ—Ö)
- –ü–∞—Ä—Å–∏–Ω–≥ JSON-–æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç LLM
- –í–∞–ª–∏–¥–∞—Ü–∏—é –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
"""

from typing import List, Any, Optional, Deque, Dict
from collections import deque
import os
import time
import json
import codecs
from tqdm import tqdm
from sys import stdout
from colorama import init
init()


from src.main.models import Chunk
from src.main.ollama_client import OllamaClient
from src.main.logger import get_logger
from src.prepare_db.json_validator import ChunkValidator
from src.prepare_db.chunk_filter import ChunkFilter
from src.prepare_db.post_processor import EnrichmentPostProcessor


class RollingContextBuffer:
    """
    Rolling context buffer –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —á–∞–Ω–∫–æ–≤.
    """

    def __init__(self, max_size: int = 10):
        self._buffer: Deque[Chunk] = deque(maxlen=max_size)
        self.max_size = max_size

    def add(self, chunk: Chunk) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫ –≤ –±—É—Ñ–µ—Ä (—Å—Ç–∞—Ä—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è—é—Ç—Å—è)."""
        self._buffer.append(chunk)

    def add_batch(self, chunks: List[Chunk]) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –±–∞—Ç—á —á–∞–Ω–∫–æ–≤ –≤ –±—É—Ñ–µ—Ä."""
        for chunk in chunks:
            self._buffer.append(chunk)

    def get_context(self, num_chunks: int = 2) -> List[Chunk]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ num_chunks —á–∞–Ω–∫–æ–≤ (–µ—Å–ª–∏ –∏—Ö –º–µ–Ω—å—à–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ)."""
        if num_chunks <= 0:
            return []
        buf_list = list(self._buffer)
        if not buf_list:
            return []
        return buf_list[-num_chunks:]

    def clear(self) -> None:
        self._buffer.clear()

    def __len__(self) -> int:
        return len(self._buffer)


class LLMEnricher:
    """
    –ö–ª–∞—Å—Å –¥–ª—è LLM-–æ–±–æ–≥–∞—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ c –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer (1 —á–∞–Ω–∫ = 1 –∑–∞–ø—Ä–æ—Å).
    """

    def __init__(
        self,
        llm_client: OllamaClient,
        max_parallel_requests: int = 4,
        context_buffer_size: int = 10,
        reset_interval: int = 50,
    ):
        self._llm = llm_client
        self._max_parallel_requests = max(1, int(max_parallel_requests))
        self._reset_interval = max(1, int(reset_interval))
        self._logger = get_logger()
        self._validator = ChunkValidator()
        self._chunk_filter = ChunkFilter(skip_first_pages=3)
        self._post_processor = EnrichmentPostProcessor()
        self._context_buffer = RollingContextBuffer(max_size=context_buffer_size)
        self._chunks_since_reset = 0

    def enrich_chunks(
            self,
            pdf_name: str,
            chunks: List[Chunk],
            skip_first_pages: int = 3,
            show_progress: bool = True,
    ) -> List[Chunk]:

        """
        –û–±–æ–≥–∞—â–∞–µ—Ç —á–∞–Ω–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (1 —á–∞–Ω–∫ = 1 –∑–∞–ø—Ä–æ—Å).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ.
        """
        if not chunks:
            return []

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        data_chunks, metadata_chunks, skip_chunks = self._chunk_filter.filter_chunks(chunks)

        # –ü—Ä–æ—Å—Ç–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö / –ø—Ä–æ–ø—É—Å–∫–æ–≤
        for chunk in metadata_chunks + skip_chunks:
            chunk.context = chunk.text[:200] if chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"

        chunks_to_process = data_chunks
        chunks_to_skip = metadata_chunks + skip_chunks

        if not chunks_to_process:
            return chunks  # –Ω–∏—á–µ–≥–æ –¥–ª—è LLM

        total_chunks = len(chunks_to_process)
        self._logger.log("llm_enricher", {"event": "start", "total_chunks": total_chunks})

        all_enriched_chunks: List[Chunk] = chunks_to_skip.copy()
        start_time = time.time()
        completed_chunks = 0

        # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ ‚Äî –ø—Ä–æ—â–µ –∏ –±–µ–∑ race condition –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        progress_iter = tqdm(
            chunks_to_process,
            total=len(chunks_to_process),
            desc=f"LLM enrich: {pdf_name}",
            mininterval=0.5,
            disable=not show_progress,
            file=stdout,
            colour="green",
        )

        for idx, chunk in enumerate(progress_iter):
            original_chunk = chunk
            enriched_chunk: Optional[Chunk] = None

            try:
                previous_ctx = self._context_buffer.get_context(num_chunks=2)
                enriched_chunk = self._enrich_single_chunk(pdf_name, chunk, previous_ctx)

                if enriched_chunk:
                    self._context_buffer.add(enriched_chunk)
                    all_enriched_chunks.append(enriched_chunk)
                else:
                    if not original_chunk.context:
                        original_chunk.context = (
                            original_chunk.text[:200] if original_chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                        )
                    self._context_buffer.add(original_chunk)
                    all_enriched_chunks.append(original_chunk)

                completed_chunks += 1
                self._chunks_since_reset += 1

                # reset –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏
                if self._chunks_since_reset >= self._reset_interval:
                    self._logger.log(
                        "llm_enricher",
                        {"event": "model_context_reset", "after_chunks": completed_chunks},
                    )
                    try:
                        self._llm.reset_context()
                    except Exception as e:
                        self._logger.log(
                            "llm_enricher",
                            {"event": "reset_error", "error": str(e)},
                        )
                    self._chunks_since_reset = 0

                # –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å (–∫–∞–∫ –±—ã–ª–æ)
                if completed_chunks % 10 == 0 or completed_chunks == total_chunks:
                    elapsed = time.time() - start_time
                    rate = completed_chunks / elapsed * 3600 if elapsed > 0 else 0
                    self._logger.log(
                        "llm_enricher",
                        {
                            "event": "progress",
                            "completed": completed_chunks,
                            "total": total_chunks,
                            "rate_per_hour": rate,
                        },
                    )

                # üëâ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –≤ tqdm
                progress_iter.set_postfix(
                    rate=f"{rate:.1f}/h",
                    ctx=len(self._context_buffer),
                )

            except Exception as e:
                self._logger.log(
                    "llm_enricher",
                    {
                        "event": "error",
                        "chunk_id": getattr(original_chunk, "id", None),
                        "error": str(e),
                    },
                )
                if not original_chunk.context:
                    original_chunk.context = (
                        original_chunk.text[:200] if original_chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                    )
                all_enriched_chunks.append(original_chunk)
                completed_chunks += 1
                self._chunks_since_reset += 1

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        chunk_order = {ch.id: i for i, ch in enumerate(chunks)}
        all_enriched_chunks.sort(key=lambda ch: chunk_order.get(ch.id, 999999))

        self._logger.log("llm_enricher", {"event": "done", "total_returned": len(all_enriched_chunks)})
        return all_enriched_chunks

    def _enrich_single_chunk(
        self,
        pdf_name: str,
        chunk: Chunk,
        previous_chunks: Optional[List[Chunk]] = None,
    ) -> Optional[Chunk]:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
        """
        previous_context: Optional[str] = None
        if previous_chunks:
            prev_texts: List[str] = []
            for prev_ch in previous_chunks[-2:]:
                if prev_ch and getattr(prev_ch, "text", None):
                    prev_texts.append(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {prev_ch.page}: {prev_ch.text[:200]}")
            if prev_texts:
                previous_context = "\n".join(prev_texts)

        system_prompt = self._build_system_prompt_single()
        prompt = self._build_prompt_single(pdf_name, chunk, previous_context)

        keep_alive = os.getenv("RAG_OLLAMA_KEEP_ALIVE", "5m")
        req_options = {"temperature": 0, "top_p": 1, "num_predict": 300}

        ollama_config = {
            "model": getattr(getattr(self._llm, "config", None), "model", None),
            "base_url": getattr(getattr(self._llm, "config", None), "base_url", None),
            "timeout": getattr(getattr(self._llm, "config", None), "timeout", None),
            "format": "json",
            "options": req_options,
        }

        # –õ–æ–≥ –∑–∞–ø—Ä–æ—Å–∞
        try:
            self._logger.log_llm_enrichment(
                event="request",
                pdf_name=pdf_name,
                chunks_count=1,
                chunk_ids=[chunk.id],
                pages=[chunk.page],
                system_prompt=system_prompt,
                prompt=prompt,
                ollama_config=ollama_config,
            )
        except Exception:
            # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ logger –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç log_llm_enrichment
            self._logger.log("llm_enricher", {"event": "request", "chunk_id": chunk.id})

        max_retries = 2
        enriched_data: Optional[Dict[str, Any]] = None
        raw_response = ""

        for attempt in range(max_retries):
            try:
                raw_response = self._llm.generate(
                    prompt,
                    system_prompt=system_prompt,
                    format="json",
                    keep_alive=keep_alive,
                    options=req_options,
                )
            except Exception as e:
                # –°–µ—Ç–µ–≤–∞—è/LLM –æ—à–∏–±–∫–∞ ‚Äî –ª–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞ (–¥–æ max_retries)
                self._logger.log("llm_enricher", {"event": "generate_error", "chunk_id": chunk.id, "attempt": attempt, "error": str(e)})
                if attempt < max_retries - 1:
                    time.sleep(0.5)
                continue

            enriched_data = self._parse_llm_single_enrichment(raw_response, chunk.id)

            if enriched_data:
                # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å id
                enriched_data["chunk_id"] = chunk.id
                break
            elif attempt < max_retries - 1:
                # –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è ‚Äî –ø—Ä–æ–±—É–µ–º –µ—â—ë —Ä–∞–∑
                self._logger.log("llm_enricher", {"event": "parse_retry", "chunk_id": chunk.id, "attempt": attempt})
                time.sleep(0.5)

        parsed_successfully = enriched_data is not None

        # –õ–æ–≥ –æ—Ç–≤–µ—Ç–∞
        try:
            self._logger.log_llm_enrichment(
                event="response",
                pdf_name=pdf_name,
                chunks_count=1,
                chunk_ids=[chunk.id],
                raw_response=raw_response,
                parsed_items=1 if parsed_successfully else 0,
                parsed_with_chunk_id=1 if (parsed_successfully and enriched_data and enriched_data.get("chunk_id")) else 0,
            )
        except Exception:
            self._logger.log("llm_enricher", {"event": "response", "chunk_id": chunk.id, "parsed": parsed_successfully})

        if not enriched_data:
            return None

        enriched_chunk = self._apply_enrichment_data_single(chunk, enriched_data)
        enriched_chunk = self._post_processor.process_chunk(enriched_chunk)
        return enriched_chunk

    def _build_system_prompt_single(self) -> str:
        return (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
            "1. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON-–æ–±—ä–µ–∫—Ç {}, –Ω–∞—á–∏–Ω–∞—é—â–∏–π—Å—è —Å '{' –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏–π—Å—è '}'\n"
            "2. –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–π –º–∞—Å—Å–∏–≤ [] - —Ç–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç {}\n"
            "3. chunk_id –¥–æ–ª–∂–µ–Ω —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–ø–æ–±–∞–π—Ç–Ω–æ, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n"
            "4. –ù–ï –¥–æ–±–∞–≤–ª—è–π —Ç–µ–∫—Å—Ç –¥–æ –∏–ª–∏ –ø–æ—Å–ª–µ JSON\n"
            "5. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π markdown code blocks (```json)\n"
            "6. –ù–ï –¥–æ–±–∞–≤–ª—è–π –ø—Ä–µ—Ñ–∏–∫—Å—ã —Ç–∏–ø–∞ 'ID:' –∫ chunk_id - –∏—Å–ø–æ–ª—å–∑—É–π –∑–Ω–∞—á–µ–Ω–∏–µ –∫–∞–∫ –µ—Å—Ç—å\n\n"
            "JSON Schema –¥–ª—è –æ–±—ä–µ–∫—Ç–∞:\n"
            "{\n"
            '  "chunk_id": "string (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)",\n'
            '  "context": "string (–º–∞–∫—Å 256 —Å–∏–º–≤–æ–ª–æ–≤, —Ä—É—Å—Å–∫–∏–π)",\n'
            '  "geo": "string | null",\n'
            '  "metrics": ["string"] | null (–º–∞–∫—Å 5, —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä),\n'
            '  "years": [int] | null (–º–∞–∫—Å 5, —Ç–æ–ª—å–∫–æ –≥–æ–¥—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞),\n'
            '  "time_granularity": "year" | "quarter" | "month" | "day" | null,\n'
            '  "oked": "string | null"\n'
            "}\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è metrics:\n"
            "- –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: '—É–¥–æ–π –º–æ–ª–æ–∫–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏')\n"
            "- –ù–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ù–ï –æ–±—ä–µ–∫—Ç—ã –Ω–∞–±–ª—é–¥–µ–Ω–∏—è, –ù–ï –∑–∞–≥–æ–ª–æ–≤–∫–∏\n"
            "- –¢–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä\n"
            "- –ú–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è years:\n"
            "- –¢–æ–ª—å–∫–æ –≥–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ\n"
            "- –ù–ï –¥–æ–±–∞–≤–ª—è–π –≥–æ–¥—ã, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ (–∏—Å–ø–æ–ª—å–∑—É–π null)\n"
            "- –ù–ï –¥–æ–±–∞–≤–ª—è–π –≥–æ–¥—ã –¥–ª—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è\n"
            "- –ú–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n"
        )

    def _build_prompt_single(
        self,
        pdf_name: str,
        chunk: Chunk,
        previous_context: Optional[str],
    ) -> str:
        context_section = ""
        if previous_context:
            context_section = (
                f"\n–ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ò–• –°–¢–†–ê–ù–ò–¶ "
                f"(–¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞):\n"
                f"{previous_context}\n\n"
            )

        return (
            f"–î–æ–∫—É–º–µ–Ω—Ç: {pdf_name}\n\n"
            f"{context_section}"
            f"–ß–∞–Ω–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n"
            f"chunk_id: {chunk.id}\n"
            f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {chunk.page}\n"
            f"–¢–µ–∫—Å—Ç: {(chunk.text or '')[:500]}\n\n"
            "–í–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç (–ù–ï –º–∞—Å—Å–∏–≤!). –§–æ—Ä–º–∞—Ç:\n"
            '{"chunk_id":"...","context":"...","geo":null,"metrics":null,'
            '"years":null,"time_granularity":null,"oked":null}\n\n'
            "–ü–æ–ª—è:\n"
            f"- chunk_id: —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ –≤—ã—à–µ: '{chunk.id}' (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤)\n"
            "- context: –∫—Ä–∞—Ç–∫–æ–µ, —Ç–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞–Ω–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ "
            "(1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —á–∞–Ω–∫–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n"
            "- geo: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞/–≥–æ—Ä–æ–¥–∞/–æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ null\n"
            "- metrics: —Å–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–º–∞–∫—Å–∏–º—É–º 5) –∏–ª–∏ null. "
            "–¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞, –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n"
            "- years: —Å–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ (–º–∞–∫—Å–∏–º—É–º 9) –∏–ª–∏ null. –¢–æ–ª—å–∫–æ –≥–æ–¥—ã, —è–≤–Ω–æ —É–ø–æ–º—è–Ω—É—Ç—ã–µ –≤ —Ç–µ–∫—Å—Ç–µ\n"
            "- time_granularity: 'year'/'quarter'/'month'/'day' –∏–ª–∏ null\n"
            "- oked: –∫–æ–¥ –û–ö–≠–î –∏–ª–∏ null\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
            "1. –í–µ—Ä–Ω–∏ –û–ë–™–ï–ö–¢ {}, –ù–ï –º–∞—Å—Å–∏–≤ []\n"
            "2. chunk_id –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ—á–Ω–æ —Ç–∞–∫–∏–º –∂–µ –∫–∞–∫ –≤—ã—à–µ (–±–µ–∑ 'ID:', –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n"
            "3. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (\\u0412) - –ø–∏—à–∏ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é"
        )

    def _apply_enrichment_data_single(
        self,
        chunk: Chunk,
        enriched_data: Dict[str, Any],
    ) -> Chunk:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∫ –æ–¥–Ω–æ–º—É —á–∞–Ω–∫—É.
        """
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º ID –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
        enriched_data["chunk_id"] = chunk.id

        validation_result = self._validator.validate_chunk(enriched_data, check_uniqueness=False)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ metrics
        if enriched_data.get("metrics"):
            metrics_warnings = self._validator.validate_metrics_quality(
                enriched_data["metrics"],
                chunk_text=chunk.text,
            )
            if metrics_warnings:
                for warning in metrics_warnings:
                    self._logger.log("llm_enricher", {"event": "metrics_warning", "chunk_id": chunk.id, "warning": warning})

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if not validation_result.is_valid:
            if enriched_data.get("context"):
                enriched_data["context"] = self._validator.normalize_context(str(enriched_data["context"]))
            if enriched_data.get("metrics"):
                enriched_data["metrics"] = self._validator.normalize_metrics(enriched_data["metrics"])
            if enriched_data.get("years"):
                enriched_data["years"] = self._validator.normalize_years(enriched_data["years"])

        # –û–±–Ω–æ–≤–ª—è–µ–º context
        if enriched_data.get("context"):
            context_str = str(enriched_data.get("context"))
            if "\\u" in context_str:
                try:
                    context_str = codecs.decode(context_str, "unicode_escape")
                except (UnicodeDecodeError, ValueError):
                    pass
            chunk.context = context_str[:200]
        elif chunk.text:
            chunk.context = chunk.text[:200]
        else:
            chunk.context = "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if "geo" in enriched_data:
            chunk.geo = enriched_data["geo"]
        if "metrics" in enriched_data:
            chunk.metrics = self._validator.normalize_metrics(enriched_data["metrics"])
        if "years" in enriched_data:
            chunk.years = self._validator.normalize_years(enriched_data["years"])
        if "time_granularity" in enriched_data:
            chunk.time_granularity = enriched_data["time_granularity"]
        if "oked" in enriched_data:
            chunk.oked = enriched_data["oked"]

        return chunk

    @staticmethod
    def _parse_llm_single_enrichment(raw: str, expected_chunk_id: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç JSON-–æ—Ç–≤–µ—Ç –æ—Ç LLM –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (–æ–∂–∏–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict –∏–ª–∏ None.
        """
        if not raw:
            return None

        cleaned = raw.strip()

        # –£–¥–∞–ª—è–µ–º code blocks
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()

        # –£–¥–∞–ª–µ–Ω–∏–µ –≤–µ–¥—É—â–∏—Ö –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        while cleaned and cleaned[0] in [" ", "\n", "\r", "\t"]:
            cleaned = cleaned[1:]

        # 1) –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        try:
            data = json.loads(cleaned)
            if isinstance(data, dict):
                # –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª—è –∏–Ω—Ç–µ—Ä–µ—Å–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—Ç
                if "chunk_id" in data or any(k in data for k in ["context", "geo", "metrics", "years"]):
                    if "chunk_id" not in data:
                        data["chunk_id"] = expected_chunk_id
                    return data
        except json.JSONDecodeError:
            pass

        # 2) –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –æ–±—ä–µ–∫—Ç { ... } –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            try:
                data = json.loads(snippet)
                if isinstance(data, dict):
                    if "chunk_id" not in data:
                        data["chunk_id"] = expected_chunk_id
                    return data
            except json.JSONDecodeError:
                pass

        return None
