from __future__ import annotations

"""
–ú–æ–¥—É–ª—å –¥–ª—è LLM-–æ–±–æ–≥–∞—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer.

–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
- –ë–∞—Ç—á–µ–≤–æ–µ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ Ollama
- Rolling context buffer (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞–Ω–∫–æ–≤ –≤–º–µ—Å—Ç–æ –≤—Å–µ—Ö)
- –ü–∞—Ä—Å–∏–Ω–≥ JSON-–æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç LLM
- –í–∞–ª–∏–¥–∞—Ü–∏—é –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
"""

from typing import List, Dict, Any, Optional, Deque
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os
import time
import codecs

from src.main.models import Chunk
from src.main.ollama_client import OllamaClient
from src.logs.logger import get_logger
from src.prepare_db.json_validator import ChunkValidator


class RollingContextBuffer:
    """
    Rolling context buffer –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —á–∞–Ω–∫–æ–≤.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–º–µ—Å—Ç–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
    –∏ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """

    def __init__(self, max_size: int = 10):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—É—Ñ–µ—Ä–∞.
        
        Args:
            max_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –≤ –±—É—Ñ–µ—Ä–µ
        """
        self._buffer: Deque[Chunk] = deque(maxlen=max_size)
        self.max_size = max_size

    def add(self, chunk: Chunk) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫ –≤ –±—É—Ñ–µ—Ä (—Å—Ç–∞—Ä—ã–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–¥–∞–ª—è—é—Ç—Å—è)."""
        self._buffer.append(chunk)

    def add_batch(self, chunks: List[Chunk]) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –±–∞—Ç—á —á–∞–Ω–∫–æ–≤ –≤ –±—É—Ñ–µ—Ä."""
        for chunk in chunks:
            self._buffer.append(chunk)

    def get_context(self, num_chunks: int = 2) -> List[Chunk]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        Args:
            num_chunks: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N —á–∞–Ω–∫–æ–≤
        """
        return list(self._buffer)[-num_chunks:]

    def clear(self) -> None:
        """–û—á–∏—â–∞–µ—Ç –±—É—Ñ–µ—Ä."""
        self._buffer.clear()

    def __len__(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞."""
        return len(self._buffer)


class LLMEnricher:
    """
    –ö–ª–∞—Å—Å –¥–ª—è LLM-–æ–±–æ–≥–∞—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–∞—Ç—á–µ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞—Ç—á–µ–π.
    """

    def __init__(
        self,
        llm_client: OllamaClient,
        batch_size: int = 5,
        batch_concurrency: int = 1,
        context_buffer_size: int = 10,
        reset_interval: int = 50,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–æ–≥–∞—Ç–∏—Ç–µ–ª—è.
        
        Args:
            llm_client: –ö–ª–∏–µ–Ω—Ç Ollama –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –≤ –æ–¥–Ω–æ–º LLM-–∑–∞–ø—Ä–æ—Å–µ)
            batch_concurrency: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –±–∞—Ç—á–µ–π
            context_buffer_size: –†–∞–∑–º–µ—Ä rolling context buffer
            reset_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–±—Ä–æ—Å–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ (–≤ —á–∞–Ω–∫–∞—Ö)
        """
        self._llm = llm_client
        self._batch_size = batch_size
        self._batch_concurrency = batch_concurrency
        self._reset_interval = reset_interval
        self._logger = get_logger()
        self._validator = ChunkValidator()
        
        # Rolling context buffer
        self._context_buffer = RollingContextBuffer(max_size=context_buffer_size)
        
        # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–±—Ä–æ—Å–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self._chunks_since_reset = 0

    def enrich_chunks(
        self,
        pdf_name: str,
        chunks: List[Chunk],
        skip_first_pages: int = 3,
    ) -> List[Chunk]:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º rolling context buffer.
        
        Args:
            pdf_name: –ò–º—è PDF —Ñ–∞–π–ª–∞
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            skip_first_pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞ (—Ç–æ–ª—å–∫–æ –æ–±–ª–æ–∂–∫–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        if not chunks:
            return []

        # –†–∞–∑–¥–µ–ª—è–µ–º —á–∞–Ω–∫–∏ –Ω–∞ —Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –∏ —Ç–µ, —á—Ç–æ –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
        chunks_to_process: List[Chunk] = []
        chunks_to_skip: List[Chunk] = []

        for chunk in chunks:
            if chunk.page <= skip_first_pages:
                # –î–ª—è –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º text –∫–∞–∫ context
                chunk.context = chunk.text[:200] if chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                chunks_to_skip.append(chunk)
            else:
                chunks_to_process.append(chunk)

        if not chunks_to_process:
            return chunks

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ env (—Å fallback –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞)
        batch_size = int(os.getenv("RAG_ENRICH_BATCH_SIZE", str(self._batch_size)))
        batch_concurrency = int(os.getenv("RAG_ENRICH_CONCURRENCY", str(self._batch_concurrency)))
        batch_size = max(1, batch_size)
        batch_concurrency = max(1, min(batch_concurrency, 8))

        total_batches = (len(chunks_to_process) + batch_size - 1) // batch_size
        print(
            f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(chunks_to_process)} —á–∞–Ω–∫–æ–≤ "
            f"(–ø—Ä–æ–ø—É—â–µ–Ω–æ {len(chunks_to_skip)} —á–∞–Ω–∫–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü 1-{skip_first_pages})"
        )
        print(
            f"   –ë–∞—Ç—á–∏: —Ä–∞–∑–º–µ—Ä={batch_size}, –±–∞—Ç—á–µ–π={total_batches}, "
            f"–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å={batch_concurrency}"
        )

        all_enriched_chunks: List[Chunk] = chunks_to_skip.copy()

        # –ü–∞—Ä–∞–ª–ª–µ–ª–∏–º —Ç–æ–ª—å–∫–æ –±–∞—Ç—á–∏ (–∞ –Ω–µ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫)
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=min(batch_concurrency, total_batches)) as executor:
            futures = []
            for start in range(0, len(chunks_to_process), batch_size):
                batch = chunks_to_process[start : start + batch_size]
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ rolling buffer (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —á–∞–Ω–∫–∞)
                previous_chunks = self._context_buffer.get_context(num_chunks=2)
                
                fut = executor.submit(
                    self._enrich_single_batch,
                    pdf_name,
                    batch,
                    previous_chunks,
                )
                fut._rag_submit_ts = time.time()  # type: ignore[attr-defined]
                futures.append((fut, batch))

            completed_batches = 0
            completed_chunks = 0

            for fut, original_batch in futures:
                try:
                    enriched_batch = fut.result()
                    submit_ts = getattr(fut, "_rag_submit_ts", None)
                    batch_time = (time.time() - submit_ts) if submit_ts else 0.0
                    
                    all_enriched_chunks.extend(enriched_batch)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –≤ rolling buffer
                    self._context_buffer.add_batch(enriched_batch)
                    
                    completed_batches += 1
                    completed_chunks += len(enriched_batch)
                    self._chunks_since_reset += len(enriched_batch)

                    # –°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–∞–∂–¥—ã–µ N —á–∞–Ω–∫–æ–≤
                    if self._chunks_since_reset >= self._reset_interval:
                        print(
                            f"   üîÑ –°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ {completed_chunks} —á–∞–Ω–∫–æ–≤..."
                        )
                        self._llm.reset_context()
                        self._chunks_since_reset = 0
                        time.sleep(0.5)

                    elapsed = time.time() - start_time
                    rate = completed_chunks / elapsed * 3600 if elapsed > 0 else 0
                    print(
                        f"   –ë–∞—Ç—á {completed_batches}/{total_batches}: "
                        f"{len(enriched_batch)} —á–∞–Ω–∫–æ–≤ –∑–∞ {batch_time:.1f}—Å | "
                        f"–í—Å–µ–≥–æ: {completed_chunks}/{len(chunks_to_process)} | "
                        f"–°–∫–æ—Ä–æ—Å—Ç—å: {rate:.0f} —á–∞–Ω–∫–æ–≤/—á–∞—Å"
                    )

                    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ç—á–∞
                    self._logger.log_prepare_db(
                        "batch",
                        pdf_name=pdf_name,
                        batch_number=completed_batches,
                        total_batches=total_batches,
                        chunks_in_batch=len(enriched_batch),
                        elapsed_time=batch_time,
                        total_chunks_processed=completed_chunks,
                        rate_per_hour=rate,
                    )
                except Exception as e:
                    submit_ts = getattr(fut, "_rag_submit_ts", None)
                    batch_time = (time.time() - submit_ts) if submit_ts else 0.0
                    completed_batches += 1
                    print(
                        f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {completed_batches}/{total_batches} "
                        f"(–≤—Ä–µ–º—è: {batch_time:.1f}—Å): {e}"
                    )
                    import traceback
                    traceback.print_exc()
                    # Fallback: –¥–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏ –±–µ–∑ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
                    for ch in original_batch:
                        if not ch.context:
                            ch.context = ch.text[:200] if ch.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                    all_enriched_chunks.extend(original_batch)
                    completed_chunks += len(original_batch)
                    self._chunks_since_reset += len(original_batch)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
        chunk_order = {ch.id: i for i, ch in enumerate(chunks)}
        all_enriched_chunks.sort(key=lambda ch: chunk_order.get(ch.id, 999999))

        return all_enriched_chunks

    def _enrich_single_batch(
        self,
        pdf_name: str,
        chunks: List[Chunk],
        previous_chunks: List[Chunk] | None = None,
    ) -> List[Chunk]:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç –æ–¥–∏–Ω –±–∞—Ç—á —á–∞–Ω–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
        
        Args:
            pdf_name: –ò–º—è PDF —Ñ–∞–π–ª–∞
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            previous_chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        if not chunks:
            return []

        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        chunks_data = []
        for ch in chunks:
            chunks_data.append({
                "chunk_id": ch.id,
                # –†–µ–∂–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                "text": (ch.text or "")[:350],
                "page": ch.page,
            })

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        previous_context = None
        if previous_chunks:
            prev_texts = []
            for prev_ch in previous_chunks[-2:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2 —á–∞–Ω–∫–∞
                if prev_ch.text:
                    prev_texts.append(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {prev_ch.page}: {prev_ch.text[:200]}")
            if prev_texts:
                previous_context = "\n".join(prev_texts)

        # System prompt
        system_prompt = (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏. "
            "STRICT RULES: Output ONLY a valid JSON array - No text before or after JSON - "
            "No markdown, comments, explanations - chunk_id must exactly match input. "
            "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: context <= 256 —Å–∏–º–≤–æ–ª–æ–≤; metrics –º–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤; "
            "years –º–∞–∫—Å–∏–º—É–º 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤."
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = self._build_prompt(pdf_name, chunks_data, previous_context)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        keep_alive = os.getenv("RAG_OLLAMA_KEEP_ALIVE", "5m")
        req_options = {
            "temperature": 0,
            "top_p": 1,
            "num_predict": min(250 * len(chunks_data) + 100, 3000),
        }

        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        ollama_config = {
            "model": getattr(getattr(self._llm, "config", None), "model", None),
            "base_url": getattr(getattr(self._llm, "config", None), "base_url", None),
            "timeout": getattr(getattr(self._llm, "config", None), "timeout", None),
            "format": "json",
            "options": req_options,
        }

        self._logger.log_llm_enrichment(
            event="request",
            pdf_name=pdf_name,
            chunks_count=len(chunks_data),
            chunk_ids=[c["chunk_id"] for c in chunks_data],
            pages=[c["page"] for c in chunks_data],
            system_prompt=system_prompt,
            prompt=prompt,
            ollama_config=ollama_config,
        )

        # –í—ã–∑—ã–≤–∞–µ–º LLM —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        max_retries_for_quality = 2
        enriched_data = []
        raw_response = ""

        for attempt in range(max_retries_for_quality):
            raw_response = self._llm.generate(
                prompt,
                system_prompt=system_prompt,
                format="json",
                keep_alive=keep_alive,
                options=req_options,
            )

            # –ü–∞—Ä—Å–∏–º JSON-–æ—Ç–≤–µ—Ç
            enriched_data = self._parse_llm_batch_enrichment(raw_response)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
            valid_items = [item for item in enriched_data if isinstance(item, dict)]
            if len(valid_items) >= len(chunks_data) * 0.8:  # –•–æ—Ç—è –±—ã 80% —á–∞–Ω–∫–æ–≤ –æ–±–æ–≥–∞—â–µ–Ω–æ
                break

            if attempt < max_retries_for_quality - 1:
                print(
                    f"   ‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ ({len(valid_items)}/{len(chunks_data)}), "
                    f"–ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}/{max_retries_for_quality}..."
                )
                time.sleep(1)

        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        valid_enriched_data_for_log = [
            item for item in enriched_data if isinstance(item, dict)
        ]
        parsed_with_chunk_id = sum(
            1 for item in valid_enriched_data_for_log if item.get("chunk_id")
        )
        self._logger.log_llm_enrichment(
            event="response",
            pdf_name=pdf_name,
            chunks_count=len(chunks_data),
            chunk_ids=[c["chunk_id"] for c in chunks_data],
            raw_response=raw_response,
            parsed_items=len(valid_enriched_data_for_log),
            parsed_with_chunk_id=parsed_with_chunk_id,
        )

        # –û–±–æ–≥–∞—â–∞–µ–º —á–∞–Ω–∫–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç LLM
        enriched_chunks = self._apply_enrichment_data(chunks, enriched_data)

        return enriched_chunks

    def _build_prompt(
        self,
        pdf_name: str,
        chunks_data: List[Dict[str, Any]],
        previous_context: Optional[str],
    ) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM."""
        context_section = ""
        if previous_context:
            context_section = (
                f"\n–ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ò–• –°–¢–†–ê–ù–ò–¶ "
                f"(–¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞):\n"
                f"{previous_context}\n\n"
            )

        if len(chunks_data) == 1:
            chunk = chunks_data[0]
            return (
                f"–î–æ–∫—É–º–µ–Ω—Ç: {pdf_name}\n\n"
                f"{context_section}"
                f"–ß–∞–Ω–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n"
                f"ID: {chunk['chunk_id']}\n"
                f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {chunk['page']}\n"
                f"–¢–µ–∫—Å—Ç: {chunk['text'][:500]}...\n\n"
                "–í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ —Å –æ–¥–Ω–∏–º –æ–±—ä–µ–∫—Ç–æ–º. –§–æ—Ä–º–∞—Ç:\n"
                '[{"chunk_id":"...","context":"...","geo":null,"metrics":null,'
                '"years":null,"time_granularity":null,"oked":null}]\n\n'
                "–ü–æ–ª—è:\n"
                "- chunk_id: —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ ID –≤—ã—à–µ\n"
                "- context: –∫—Ä–∞—Ç–∫–æ–µ, —Ç–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞–Ω–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ "
                "(1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —á–∞–Ω–∫–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n"
                "- geo: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞/–≥–æ—Ä–æ–¥–∞/–æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ null\n"
                "- metrics: —Å–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–º–∞–∫—Å–∏–º—É–º 5) –∏–ª–∏ null\n"
                "- years: —Å–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ (–º–∞–∫—Å–∏–º—É–º 5) –∏–ª–∏ null\n"
                "- time_granularity: 'year'/'quarter'/'month'/'day' –∏–ª–∏ null\n"
                "- oked: –∫–æ–¥ –û–ö–≠–î –∏–ª–∏ null\n\n"
                "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –º–∞—Å—Å–∏–≤ [{}], –ù–ï –æ–±—ä–µ–∫—Ç {}!"
            )
        else:
            return (
                f"–î–æ–∫—É–º–µ–Ω—Ç: {pdf_name}\n\n"
                f"{context_section}"
                f"–û–±—Ä–∞–±–æ—Ç–∞–π {len(chunks_data)} —á–∞–Ω–∫–æ–≤. "
                f"–í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ –∏–∑ –†–û–í–ù–û {len(chunks_data)} –æ–±—ä–µ–∫—Ç–æ–≤.\n\n"
                "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–∏–º–µ—Ä –¥–ª—è 2 —á–∞–Ω–∫–æ–≤):\n"
                '[{"chunk_id":"doc.pdf::page1::chunk0","context":"–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ",'
                '"geo":null,"metrics":["–º–µ—Ç—Ä–∏–∫–∞1"],"years":[2024],"time_granularity":null,'
                '"oked":null},'
                '{"chunk_id":"doc.pdf::page1::chunk1","context":"–î—Ä—É–≥–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ",'
                '"geo":"–ú–∏–Ω—Å–∫","metrics":null,"years":null,"time_granularity":"year",'
                '"oked":null}]\n\n'
                "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞:\n"
                "- chunk_id: —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)\n"
                "- context: –∫—Ä–∞—Ç–∫–æ–µ, —Ç–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞–Ω–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ "
                "(1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —á–∞–Ω–∫–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n"
                "- metrics: —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —Ä—É—Å—Å–∫–∏–π, "
                "–º–∞–∫—Å–∏–º—É–º 5, –∏–ª–∏ null\n"
                "- years: —Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞, –º–∞–∫—Å–∏–º—É–º 5, –∏–ª–∏ null\n"
                "- geo: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞/–≥–æ—Ä–æ–¥–∞/–æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ null\n"
                "- time_granularity: 'year'/'quarter'/'month'/'day' –∏–ª–∏ null\n"
                "- oked: –∫–æ–¥ –û–ö–≠–î –∏–ª–∏ null\n\n"
                "–í—Ö–æ–¥–Ω—ã–µ —á–∞–Ω–∫–∏:\n"
                f"{json.dumps(chunks_data, ensure_ascii=False, separators=(',', ':'))}\n\n"
                "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
                "1. –í–µ—Ä–Ω–∏ –ú–ê–°–°–ò–í [{}, {}, ...], –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å '[' –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è ']'\n"
                "2. –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–π –æ–±—ä–µ–∫—Ç {}\n"
                f"3. –í –º–∞—Å—Å–∏–≤–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –†–û–í–ù–û {len(chunks_data)} –æ–±—ä–µ–∫—Ç–æ–≤\n"
                "4. –ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å chunk_id –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
                "5. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (\\u0412) - "
                "–ø–∏—à–∏ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é"
            )

    def _apply_enrichment_data(
        self,
        chunks: List[Chunk],
        enriched_data: List[Dict[str, Any]],
    ) -> List[Chunk]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∫ —á–∞–Ω–∫–∞–º."""
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏
        valid_enriched_data = [
            item for item in enriched_data if isinstance(item, dict)
        ]

        # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å chunk_id -> enriched_data
        enriched_map: Dict[str, Dict[str, Any]] = {}
        for item in valid_enriched_data:
            chunk_id = item.get("chunk_id")
            if chunk_id:
                enriched_map[str(chunk_id)] = item

        # –ï—Å–ª–∏ –Ω–µ –≤—Å–µ —á–∞–Ω–∫–∏ –Ω–∞–π–¥–µ–Ω—ã –ø–æ ID, –ø—ã—Ç–∞–µ–º—Å—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ –ø–æ—Ä—è–¥–∫—É
        if len(enriched_map) < len(chunks) and valid_enriched_data:
            if len(valid_enriched_data) == len(chunks):
                for i, ch in enumerate(chunks):
                    if ch.id not in enriched_map and i < len(valid_enriched_data):
                        enriched_map[ch.id] = valid_enriched_data[i]
                        enriched_map[ch.id]["chunk_id"] = ch.id
            elif len(valid_enriched_data) == 1 and len(chunks) > 1:
                first_chunk = chunks[0]
                if first_chunk.id not in enriched_map:
                    enriched_map[first_chunk.id] = valid_enriched_data[0].copy()
                    enriched_map[first_chunk.id]["chunk_id"] = first_chunk.id

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫ —á–∞–Ω–∫–∞–º
        enriched_chunks: List[Chunk] = []
        for ch in chunks:
            enriched = enriched_map.get(ch.id, {})

            # –ï—Å–ª–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ ID, –Ω–æ –µ—Å—Ç—å –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if not enriched and len(chunks) == 1 and valid_enriched_data:
                enriched = valid_enriched_data[0]
                if "chunk_id" not in enriched:
                    enriched["chunk_id"] = ch.id

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            validation_result = self._validator.validate_chunk(enriched, check_uniqueness=False)
            if not validation_result.is_valid:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫
                if enriched.get("context"):
                    enriched["context"] = self._validator.normalize_context(
                        str(enriched["context"])
                    )
                if enriched.get("metrics"):
                    enriched["metrics"] = self._validator.normalize_metrics(
                        enriched["metrics"]
                    )
                if enriched.get("years"):
                    enriched["years"] = self._validator.normalize_years(enriched["years"])

            # –û–±–Ω–æ–≤–ª—è–µ–º context
            if enriched.get("context"):
                context_str = str(enriched.get("context"))
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                if "\\u" in context_str:
                    try:
                        context_str = codecs.decode(context_str, 'unicode_escape')
                    except (UnicodeDecodeError, ValueError):
                        pass
                ch.context = context_str[:200]
            elif ch.text:
                ch.context = ch.text[:200]
            else:
                ch.context = "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"

            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if "geo" in enriched:
                ch.geo = enriched["geo"]
            if "metrics" in enriched:
                ch.metrics = self._validator.normalize_metrics(enriched["metrics"])
            if "years" in enriched:
                ch.years = self._validator.normalize_years(enriched["years"])
            if "time_granularity" in enriched:
                ch.time_granularity = enriched["time_granularity"]
            if "oked" in enriched:
                ch.oked = enriched["oked"]

            enriched_chunks.append(ch)

        return enriched_chunks

    @staticmethod
    def _parse_llm_batch_enrichment(raw: str) -> List[Dict[str, Any]]:
        """
        –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–µ—Ä JSON-–æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM –¥–ª—è –±–∞—Ç—á–µ–≤–æ–≥–æ enrichment.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤.
        """
        if not raw:
            return []

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£–¥–∞–ª–µ–Ω–∏–µ markdown code blocks
        cleaned = raw.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()

        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        while cleaned and cleaned[0] in [' ', '\n', '\r', '\t']:
            cleaned = cleaned[1:]

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü—Ä—è–º–æ–π –ø–∞—Ä—Å–∏–Ω–≥
        try:
            data = json.loads(cleaned)
            if isinstance(data, list):
                return LLMEnricher._validate_and_fix_enrichment_data(data)
            elif isinstance(data, dict):
                for key in ["chunks", "data", "results", "items", "array"]:
                    if key in data and isinstance(data[key], list):
                        return LLMEnricher._validate_and_fix_enrichment_data(data[key])
                if "chunk_id" in data or any(
                    key in data for key in ["context", "geo", "metrics", "years"]
                ):
                    return LLMEnricher._validate_and_fix_enrichment_data([data])
        except json.JSONDecodeError:
            try:
                decoded = codecs.decode(cleaned, 'unicode_escape')
                data = json.loads(decoded)
                if isinstance(data, list):
                    return LLMEnricher._validate_and_fix_enrichment_data(data)
                elif isinstance(data, dict):
                    if "chunk_id" in data or any(
                        key in data for key in ["context", "geo", "metrics", "years"]
                    ):
                        return LLMEnricher._validate_and_fix_enrichment_data([data])
            except (json.JSONDecodeError, UnicodeDecodeError, ValueError):
                pass

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ JSON-–º–∞—Å—Å–∏–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        start = cleaned.find("[")
        end = cleaned.rfind("]")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            try:
                data = json.loads(snippet)
                if isinstance(data, list):
                    return LLMEnricher._validate_and_fix_enrichment_data(data)
            except json.JSONDecodeError:
                pass

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö JSON –æ–±—ä–µ–∫—Ç–æ–≤ (–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        objects = []
        i = 0
        while i < len(cleaned):
            if cleaned[i] == '{':
                depth = 0
                j = i
                while j < len(cleaned):
                    if cleaned[j] == '{':
                        depth += 1
                    elif cleaned[j] == '}':
                        depth -= 1
                        if depth == 0:
                            try:
                                obj_str = cleaned[i:j+1]
                                obj = json.loads(obj_str)
                                if isinstance(obj, dict):
                                    if "chunk_id" in obj or any(
                                        key in obj
                                        for key in ["context", "geo", "metrics", "years"]
                                    ):
                                        objects.append(obj)
                            except json.JSONDecodeError:
                                pass
                            i = j + 1
                            break
                    j += 1
                else:
                    i += 1
            else:
                i += 1

        if objects:
            return LLMEnricher._validate_and_fix_enrichment_data(objects)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü–æ–∏—Å–∫ –æ–¥–Ω–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç–∞
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            try:
                data = json.loads(snippet)
                if isinstance(data, dict):
                    for key in ["chunks", "data", "results", "items", "array"]:
                        if key in data and isinstance(data[key], list):
                            return LLMEnricher._validate_and_fix_enrichment_data(
                                data[key]
                            )
                    if "chunk_id" in data or any(
                        key in data for key in ["context", "geo", "metrics", "years"]
                    ):
                        return LLMEnricher._validate_and_fix_enrichment_data([data])
            except json.JSONDecodeError:
                pass

        print(
            f"‚ö†Ô∏è  WARNING: –ù–µ –Ω–∞–π–¥–µ–Ω JSON-–º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ LLM. "
            f"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤: {raw[:500]}"
        )
        return []

    @staticmethod
    def _validate_and_fix_enrichment_data(
        data: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è."""
        if not isinstance(data, list):
            print(f"‚ö†Ô∏è  WARNING: –ü–∞—Ä—Å–µ—Ä –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫, –∞ {type(data).__name__}")
            return []

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏
        valid_items = []
        for item in data:
            if isinstance(item, dict):
                valid_items.append(item)

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –ø–æ–ª—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        required_fields = [
            "chunk_id",
            "context",
            "geo",
            "metrics",
            "years",
            "time_granularity",
            "oked",
        ]
        for item in valid_items:
            for field in required_fields:
                if field not in item:
                    item[field] = None

        return valid_items
