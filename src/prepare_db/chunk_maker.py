# src/prepare_db/chunk_maker.py
import json
import re
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass

import numpy as np
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer

from src.main.input_normalizer import normalize_text_lemmatized
from src.main.vectorizer import HashVectorizer


@dataclass
class BuildArtifacts:
    index_path: Path
    metadata_path: Path
    data_path: Path


class ChunkMaker:
    """
    ChunkMaker –¥–ª—è –¢–ê–ë–õ–ò–¶: –∏—â–µ—Ç –±–ª–æ–∫–∏ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü
    –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–≥–æ–ª–æ–≤–∫—É (–∏–ª–∏ —Ç–µ–∫—Å—Ç—É).
    """

    AUTHOR_RE = re.compile(r"(^[–ê-–Ø–Å][\w\-]+(?:\s+[–ê-–Ø]\.){1,3}|^[A-Z]\.[A-Z]\.|—Ä–µ–¥–∞–∫—Ü–∏|¬©|–∏–∑–¥–∞—Ç–µ–ª—å|–∏–∑–¥–∞–Ω–∏–µ|—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç)", re.IGNORECASE)
    NUMBER_RE = re.compile(r"[-+]?\d[\d\s\.,]*\d|^\d+$")
    MULTI_SPACES = re.compile(r"\s{2,}|\t")
    WORD_RE = re.compile(r"[–∞-—è—ëa-z]+", re.IGNORECASE)

    def __init__(
        self,
        vectorizer: HashVectorizer,
        documents_dir: Path,
        # —Ç–∞–±–ª–∏—Ü–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è, –µ—Å–ª–∏ –ø–æ–¥—Ä—è–¥ –Ω–∞–π–¥–µ–Ω–æ min_rows —Å—Ç—Ä–æ–∫ —Å —á–∏—Å–ª–∞–º–∏/—Ç–∞–±–ª–∏—á–Ω—ã–º –≤–∏–¥–æ–º
        min_rows: int = 2,
        # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–æ–ª–æ–Ω–æ–∫ (–æ—Ü–µ–Ω–∫–∞ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é) —á—Ç–æ–±—ã –Ω–µ –±—Ä–∞—Ç—å –æ–¥–∏–Ω —Å—Ç–æ–ª–±–∏–∫
        min_cols: int = 2,
        # lookback –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (—Å—Ç—Ä–æ–∫ –ø–µ—Ä–µ–¥ —Ç–∞–±–ª–∏—Ü–µ–π)
        lookback_title_lines: int = 6,
        # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ (–µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–æ—Ä–æ—Ç–∫–∏–π ‚Äî –º–æ–∂–µ—Ç –±—ã—Ç—å –º—É—Å–æ—Ä–æ–º)
        min_title_words: int = 2,
        # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –∏—Ç–æ–≥–æ–≤–æ–º –∑–∞–≥–æ–ª–æ–≤–∫–µ (—É—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ)
        max_title_words: int = 20,
    ):
        self.vectorizer = vectorizer
        self.documents_dir = Path(documents_dir)
        self.min_rows = min_rows
        self.min_cols = min_cols
        self.lookback_title_lines = lookback_title_lines
        self.min_title_words = min_title_words
        self.max_title_words = max_title_words

    # -----------------------------

    def build_tables_from_pdfs(self, output_dir: Path) -> BuildArtifacts:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        tables: List[Dict] = []
        embeddings: List[np.ndarray] = []
        table_id = 0

        for pdf_path in sorted(self.documents_dir.glob("*.pdf")):
            print(f"üìò –û–±—Ä–∞–±–æ—Ç–∫–∞: {pdf_path.name}")

            for page_num, page_layout in enumerate(extract_pages(pdf_path), start=1):
                # —Å–æ–±–µ—Ä—ë–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
                page_blocks: List[str] = []
                for element in page_layout:
                    if isinstance(element, LTTextContainer):
                        text = element.get_text()
                        if text and text.strip():
                            page_blocks.append(text)

                if not page_blocks:
                    continue

                lines = self._blocks_to_lines(page_blocks)
                # detect table regions (start_idx, end_idx)
                table_regions = self._detect_table_regions(lines)

                for start_idx, end_idx in table_regions:
                    table_lines = lines[start_idx:end_idx]
                    # estimate columns
                    est_cols = self._estimate_columns(table_lines)
                    if est_cols < self.min_cols:
                        # —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –∫–æ–ª–æ–Ω–æ–∫ ‚Äî –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ —Ç–∞–±–ª–∏—Ü–∞
                        continue

                    title = self._find_title(lines, start_idx, page_top_lines=page_blocks[:3])
                    # fallback: take first non-empty line in table if no title
                    if not title:
                        # try first non-empty row that looks like a header (letters)
                        for ln in table_lines[:3]:
                            if self.WORD_RE.search(ln) and len(ln.split()) >= self.min_title_words:
                                title = ln.strip()
                                break

                    raw_table_text = "\n".join(table_lines).strip()

                    # normalized title or fallback normalized text
                    normalized_title = normalize_text_lemmatized(title) if title else ""
                    if not normalized_title:
                        normalized_title = normalize_text_lemmatized(raw_table_text)

                    if not normalized_title:
                        # nothing to embed
                        continue

                    emb = self.vectorizer.embed(normalized_title)

                    tables.append(
                        {
                            "id": table_id,
                            "title": self._clean_title(title) if title else "",
                            "normalized": normalized_title,
                            "text": raw_table_text,
                            "source": pdf_path.name,
                            "page": page_num,
                            "rows": [self._split_row_to_cells(r) for r in table_lines],
                            "est_columns": est_cols,
                        }
                    )

                    embeddings.append(emb)
                    table_id += 1

        if not embeddings:
            embeddings_np = np.zeros((0, self.vectorizer.dimension), dtype=np.float32)
        else:
            embeddings_np = np.vstack([np.asarray(e, dtype=np.float32).reshape(1, -1) for e in embeddings]).astype(np.float32)

        index_path = output_dir / "index.npy"
        data_path = output_dir / "data.json"
        meta_path = output_dir / "metadata.json"

        np.save(index_path, embeddings_np)

        with open(data_path, "w", encoding="utf-8") as f:
            json.dump(tables, f, ensure_ascii=False, indent=2)

        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "vectorizer": type(self.vectorizer).__name__,
                    "dimension": self.vectorizer.dimension,
                    "tables": len(tables),
                },
                f,
                ensure_ascii=False,
                indent=2,
            )

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
        return BuildArtifacts(index_path=index_path, metadata_path=meta_path, data_path=data_path)

    # -----------------------------

    def _blocks_to_lines(self, blocks: List[str]) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ–º –±–ª–æ–∫–∏ –Ω–∞ —Å—Ç—Ä–æ–∫–∏, —á–∏—Å—Ç–∏–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã.
        –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫.
        """
        lines: List[str] = []
        for block in blocks:
            for ln in block.splitlines():
                ln2 = ln.strip()
                if not ln2:
                    continue
                # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã (—Ç–∞–±–ª–∏—á–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —á–∞—Å—Ç–æ ‚Äî –º–Ω. –ø—Ä–æ–±–µ–ª—ã)
                ln2 = self.MULTI_SPACES.sub("  ", ln2)  # –¥–≤–æ–π–Ω–æ–π –ø—Ä–æ–±–µ–ª –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ split
                lines.append(ln2)
        return lines

    def _detect_table_regions(self, lines: List[str]) -> List[tuple]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (start_idx, end_idx) –±–ª–æ–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è —Ç–∞–±–ª–∏—Ü–∞–º–∏.
        –ö—Ä–∏—Ç–µ—Ä–∏–π: –ø–æ–¥—Ä—è–¥ min_rows –ª–∏–Ω–∏–π, –≥–¥–µ –µ—Å—Ç—å —á–∏—Å–ª–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã / —Ç–∞–±–ª–∏—Ü–æ-–ø–æ–¥–æ–±–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞.
        """
        regions = []
        n = len(lines)
        i = 0
        while i < n:
            # –∏—â–µ–º –Ω–∞—á–∞–ª–æ –ø–æ—Ç–µ–Ω—Ü. —Ç–∞–±–ª–∏—Ü—ã
            if self._is_table_like_line(lines[i]):
                # —Ä–∞—Å—Ç—ë–º –≤–Ω–∏–∑, –ø–æ–∫–∞ –∏–¥—É—Ç table-like –ª–∏–Ω–∏–∏
                j = i + 1
                while j < n and self._is_table_like_line(lines[j]):
                    j += 1
                # –¥–ª–∏–Ω–∞ –±–ª–æ–∫–∞
                block_len = j - i
                if block_len >= self.min_rows:
                    regions.append((i, j))
                    i = j
                    continue
            i += 1
        return regions

    def _is_table_like_line(self, line: str) -> bool:
        """
        –õ–∏–Ω–∏—è '—Ç–∞–±–ª–∏—á–Ω–∞', –µ—Å–ª–∏:
         - —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–∞ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —è—á–µ–π–∫–∞—Ö, –∏–ª–∏
         - —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ (–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã), –∏–ª–∏
         - —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ —Ü–∏—Ñ—Ä –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–ª–æ–≤
        """
        if not line or len(line) < 3:
            return False

        # count numeric tokens
        numeric_tokens = re.findall(r"[-+]?\d[\d\.,]*", line)
        words = re.findall(r"\w+", line)
        numeric_count = len(numeric_tokens)
        total_tokens = max(len(words), 1)
        numeric_ratio = numeric_count / total_tokens

        # check separators (–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã or tabs) ‚Äî –ø—Ä–∏–∑–Ω–∞–∫ –∫–æ–ª–æ–Ω–æ–∫
        col_splits = re.split(r"\s{2,}|\t", line)
        est_cols = len([s for s in col_splits if s.strip() != ""])

        # heuristics
        if numeric_count >= 2:
            return True
        if numeric_ratio >= 0.35 and total_tokens >= 3:
            return True
        if est_cols >= self.min_cols:
            return True

        return False

    def _estimate_columns(self, lines: List[str]) -> int:
        max_cols = 0
        for ln in lines:
            parts = re.split(r"\s{2,}|\t", ln)
            non_empty = [p for p in parts if p.strip()]
            max_cols = max(max_cols, len(non_empty))
        return max_cols

    def _find_title(self, lines: List[str], table_start_idx: int, page_top_lines: List[str] = None) -> Optional[str]:
        """
        –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã:
         1) –í–µ—Ä—Ö–Ω—è—è —Å—Ç—Ä–æ–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–µ—Å–ª–∏ UPPERCASE –∏ –Ω–µ –∞–≤—Ç–æ—Ä—Å–∫–∞—è/–∏–∑–¥–∞—Ç–µ–ª—å—Å–∫–∞—è)
         2) –ù–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ç–∞–±–ª–∏—Ü—ã (lookback_title_lines),
            –±–µ—Ä—ë–º —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ—á–Ω—ã–µ (–Ω–µ –∞–≤—Ç–æ—Ä, –Ω–µ –∫–æ—Ä–æ—Ç–∫–∏–π –º—É—Å–æ—Ä)
         3) –í –∫–∞—á–µ—Å—Ç–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–∏ ‚Äî –ª—é–±–∞—è –±–ª–∏–∂–∞–π—à–∞—è –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –±—É–∫–≤–∞–º–∏
        """
        # 1) –≤–µ—Ä—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if page_top_lines:
            for block in page_top_lines:
                first_line = block.splitlines()[0].strip() if block else ""
                if first_line and first_line.isupper() and self.WORD_RE.search(first_line):
                    if not self._is_author_or_publisher_line(first_line):
                        return first_line

        # 2) —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ —Ç–∞–±–ª–∏—Ü–µ–π
        start_search = max(0, table_start_idx - self.lookback_title_lines)
        candidate_lines = []
        for i in range(start_search, table_start_idx):
            ln = lines[i].strip()
            if not ln:
                continue
            if self._is_author_or_publisher_line(ln):
                continue
            # —É–±–∏—Ä–∞–µ–º –ª–∏–Ω–∏–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ –ø—É–Ω–∫—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä "1.", "2.")
            if re.fullmatch(r"^[\d\.\)\-]+$", ln):
                continue
            # clean numeric-only short lines
            words = ln.split()
            # ignore if mostly digits
            digits = sum(1 for w in words if self.NUMBER_RE.search(w))
            if digits >= len(words):
                continue
            # accept if has letters and not too short
            if self.WORD_RE.search(ln) and len(words) >= 1:
                candidate_lines.append(ln)

        if candidate_lines:
            # keep only meaningful ones, drop too short
            filtered = [c for c in candidate_lines if len(c.split()) >= self.min_title_words]
            if not filtered:
                filtered = candidate_lines[-1:]  # last resort: last candidate
            # join but limit words
            joined = " | ".join(filtered)
            joined = self._truncate_title(joined)
            return joined

        # 3) fallback: nearest previous non-empty non-author line
        for i in range(table_start_idx - 1, max(-1, table_start_idx - self.lookback_title_lines - 1), -1):
            ln = lines[i].strip()
            if ln and not self._is_author_or_publisher_line(ln) and self.WORD_RE.search(ln):
                return self._truncate_title(ln)

        return None

    def _is_author_or_publisher_line(self, line: str) -> bool:
        # –∞–≤—Ç–æ—Ä—Å–∫–∏–µ –ø–æ–¥–ø–∏—Å–∏: –º–Ω–æ–≥–æ –∑–∞–ø—è—Ç—ã—Ö, –∏–Ω–∏—Ü–∏–∞–ª—ã –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ "—Ä–µ–¥–∞–∫—Ü–∏—è", "–∏–∑–¥–∞–Ω–∏–µ"
        if not line:
            return True
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–Ω–∏—Ü–∏–∞–ª—ã –≤–∏–¥–∞ "–ò.–û.–§–∞–º–∏–ª–∏—è" –∏–ª–∏ "–§–∞–º–∏–ª–∏—è –ò.–û."
        if re.search(r"\b[–ê-–Ø]\.[–ê-–Ø]\.", line):
            return True
        if re.search(r"\b[–ê-–Ø]\.[–ê-–Ø]\.[–ê-–Ø]\.", line):
            return True
        # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ —Å –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏, —á–∞—Å—Ç–æ —Ñ–∞–º–∏–ª–∏–∏/–∏–Ω–∏—Ü–∏–∞–ª—ã
        if len(line) < 4 and re.fullmatch(r"[–ê-–Ø–ÅA-Z\.\-]+", line):
            return True
        # –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞/—Ä–µ–¥–∞–∫—Ü–∏–∏
        if re.search(r"(—Ä–µ–¥–∞–∫—Ü–∏|–∏–∑–¥–∞–Ω–∏–µ|¬©|–ø—Ä–∏ —É—á–∞—Å—Ç–∏–∏|–∞–≤—Ç–æ—Ä—ã|–ø–æ–¥ —Ä–µ–¥|–æ—Ç–≤\.|–∏–∑–¥–∞—Ç–µ–ª—å)", line, flags=re.IGNORECASE):
            return True
        # If many commas and capitals (author list)
        if line.count(",") >= 2 and re.search(r"[–ê-–Ø]\w+", line):
            return True
        # heuristics by regex
        if self.AUTHOR_RE.search(line):
            return True
        return False

    def _truncate_title(self, title: str) -> str:
        words = title.split()
        if len(words) <= self.max_title_words:
            return title.strip()
        return " ".join(words[: self.max_title_words]) + " ‚Ä¶"

    def _clean_title(self, title: str) -> str:
        if not title:
            return ""
        t = title.strip()
        # remove leading numbering like "1.8." or "1)"
        t = re.sub(r"^[\d\.\)\-]+\s*", "", t)
        # replace multiple separators
        t = re.sub(r"\s*\|\s*", " | ", t)
        t = re.sub(r"\s+", " ", t)
        # remove trailing page footers like "–ù–ê–¶–ò–û–ù–ê–õ–¨–ù–´–ï –°–ß–ï–¢–ê –†–ï–°–ü–£–ë–õ–ò–ö–ò –ë–ï–õ–ê–†–£–°–¨, 2018 ‚Äì 2023 27"
        t = re.sub(r"\s+\d{1,4}$", "", t)
        return t.strip()

    def _split_row_to_cells(self, row: str) -> List[str]:
        # split by 2+ spaces or tab, fallback to single space
        parts = re.split(r"\s{2,}|\t", row)
        parts = [p.strip() for p in parts if p.strip() != ""]
        if len(parts) <= 1:
            # fallback try splitting by single space but keep multi-digit groups together
            parts = [p.strip() for p in re.split(r"\s+", row) if p.strip() != ""]
        return parts
