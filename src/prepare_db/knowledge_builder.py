from __future__ import annotations

"""
PIPELINE 1: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (prepare_db).

–ó–∞–¥–∞—á–∏ (–≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π skeleton):
1. –í–∑—è—Ç—å PDF –∏–∑ src/prepare_db/documents/
2. –†–∞–∑–±–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–æ–º–æ—â—å—é LlamaIndex
3. –ü–µ—Ä–µ–¥–∞—Ç—å –í–°–ï —á–∞–Ω–∫–∏ –≤ LLM (Ollama) –û–î–ù–ò–ú –ó–ê–ü–†–û–°–û–ú
   –∏ –ø–æ–ª—É—á–∏—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞:
   - context (–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
   - geo / metrics / years / time_granularity / oked
4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–∞–Ω–∫–∏ –≤ data.json
5. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å embedding –¢–û–õ–¨–ö–û –¥–ª—è –ø–æ–ª—è context
6. –ó–∞–≥—Ä—É–∑–∏—Ç—å embeddings –≤ FAISS (index.faiss)

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ –æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ skeleton, —á—Ç–æ–±—ã:
- –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
- –Ω–µ —Å–º–µ—à–∏–≤–∞—Ç—å –∑–¥–µ—Å—å —Å–µ—Ç–µ–≤–æ–π/IO‚Äë–∫–æ–¥ —Å –±–∏–∑–Ω–µ—Å‚Äë–ª–æ–≥–∏–∫–æ–π
"""

from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
import shutil
import faiss
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import time
import threading

from llama_index.readers.file import PDFReader
from llama_index.core.node_parser import SentenceSplitter

from src.main.models import Chunk
from src.main.ollama_client import OllamaClient
from src.main.vectorizer import HashVectorizer
from src.logs.logger import get_logger


@dataclass
class BuildConfig:
    documents_dir: Path
    output_dir: Path
    vector_dim: int = 256


class KnowledgeBaseBuilder:
    """
    –í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —Ñ–∞—Å–∞–¥ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

    –í–ê–ñ–ù–û:
    - —ç—Ç–æ—Ç –∫–ª–∞—Å—Å –ù–ï –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º PDF (site_parser.py –æ—Å—Ç–∞—ë—Ç—Å—è –∑–∞–≥–ª—É—à–∫–æ–π)
    - LlamaIndex –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∞ –∫–∞–∫ TODO‚Äë–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (—Å–∫–µ–ª–µ—Ç)
    """

    def __init__(self, config: BuildConfig, llm_client: OllamaClient | None = None):
        self._config = config
        self._llm = llm_client or OllamaClient()
        self._vectorizer = HashVectorizer(dimension=config.vector_dim)
        self._logger = get_logger()

    # -------------------- –ü—É–±–ª–∏—á–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å -------------------- #

    def build(self) -> None:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π entrypoint –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

        –†–µ–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞:
        - –ø—Ä–æ–π—Ç–∏ –ø–æ –≤—Å–µ–º PDF
        - –≤—ã–∑–≤–∞—Ç—å `_chunk_pdf_with_llamaindex`
        - –∑–∞—Ç–µ–º `_enrich_chunks_with_llm_batch`
        - —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON –∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å FAISS‚Äë–∏–Ω–¥–µ–∫—Å
        """
        self._config.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        self._logger.log_prepare_db("start", total_pdfs=len(list(self._config.documents_dir.glob("*.pdf"))))
        build_start_time = time.time()

        all_chunks: List[Chunk] = []
        chunk_id_counter = 0

        for pdf_path in sorted(self._config.documents_dir.glob("*.pdf")):
            pdf_start_time = time.time()
            self._logger.log_prepare_db("pdf_start", pdf_name=pdf_path.name)
            
            # 1‚Äì2. –ß–∞–Ω–∫–∏–Ω–≥ PDF —á–µ—Ä–µ–∑ LlamaIndex (skeleton)
            raw_chunks = self._chunk_pdf_with_llamaindex(pdf_path)

            # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º id –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞
            for ch in raw_chunks:
                ch.id = f"{pdf_path.name}::page{ch.page}::chunk{chunk_id_counter}"
                chunk_id_counter += 1

            # 3. LLM‚Äëenrichment –¥–ª—è —á–∞–Ω–∫–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            # –ü–µ—Ä–µ–¥–∞–µ–º all_chunks –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            enriched_chunks = self._enrich_chunks_with_llm_batch(
                pdf_path.name,
                raw_chunks,
                all_chunks=all_chunks + raw_chunks  # –í—Å–µ —á–∞–Ω–∫–∏ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ PDF + —Ç–µ–∫—É—â–∏–µ
            )
            all_chunks.extend(enriched_chunks)
            
            pdf_elapsed = time.time() - pdf_start_time
            self._logger.log_prepare_db(
                "pdf_end",
                pdf_name=pdf_path.name,
                chunks_count=len(enriched_chunks),
                elapsed_time=pdf_elapsed
            )

        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ data.json
        data_json_path = self._config.output_dir / "data.json"
        with open(data_json_path, "w", encoding="utf-8") as f:
            json.dump(
                [
                    {
                        "id": ch.id,
                        "context": ch.context,
                        "text": ch.text,
                        "source": ch.source,
                        "page": ch.page,
                        "geo": ch.geo,
                        "metrics": ch.metrics,
                        "years": ch.years,
                        "time_granularity": ch.time_granularity,
                        "oked": ch.oked,
                    }
                    for ch in all_chunks
                ],
                f,
                ensure_ascii=False,
                indent=2,
            )

        # 5‚Äì6. Embeddings –ø–æ context + FAISS
        self._build_faiss_index(all_chunks, self._config.output_dir / "index.faiss")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –º–µ—Ç–∞‚Äë–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        meta_path = self._config.output_dir / "metadata.json"
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "vectorizer": type(self._vectorizer).__name__,
                    "dimension": self._vectorizer.dimension,
                    "chunks": len(all_chunks),
                },
                f,
                ensure_ascii=False,
                indent=2,
            )
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞
        build_elapsed = time.time() - build_start_time
        self._logger.log_prepare_db(
            "end",
            total_chunks=len(all_chunks),
            elapsed_time=build_elapsed
        )

    # -------------------- Skeleton‚Äë–º–µ—Ç–æ–¥—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ -------------------- #

    def _chunk_pdf_with_llamaindex(self, pdf_path: Path) -> List[Chunk]:
        """
        –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LlamaIndex –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞ PDF.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
        - PDFReader –¥–ª—è —á—Ç–µ–Ω–∏—è PDF
        - SimpleNodeParser –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏ (chunk_size=512, chunk_overlap=50)
        - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ node.metadata

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ Chunk —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏:
        - text (—Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞)
        - source (–∏–º—è PDF)
        - page (–Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
        - context –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ–∫–∞ –ø—É—Å—Ç—ã–µ (–∑–∞–ø–æ–ª–Ω—è—Ç—Å—è –≤ _enrich_chunks_with_llm_batch)
        """
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –æ–¥–Ω–∏–º PDF –¥–ª—è LlamaIndex
        # (PDFReader —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º–∏)
        temp_dir = pdf_path.parent / f"_temp_{pdf_path.stem}"
        temp_dir.mkdir(exist_ok=True)
        temp_pdf = temp_dir / pdf_path.name

        try:
            # –ö–æ–ø–∏—Ä—É–µ–º PDF –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            shutil.copy2(pdf_path, temp_pdf)

            # –ß–∏—Ç–∞–µ–º PDF —á–µ—Ä–µ–∑ LlamaIndex
            pdf_reader = PDFReader()
            documents = pdf_reader.load_data(file=str(temp_pdf))

            # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –Ω–æ–¥—ã (—á–∞–Ω–∫–∏)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—É–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            node_parser = SentenceSplitter(
                chunk_size=1500,
                chunk_overlap=200,
                paragraph_separator="\n\n",
            )

            chunks: List[Chunk] = []
            for doc in documents:
                nodes = node_parser.get_nodes_from_documents([doc])

                for node in nodes:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    # LlamaIndex –æ–±—ã—á–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç page_label –∏–ª–∏ page_number
                    page_num = 0
                    if hasattr(node, "metadata"):
                        page_num = node.metadata.get("page_label", 0)
                        if not page_num:
                            page_num = node.metadata.get("page_number", 0)
                        if not page_num:
                            page_num = node.metadata.get("page", 0)
                        try:
                            page_num = int(page_num) if page_num else 0
                        except (ValueError, TypeError):
                            page_num = 0

                    # –°–æ–∑–¥–∞—ë–º Chunk —Å –ø—É—Å—Ç—ã–º–∏ –ø–æ–ª—è–º–∏ –¥–ª—è enrichment
                    chunk = Chunk(
                        id="",  # –±—É–¥–µ—Ç –ø—Ä–∏—Å–≤–æ–µ–Ω –≤ build()
                        context="",  # –∑–∞–ø–æ–ª–Ω–∏—Ç—Å—è –≤ _enrich_chunks_with_llm_batch
                        text=node.text or "",
                        source=pdf_path.name,
                        page=page_num,
                        geo=None,
                        metrics=None,
                        years=None,
                        time_granularity=None,
                        oked=None,
                    )
                    chunks.append(chunk)

        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            if temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)

        return chunks

    def _enrich_chunks_with_llm_batch(self, pdf_name: str, chunks: List[Chunk], all_chunks: List[Chunk] | None = None) -> List[Chunk]:
        """
        –ë–∞—Ç—á–µ–≤–æ–µ LLM‚Äë–æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.

        –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è:
        - 1 —á–∞–Ω–∫–∞ –¥–æ –∏ 1 —á–∞–Ω–∫–∞ –ø–æ—Å–ª–µ —Ç–µ–∫—É—â–µ–≥–æ
        - –¢–µ–∫—É—â–∏–π —á–∞–Ω–∫ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è

        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –±–∞—Ç—á–∞–º–∏ (–æ–¥–∏–Ω LLM-–∑–∞–ø—Ä–æ—Å –Ω–∞ N —á–∞–Ω–∫–æ–≤), —á—Ç–æ–±—ã
        —Ä–µ–∑–∫–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Ollama –∏ –ø–æ–ª—É—á–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å 1600‚Äì3200 —á–∞–Ω–∫–æ–≤/—á–∞—Å.
        –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –±–∞—Ç—á–µ–π (–æ–±—ã—á–Ω–æ 1‚Äì2 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞).
        –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç LLM –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü 1-3 (—Ç–æ–ª—å–∫–æ –æ–±–ª–æ–∂–∫–∞ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ).
        """
        if not chunks:
            return []
        
        
        # –†–∞–∑–¥–µ–ª—è–µ–º —á–∞–Ω–∫–∏ –Ω–∞ —Ç–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å LLM –∏ —Ç–µ, —á—Ç–æ –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
        chunks_to_process: List[Chunk] = []
        chunks_to_skip: List[Chunk] = []
        
        for chunk in chunks:
            if chunk.page <= 3:
                # –î–ª—è –ø–µ—Ä–≤—ã—Ö —Ç—Ä—ë—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º text –∫–∞–∫ context
                chunk.context = chunk.text[:200] if chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                chunks_to_skip.append(chunk)
            else:
                chunks_to_process.append(chunk)
        
        if not chunks_to_process:
            return chunks
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏—è/–ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ —á–µ—Ä–µ–∑ env:
        # - RAG_ENRICH_BATCH_SIZE: —Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –≤ –æ–¥–Ω–æ–º LLM-–∑–∞–ø—Ä–æ—Å–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)
        # - RAG_ENRICH_CONCURRENCY: —Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
        #
        # –í–ê–ñ–ù–û: –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ (llama3.2:3b) –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å –º–µ–Ω—å—à–∏–º–∏ –±–∞—Ç—á–∞–º–∏ (5 –≤–º–µ—Å—Ç–æ 10)
        # –≠—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å–Ω–∏–∂–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–∞–π–º–∞—É—Ç–æ–≤
        batch_size = int(os.getenv("RAG_ENRICH_BATCH_SIZE", "5"))  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 10 –¥–æ 5 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        batch_concurrency = int(os.getenv("RAG_ENRICH_CONCURRENCY", "1"))  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 1 –¥–æ 2 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        batch_size = max(1, batch_size)
        batch_concurrency = max(1, min(batch_concurrency, 8))  # –ú–∞–∫—Å–∏–º—É–º 4 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞

        total_batches = (len(chunks_to_process) + batch_size - 1) // batch_size
        print(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(chunks_to_process)} —á–∞–Ω–∫–æ–≤ (–ø—Ä–æ–ø—É—â–µ–Ω–æ {len(chunks_to_skip)} —á–∞–Ω–∫–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü 1-3)")
        print(f"   –ë–∞—Ç—á–∏: —Ä–∞–∑–º–µ—Ä={batch_size}, –±–∞—Ç—á–µ–π={total_batches}, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å={batch_concurrency}")

        all_enriched_chunks: List[Chunk] = chunks_to_skip.copy()

        # –ü–∞—Ä–∞–ª–ª–µ–ª–∏–º —Ç–æ–ª—å–∫–æ –±–∞—Ç—á–∏ (–∞ –Ω–µ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫), —á—Ç–æ–±—ã –Ω–µ —É–±–∏—Ç—å Ollama —Å–æ—Ç–Ω—è–º–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        start_time = time.time()
        
        # –°—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–±—Ä–æ—Å–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–∞–∂–¥—ã–µ 50 —á–∞–Ω–∫–æ–≤
        chunks_since_reset = 0
        reset_interval = int(os.getenv("RAG_ENRICH_RESET_INTERVAL", "50"))  # –ö–∞–∂–¥—ã–µ 50 —á–∞–Ω–∫–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        all_chunks_ref = all_chunks if all_chunks is not None else chunks
        
        with ThreadPoolExecutor(max_workers=min(batch_concurrency, total_batches)) as executor:
            futures = []
            for start in range(0, len(chunks_to_process), batch_size):
                batch = chunks_to_process[start : start + batch_size]
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –±–∞—Ç—á–∞
                # –ë–µ—Ä–µ–º —á–∞–Ω–∫–∏ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ all_chunks_ref
                # –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –±–∞—Ç—á–∏ –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                previous_chunks_for_batch = None
                if start > 0:
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 —á–∞–Ω–∫–∞ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ chunks (–¥–æ —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞)
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫ all_chunks_ref, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∞–∂–µ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ
                    prev_end = start
                    prev_start = max(0, prev_end - 2)
                    # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —á–∞–Ω–∫–∏ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Å–ø–∏—Å–∫–µ chunks
                    prev_chunk_indices = []
                    for i in range(prev_start, prev_end):
                        if i < len(chunks_to_process):
                            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —á–∞–Ω–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Å–ø–∏—Å–∫–µ chunks
                            # chunks_to_process —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∞–Ω–∫–∏ –Ω–∞—á–∏–Ω–∞—è —Å –∏–Ω–¥–µ–∫—Å–∞ len(chunks_to_skip)
                            chunk_idx = len(chunks_to_skip) + i
                            if chunk_idx < len(all_chunks_ref):
                                prev_chunk_indices.append(chunk_idx)
                    
                    if prev_chunk_indices:
                        previous_chunks_for_batch = [all_chunks_ref[idx] for idx in prev_chunk_indices]
                
                fut = executor.submit(
                    self._enrich_single_batch,
                    pdf_name,
                    batch,
                    previous_chunks_for_batch
                )
                # —Å–æ—Ö—Ä–∞–Ω—è–µ–º, –∫–∞–∫–æ–π –±–∞—Ç—á —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç future + –∫–æ–≥–¥–∞ –æ–Ω –±—ã–ª –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω
                fut._rag_submit_ts = time.time()  # type: ignore[attr-defined]
                futures.append(fut)

            completed_batches = 0
            completed_chunks = 0
            batch_to_future = {}
            for i, fut in enumerate(futures):
                batch_start_idx = i * batch_size
                batch_end_idx = min(batch_start_idx + batch_size, len(chunks_to_process))
                batch_to_future[fut] = chunks_to_process[batch_start_idx:batch_end_idx]
            
            for fut in as_completed(futures):
                original_batch = batch_to_future[fut]
                try:
                    enriched_batch = fut.result()
                    submit_ts = getattr(fut, "_rag_submit_ts", None)
                    batch_time = (time.time() - submit_ts) if submit_ts else 0.0
                    all_enriched_chunks.extend(enriched_batch)
                    completed_batches += 1
                    completed_chunks += len(enriched_batch)
                    chunks_since_reset += len(enriched_batch)
                    
                    # –°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫–∞–∂–¥—ã–µ N —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    if chunks_since_reset >= reset_interval:
                        print(f"   üîÑ –°–±—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ {completed_chunks} —á–∞–Ω–∫–æ–≤...")
                        self._llm.reset_context()
                        chunks_since_reset = 0
                        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ —Å–±—Ä–æ—Å–∞
                        time.sleep(0.5)
                    
                    elapsed = time.time() - start_time
                    rate = completed_chunks / elapsed * 3600 if elapsed > 0 else 0
                    print(f"   –ë–∞—Ç—á {completed_batches}/{total_batches}: {len(enriched_batch)} —á–∞–Ω–∫–æ–≤ –∑–∞ {batch_time:.1f}—Å | –í—Å–µ–≥–æ: {completed_chunks}/{len(chunks_to_process)} | –°–∫–æ—Ä–æ—Å—Ç—å: {rate:.0f} —á–∞–Ω–∫–æ–≤/—á–∞—Å")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ç—á–∞
                    self._logger.log_prepare_db(
                        "batch",
                        pdf_name=pdf_name,
                        batch_number=completed_batches,
                        total_batches=total_batches,
                        chunks_in_batch=len(enriched_batch),
                        elapsed_time=batch_time,
                        total_chunks_processed=completed_chunks,
                        rate_per_hour=rate
                    )
                except Exception as e:
                    submit_ts = getattr(fut, "_rag_submit_ts", None)
                    batch_time = (time.time() - submit_ts) if submit_ts else 0.0
                    completed_batches += 1
                    print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞ {completed_batches}/{total_batches} (–≤—Ä–µ–º—è: {batch_time:.1f}—Å): {e}")
                    import traceback
                    traceback.print_exc()
                    # Fallback: –¥–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏ –±–µ–∑ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
                    for ch in original_batch:
                        if not ch.context:
                            ch.context = ch.text[:200] if ch.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                    all_enriched_chunks.extend(original_batch)
                    completed_chunks += len(original_batch)
                    chunks_since_reset += len(original_batch)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É
        chunk_order = {ch.id: i for i, ch in enumerate(chunks)}
        all_enriched_chunks.sort(key=lambda ch: chunk_order.get(ch.id, 999999))
        
        return all_enriched_chunks
    
    
    def _enrich_single_with_context(self, pdf_name: str, chunk: Chunk, context_data: Dict[str, Any]) -> Chunk:
        """
        –û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
        """
        system_prompt = (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å —á–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏. "
            "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç (–Ω–µ –º–∞—Å—Å–∏–≤!) —Å –ø–æ–ª—è–º–∏: chunk_id, context, geo, metrics, years, time_granularity, oked."
        )
        
        prompt = (
            f"–î–æ–∫—É–º–µ–Ω—Ç: {pdf_name}\n\n"
            "–ö–û–ù–¢–ï–ö–°–¢ –î–û –¢–ï–ö–£–©–ï–ì–û –ß–ê–ù–ö–ê (1 –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —á–∞–Ω–∫–∞):\n"
            f"{json.dumps(context_data['before'], ensure_ascii=False, indent=2)}\n\n"
            "–¢–ï–ö–£–©–ò–ô –ß–ê–ù–ö –î–õ–Ø –û–ë–û–ì–ê–©–ï–ù–ò–Ø:\n"
            f"ID: {context_data['target']['chunk_id']}\n"
            f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {context_data['target']['page']}\n"
            f"–¢–µ–∫—Å—Ç: {context_data['target']['text']}\n\n"
            "–ö–û–ù–¢–ï–ö–°–¢ –ü–û–°–õ–ï –¢–ï–ö–£–©–ï–ì–û –ß–ê–ù–ö–ê (1 —Å–ª–µ–¥—É—é—â–∏—Ö —á–∞–Ω–∫–∞):\n"
            f"{json.dumps(context_data['after'], ensure_ascii=False, indent=2)}\n\n"
            "–ó–∞–¥–∞—á–∞: –æ–ø–∏—à–∏ –¢–ï–ö–£–©–ò–ô –ß–ê–ù–ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n"
            "–í–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç —Å –ø–æ–ª—è–º–∏:\n"
            "- chunk_id: —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ ID –≤—ã—à–µ\n"
            "- context: –∫—Ä–∞—Ç–∫–æ–µ, —Ç–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞–Ω–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –æ—Ç—Ä–∞–∂–∞—é—â–µ–µ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É –∏ –¥–∞–Ω–Ω—ã–µ\n"
            "- geo: –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞, –≥–æ—Ä–æ–¥–∞, –æ–±–ª–∞—Å—Ç–∏) –∏–ª–∏ null, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω\n"
            "- metrics: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–µ—Ç—Ä–∏–∫/–ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: ['—É–¥–æ–π –º–æ–ª–æ–∫–∞', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞–ø–∏—Ç–∞–ª', '–¥–æ–ª—è –¥–æ–º–∞—à–Ω–∏—Ö —Ö–æ–∑—è–π—Å—Ç–≤, –∏–º–µ—é—â–∏—Ö –∫–æ–º–ø—å—é—Ç–µ—Ä']) –∏–ª–∏ null. "
            "–ò–∑–≤–ª–µ–∫–∞–π —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π. –ö–∞–∂–¥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ.\n"
            "- years: —Å–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ (—Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä [2023, 2024]) –∏–ª–∏ null\n"
            "- time_granularity: 'year'/'quarter'/'month'/'day' –∏–ª–∏ null\n"
            "- oked: –∫–æ–¥ –û–ö–≠–î –∏–ª–∏ null\n\n"
            "–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç {}, –ù–ï –º–∞—Å—Å–∏–≤!"
        )
        
        try:
            raw_response = self._llm.generate(prompt, system_prompt=system_prompt, format="json")
            enriched_data = self._parse_llm_single_enrichment(raw_response, chunk.id)
            
            if enriched_data:
                # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞–Ω–∫ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç LLM
                if enriched_data.get("context"):
                    chunk.context = str(enriched_data["context"])[:200]
                elif chunk.text:
                    chunk.context = chunk.text[:200]
                else:
                    chunk.context = "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
                
                if "geo" in enriched_data:
                    chunk.geo = enriched_data["geo"]
                if "metrics" in enriched_data:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏: –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                    metrics = enriched_data["metrics"]
                    if metrics and isinstance(metrics, list):
                        normalized_metrics = []
                        for m in metrics:
                            if isinstance(m, str) and m.strip():
                                # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
                                normalized = m.strip().lower()
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç (—Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É)
                                if any('\u0400' <= char <= '\u04FF' for char in normalized):
                                    normalized_metrics.append(normalized)
                        chunk.metrics = normalized_metrics if normalized_metrics else None
                    else:
                        chunk.metrics = None
                if "years" in enriched_data:
                    chunk.years = self._normalize_years(enriched_data["years"])
                if "time_granularity" in enriched_data:
                    chunk.time_granularity = enriched_data["time_granularity"]
                if "oked" in enriched_data:
                    chunk.oked = enriched_data["oked"]
        except Exception as e:
            print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–∏ —á–∞–Ω–∫–∞ {chunk.id}: {e}")
            # –û—Å—Ç–∞–≤–ª—è–µ–º —á–∞–Ω–∫ –±–µ–∑ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            if not chunk.context:
                chunk.context = chunk.text[:200] if chunk.text else "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
        
        return chunk
    
    def _enrich_single_batch(
        self,
        pdf_name: str,
        chunks: List[Chunk],
        previous_chunks: List[Chunk] | None = None
    ) -> List[Chunk]:
        """
        –û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ —á–∞–Ω–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.
        
        Args:
            pdf_name: –ò–º—è PDF —Ñ–∞–π–ª–∞
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            previous_chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        if not chunks:
            return []

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –≤—Å–µ–º–∏ —á–∞–Ω–∫–∞–º–∏
        chunks_data = []
        for i, ch in enumerate(chunks):
            chunks_data.append({
                "chunk_id": ch.id,
                # –í–ê–ñ–ù–û: —Ä–µ–∂–µ–º —Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ —Å–Ω–∏–∑–∏—Ç—å –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
                # –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫/–ª–µ—Ç/–≥–µ–æ –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–µ—Ä–≤—ã—Ö ~350 —Å–∏–º–≤–æ–ª–æ–≤.
                "text": (ch.text or "")[:350],
                "page": ch.page,
            })
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        previous_context = None
        if previous_chunks:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1-2 —á–∞–Ω–∫–∞ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            prev_texts = []
            for prev_ch in previous_chunks[-2:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2 —á–∞–Ω–∫–∞
                if prev_ch.text:
                    prev_texts.append(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {prev_ch.page}: {prev_ch.text[:200]}")
            if prev_texts:
                previous_context = "\n".join(prev_texts)

        # System prompt –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ (–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è ‚Äî –∂—ë—Å—Ç–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
        system_prompt = (
            "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å. "
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–±–æ–≥–∞—Ç–∏—Ç—å —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏. "
            "STRICT RULES:- Output ONLY a valid JSON array of exactly 5 objects - No text before or after JSON - No markdown, comments, explanations - chunk_id must exactly match input"
            "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: context <= 180 —Å–∏–º–≤–æ–ª–æ–≤; metrics –º–∞–∫—Å–∏–º—É–º 3 —ç–ª–µ–º–µ–Ω—Ç–∞; years –º–∞–∫—Å–∏–º—É–º 4 —ç–ª–µ–º–µ–Ω—Ç–∞."
        )
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
        if len(chunks_data) == 1:
            # –î–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ - –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            chunk = chunks_data[0]
            context_section = ""
            if previous_context:
                context_section = (
                    f"\n–ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ò–• –°–¢–†–ê–ù–ò–¶ (–¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞):\n"
                    f"{previous_context}\n\n"
                )
            
            prompt = (
                f"–î–æ–∫—É–º–µ–Ω—Ç: {pdf_name}\n\n"
                f"{context_section}"
                f"–ß–∞–Ω–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n"
                f"ID: {chunk['chunk_id']}\n"
                f"–°—Ç—Ä–∞–Ω–∏—Ü–∞: {chunk['page']}\n"
                f"–¢–µ–∫—Å—Ç: {chunk['text'][:500]}...\n\n"
                "–í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ —Å –æ–¥–Ω–∏–º –æ–±—ä–µ–∫—Ç–æ–º. –§–æ—Ä–º–∞—Ç:\n"
                "[{\"chunk_id\":\"...\",\"context\":\"...\",\"geo\":null,\"metrics\":null,\"years\":null,\"time_granularity\":null,\"oked\":null}]\n\n"
                "–ü–æ–ª—è:\n"
                "- chunk_id: —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ ID –≤—ã—à–µ\n"
                "- context: –∫—Ä–∞—Ç–∫–æ–µ, —Ç–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞–Ω–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), "
                "–æ—Ç—Ä–∞–∂–∞—é—â–µ–µ —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —á–∞–Ω–∫–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü\n"
                "- geo: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞/–≥–æ—Ä–æ–¥–∞/–æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ null\n"
                "- metrics: —Å–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–º–∞–∫—Å–∏–º—É–º 3) –∏–ª–∏ null\n"
                "- years: —Å–ø–∏—Å–æ–∫ –≥–æ–¥–æ–≤ (–º–∞–∫—Å–∏–º—É–º 4) –∏–ª–∏ null\n"
                "- time_granularity: 'year'/'quarter'/'month'/'day' –∏–ª–∏ null\n"
                "- oked: –∫–æ–¥ –û–ö–≠–î –∏–ª–∏ null\n\n"
                "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –º–∞—Å—Å–∏–≤ [{}], –ù–ï –æ–±—ä–µ–∫—Ç {}!"
            )
        else:
            # –î–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤ ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —á–µ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç —Å –ø—Ä–∏–º–µ—Ä–æ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            context_section = ""
            if previous_context:
                context_section = (
                    f"\n–ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ò–• –°–¢–†–ê–ù–ò–¶ (–¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –æ–±—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞):\n"
                    f"{previous_context}\n\n"
                )
            
            prompt = (
                f"–î–æ–∫—É–º–µ–Ω—Ç: {pdf_name}\n\n"
                f"{context_section}"
                f"–û–±—Ä–∞–±–æ—Ç–∞–π {len(chunks_data)} —á–∞–Ω–∫–æ–≤. –í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ –∏–∑ –†–û–í–ù–û {len(chunks_data)} –æ–±—ä–µ–∫—Ç–æ–≤.\n\n"
                "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–∏–º–µ—Ä –¥–ª—è 2 —á–∞–Ω–∫–æ–≤):\n"
                "[{\"chunk_id\":\"doc.pdf::page1::chunk0\",\"context\":\"–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\",\"geo\":null,\"metrics\":[\"–º–µ—Ç—Ä–∏–∫–∞1\"],\"years\":[2024],\"time_granularity\":null,\"oked\":null},"
                "{\"chunk_id\":\"doc.pdf::page1::chunk1\",\"context\":\"–î—Ä—É–≥–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ\",\"geo\":\"–ú–∏–Ω—Å–∫\",\"metrics\":null,\"years\":null,\"time_granularity\":\"year\",\"oked\":null}]\n\n"
                "–ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞:\n"
                "- chunk_id: —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)\n"
                "- context: –∫—Ä–∞—Ç–∫–æ–µ, —Ç–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞–Ω–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), "
                "–æ—Ç—Ä–∞–∂–∞—é—â–µ–µ —á—Ç–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —á–∞–Ω–∫–µ —Å —É—á—ë—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–∞–ø—Ä–∏–º–µ—Ä, "
                "\"–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä—É–¥–∞ –ø–æ –í–í–ü, –í–†–ü, –í–í–ü, –Ω–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–Ω—è—Ç–æ–≥–æ –≤ —ç–∫–æ–Ω–æ–º–∏–∫–µ, —Ä—É–±–ª–µ–π –í–†–ü –Ω–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–Ω—è—Ç–æ–≥–æ –≤ —ç–∫–æ–Ω–æ–º–∏–∫–µ, —Ä—É–±–ª–µ–π\"\n"
                "- metrics: —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞, –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —Ä—É—Å—Å–∫–∏–π, –º–∞–∫—Å–∏–º—É–º 3, –∏–ª–∏ null\n"
                "- years: —Ç–æ–ª—å–∫–æ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞, –º–∞–∫—Å–∏–º—É–º 4, –∏–ª–∏ null\n"
                "- geo: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω–∞/–≥–æ—Ä–æ–¥–∞/–æ–±–ª–∞—Å—Ç–∏ –∏–ª–∏ null\n"
                "- time_granularity: 'year'/'quarter'/'month'/'day' –∏–ª–∏ null\n"
                "- oked: –∫–æ–¥ –û–ö–≠–î –∏–ª–∏ null\n\n"
                "–í—Ö–æ–¥–Ω—ã–µ —á–∞–Ω–∫–∏:\n"
                f"{json.dumps(chunks_data, ensure_ascii=False, separators=(',',':'))}\n\n"
                "–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:\n"
                "1. –í–µ—Ä–Ω–∏ –ú–ê–°–°–ò–í [{}, {}, ...], –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å '[' –∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è ']'\n"
                "2. –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–π –æ–±—ä–µ–∫—Ç {}\n"
                f"3. –í –º–∞—Å—Å–∏–≤–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –†–û–í–ù–û {len(chunks_data)} –æ–±—ä–µ–∫—Ç–æ–≤\n"
                "4. –ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å chunk_id –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
                "5. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (\\u0412) - –ø–∏—à–∏ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é"
            )

        # keep_alive: –¥–µ—Ä–∂–∏—Ç –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏ GPU –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –±–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏
        # "5m" = 5 –º–∏–Ω—É—Ç (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ä–µ–∑ RAG_OLLAMA_KEEP_ALIVE)
        keep_alive = os.getenv("RAG_OLLAMA_KEEP_ALIVE", "5m")
        
        req_options = {
            "temperature": 0,
            "top_p": 1,
            # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –≤—ã–≤–æ–¥–∞ (–≤–∞–∂–Ω–æ: –∏–Ω–∞—á–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç ‚Äú—Ä–∞–∑–ª–∏–≤–∞—Ç—å—Å—è‚Äù –Ω–∞ —Å–æ—Ç–Ω–∏ —Å–µ–∫—É–Ω–¥)
            "num_predict": min(250 * len(chunks_data) + 100, 3000),  # ~250 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —á–∞–Ω–∫, –º–∞–∫—Å 3000
        }

        # –õ–æ–≥–∏—Ä—É–µ–º, —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
        ollama_config = {
            "model": getattr(getattr(self._llm, "config", None), "model", None),
            "base_url": getattr(getattr(self._llm, "config", None), "base_url", None),
            "timeout": getattr(getattr(self._llm, "config", None), "timeout", None),
            "format": "json",
            "options": req_options,
        }
        
        self._logger.log_llm_enrichment(
            event="request",
            pdf_name=pdf_name,
            chunks_count=len(chunks_data),
            chunk_ids=[c["chunk_id"] for c in chunks_data],
            pages=[c["page"] for c in chunks_data],
            system_prompt=system_prompt,
            prompt=prompt,
            ollama_config=ollama_config,
        )

        # –í—ã–∑—ã–≤–∞–µ–º LLM —Å —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–∏–µ–º JSON —Ñ–æ—Ä–º–∞—Ç–∞
        # Ollama –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä format –¥–ª—è —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–∏—è JSON
        # –î–µ–ª–∞–µ–º –¥–æ 2 –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –ø–ª–æ—Ö–æ–º –∫–∞—á–µ—Å—Ç–≤–µ –æ—Ç–≤–µ—Ç–∞
        max_retries_for_quality = 2
        enriched_data = []
        raw_response = ""
        
        for attempt in range(max_retries_for_quality):
            raw_response = self._llm.generate(
                prompt,
                system_prompt=system_prompt,
                format="json",
                # keep_alive –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –Ω–∞ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ payload –≤ Ollama API
                keep_alive=keep_alive,
                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è/—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ (Ollama options)
                options=req_options,
            )

            # –ü–∞—Ä—Å–∏–º JSON‚Äë–æ—Ç–≤–µ—Ç
            enriched_data = self._parse_llm_batch_enrichment(raw_response)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
            valid_items = [item for item in enriched_data if isinstance(item, dict)]
            if len(valid_items) >= len(chunks_data) * 0.8:  # –•–æ—Ç—è –±—ã 80% —á–∞–Ω–∫–æ–≤ –æ–±–æ–≥–∞—â–µ–Ω–æ
                break  # –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–µ–º–ª–µ–º–æ–µ, –≤—ã—Ö–æ–¥–∏–º
            
            # –ï—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø–ª–æ—Ö–æ–µ –∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –ø–æ–≤—Ç–æ—Ä—è–µ–º
            if attempt < max_retries_for_quality - 1:
                print(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞ ({len(valid_items)}/{len(chunks_data)}), –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}/{max_retries_for_quality}...")
                time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º

        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç LLM (—Å—ã—Ä–æ–π) + —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞
        valid_enriched_data_for_log = [item for item in enriched_data if isinstance(item, dict)]
        parsed_with_chunk_id = sum(
            1 for item in valid_enriched_data_for_log if item.get("chunk_id")
        )
        self._logger.log_llm_enrichment(
            event="response",
            pdf_name=pdf_name,
            chunks_count=len(chunks_data),
            chunk_ids=[c["chunk_id"] for c in chunks_data],
            raw_response=raw_response,
            parsed_items=len(valid_enriched_data_for_log),
            parsed_with_chunk_id=parsed_with_chunk_id,
        )
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if not enriched_data:
            print(f"‚ö†Ô∏è  WARNING: LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {pdf_name}")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            print(f"   –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ LLM: {len(raw_response)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞ LLM:\n{raw_response[:1000]}")
            if len(raw_response) > 1000:
                print(f"   –ü–æ—Å–ª–µ–¥–Ω–∏–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞ LLM:\n{raw_response[-500:]}")

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ enriched_data
        valid_enriched_data = [item for item in enriched_data if isinstance(item, dict)]
        
        # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å chunk_id -> enriched_data –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        enriched_map: Dict[str, Dict[str, Any]] = {}
        for item in valid_enriched_data:
            chunk_id = item.get("chunk_id")
            if chunk_id:
                enriched_map[str(chunk_id)] = item
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –±—ã–ª–æ –æ–±–æ–≥–∞—â–µ–Ω–æ
        if len(enriched_map) < len(chunks):
            print(f"‚ö†Ô∏è  WARNING: –¢–æ–ª—å–∫–æ {len(enriched_map)} –∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤ –±—ã–ª–∏ –æ–±–æ–≥–∞—â–µ–Ω—ã –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {pdf_name}")
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã chunk_id –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if chunks:
                print(f"   –ü—Ä–∏–º–µ—Ä chunk_id –∏–∑ —á–∞–Ω–∫–æ–≤: {chunks[0].id}")
            if enriched_map:
                example_id = list(enriched_map.keys())[0]
                print(f"   –ü—Ä–∏–º–µ—Ä chunk_id –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM: {example_id}")

        # –û–±–æ–≥–∞—â–∞–µ–º —á–∞–Ω–∫–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç LLM
        enriched_chunks: List[Chunk] = []
        
        # –ï—Å–ª–∏ LLM –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ, –Ω–æ –Ω–µ –≤—Å–µ —á–∞–Ω–∫–∏ –Ω–∞–π–¥–µ–Ω—ã –ø–æ ID, –ø—ã—Ç–∞–µ–º—Å—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ –ø–æ—Ä—è–¥–∫—É
        if len(enriched_map) < len(chunks) and valid_enriched_data:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –µ—Å–ª–∏ LLM –≤–µ—Ä–Ω—É–ª —Ç–æ–ª—å–∫–æ 1 –æ–±—ä–µ–∫—Ç –≤–º–µ—Å—Ç–æ –º–∞—Å—Å–∏–≤–∞
            # (—á–∞—Å—Ç–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏)
            if len(valid_enriched_data) == 1 and len(chunks) > 1:
                # LLM –≤–µ—Ä–Ω—É–ª —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞
                # –∏ —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –æ–±—ä–µ–∫—Ç—ã –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                first_chunk = chunks[0]
                if first_chunk.id not in enriched_map:
                    enriched_map[first_chunk.id] = valid_enriched_data[0].copy()
                    enriched_map[first_chunk.id]["chunk_id"] = first_chunk.id
                # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –æ–±—ä–µ–∫—Ç—ã (–±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã fallback –∑–Ω–∞—á–µ–Ω–∏—è)
            # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ –ø–æ—Ä—è–¥–∫—É (–µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç)
            elif len(valid_enriched_data) == len(chunks):
                for i, ch in enumerate(chunks):
                    if ch.id not in enriched_map and i < len(valid_enriched_data):
                        enriched_map[ch.id] = valid_enriched_data[i]
                        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ chunk_id —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ
                        enriched_map[ch.id]["chunk_id"] = ch.id
            # –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –Ω–æ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ - –ø—ã—Ç–∞–µ–º—Å—è —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ –ø–æ—Ä—è–¥–∫—É
            # –¥–ª—è —Ç–µ—Ö —á–∞–Ω–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –æ–±–æ–≥–∞—â–µ–Ω—ã
            elif len(valid_enriched_data) > 0:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ—Ä—è–¥–∫—É –¥–ª—è –Ω–µ–æ–±–æ–≥–∞—â–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
                used_indices = set()
                for i, ch in enumerate(chunks):
                    if ch.id not in enriched_map:
                        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ valid_enriched_data
                        for j, data in enumerate(valid_enriched_data):
                            if j not in used_indices:
                                enriched_map[ch.id] = data.copy()
                                enriched_map[ch.id]["chunk_id"] = ch.id
                                used_indices.add(j)
                                break
        
        for ch in chunks:
            enriched = enriched_map.get(ch.id, {})
            
            # –ï—Å–ª–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ ID, –Ω–æ –µ—Å—Ç—å –æ–¥–∏–Ω –æ–±—ä–µ–∫—Ç –≤ enriched_data - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if not enriched and len(chunks) == 1 and valid_enriched_data:
                # –î–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –≤–µ—Ä–Ω—É—Ç—å –æ–±—ä–µ–∫—Ç –±–µ–∑ chunk_id
                if len(valid_enriched_data) == 1:
                    enriched = valid_enriched_data[0]
                    # –î–æ–±–∞–≤–ª—è–µ–º chunk_id –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    if "chunk_id" not in enriched:
                        enriched["chunk_id"] = ch.id
                elif len(valid_enriched_data) > 0:
                    # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π
                    enriched = valid_enriched_data[0]
                    if "chunk_id" not in enriched:
                        enriched["chunk_id"] = ch.id
            
            # –û–±–Ω–æ–≤–ª—è–µ–º context (—Å fallback –Ω–∞ text)
            if enriched.get("context"):
                context_str = str(enriched.get("context"))
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                if "\\u" in context_str:
                    try:
                        import codecs
                        context_str = codecs.decode(context_str, 'unicode_escape')
                    except (UnicodeDecodeError, ValueError):
                        pass  # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                ch.context = context_str[:200]
            elif ch.text:
                ch.context = ch.text[:200]
            else:
                ch.context = "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: –≤—Å–µ–≥–¥–∞ –æ–±–Ω–æ–≤–ª—è–µ–º, –µ—Å–ª–∏ –∫–ª—é—á –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ enriched
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —è–≤–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å None –¥–ª—è –ø–æ–ª–µ–π, –∫–æ—Ç–æ—Ä—ã–µ LLM –≤–µ—Ä–Ω—É–ª –∫–∞–∫ null
            if "geo" in enriched:
                ch.geo = enriched["geo"]
            if "metrics" in enriched:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏: –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                metrics = enriched["metrics"]
                if metrics and isinstance(metrics, list):
                    normalized_metrics = []
                    for m in metrics:
                        if isinstance(m, str) and m.strip():
                            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
                            normalized = m.strip().lower()
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç (—Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É)
                            if any('\u0400' <= char <= '\u04FF' for char in normalized):
                                normalized_metrics.append(normalized)
                    ch.metrics = normalized_metrics if normalized_metrics else None
                else:
                    ch.metrics = None
            if "years" in enriched:
                ch.years = self._normalize_years(enriched["years"])
            if "time_granularity" in enriched:
                ch.time_granularity = enriched["time_granularity"]
            if "oked" in enriched:
                ch.oked = enriched["oked"]
            
            enriched_chunks.append(ch)

        return enriched_chunks


    @staticmethod
    def _parse_llm_single_enrichment(raw: str, expected_chunk_id: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (–æ–±—ä–µ–∫—Ç, –Ω–µ –º–∞—Å—Å–∏–≤).
        """
        if not raw:
            return None
        
        # –£–¥–∞–ª–µ–Ω–∏–µ markdown code blocks
        cleaned = raw.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        cleaned = cleaned.strip()
        
        # –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ –æ–±—ä–µ–∫—Ç
        try:
            data = json.loads(cleaned)
            if isinstance(data, dict):
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á–∏ (ID -> chunk_id, –∏ —Ç.–¥.)
                normalized = KnowledgeBaseBuilder._normalize_enrichment_object(data, expected_chunk_id)
                return normalized
        except json.JSONDecodeError:
            pass
        
        # –ü–æ–∏—Å–∫ –æ–±—ä–µ–∫—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            try:
                data = json.loads(cleaned[start:end+1])
                if isinstance(data, dict):
                    normalized = KnowledgeBaseBuilder._normalize_enrichment_object(data, expected_chunk_id)
                    return normalized
            except json.JSONDecodeError:
                pass
        
        return None
    
    @staticmethod
    def _normalize_enrichment_object(obj: Dict[str, Any], expected_chunk_id: str) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç –æ–±–æ–≥–∞—â–µ–Ω–∏—è: –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∫–ª—é—á–∏, –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
        """
        result = {}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è chunk_id (–º–æ–∂–µ—Ç –±—ã—Ç—å ID, chunk_id, –ò–î –∏ —Ç.–¥.)
        for key in ["chunk_id", "ID", "–ò–î", "id", "chunkId"]:
            if key in obj:
                chunk_id_value = str(obj[key])
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (page153::chunk1351 -> –ø–æ–ª–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
                if "::" not in chunk_id_value and "::" in expected_chunk_id:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä –∏–∑ chunk_id –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å expected_chunk_id
                    result["chunk_id"] = expected_chunk_id
                else:
                    result["chunk_id"] = chunk_id_value
                break
        if "chunk_id" not in result:
            result["chunk_id"] = expected_chunk_id
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        for field in ["context", "geo", "metrics", "years", "time_granularity", "oked"]:
            if field in obj:
                result[field] = obj[field]
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
                found = False
                for key, value in obj.items():
                    if isinstance(value, dict):
                        if field in value:
                            result[field] = value[field]
                            found = True
                            break
                        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º –≤ –≥–ª—É–±–æ–∫–æ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–∞—Ö
                        elif any(isinstance(v, dict) for v in value.values() if isinstance(v, dict)):
                            for sub_key, sub_value in value.items():
                                if isinstance(sub_value, dict) and field in sub_value:
                                    result[field] = sub_value[field]
                                    found = True
                                    break
                            if found:
                                break
                if not found:
                    result[field] = None
        
        # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã —Å –Ω—É–∂–Ω—ã–º–∏ –ø–æ–ª—è–º–∏, –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π
        # (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç {"–°–≤–µ—Ç–ª–æ–≥–æ—Ä—Å–∫–∏–π": {"chunk_id": ..., "context": ...}})
        for key, value in obj.items():
            if isinstance(value, dict):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –≤–ª–æ–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
                has_enrichment_fields = any(field in value for field in ["context", "geo", "metrics", "chunk_id"])
                if has_enrichment_fields:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç
                    nested = KnowledgeBaseBuilder._normalize_enrichment_object(value, expected_chunk_id)
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–≤–ª–æ–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                    for k, v in nested.items():
                        if v is not None or k not in result:
                            result[k] = v
                    break
        
        return result
    
    @staticmethod
    def _parse_llm_batch_enrichment(raw: str) -> List[Dict[str, Any]]:
        """
        –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–µ—Ä JSON‚Äë–æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM –¥–ª—è –±–∞—Ç—á–µ–≤–æ–≥–æ enrichment.

        –ò—â–µ—Ç JSON‚Äë–º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å.
        –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –æ–±—ä–µ–∫—Ç –≤–º–µ—Å—Ç–æ –º–∞—Å—Å–∏–≤–∞, –ø—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –Ω–µ–≥–æ –¥–∞–Ω–Ω—ã–µ.
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–∏, –∫–æ–≥–¥–∞ LLM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ–¥—Ä—è–¥ (–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ).
        –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
        """
        if not raw:
            return []

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –£–¥–∞–ª–µ–Ω–∏–µ markdown code blocks –∏ –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        cleaned = raw.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]  # –£–¥–∞–ª—è–µ–º ```json
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]  # –£–¥–∞–ª—è–µ–º ```
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]  # –£–¥–∞–ª—è–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π ```
        cleaned = cleaned.strip()
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ (–∏–Ω–æ–≥–¥–∞ LLM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç "{  "["...)
        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º –∑–Ω–∞—á–∏–º—ã–º —Å–∏–º–≤–æ–ª–æ–º
        while cleaned and cleaned[0] in [' ', '\n', '\r', '\t']:
            cleaned = cleaned[1:]
        
        # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "{  "[" - —ç—Ç–æ –æ–±—ä–µ–∫—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å—Ç—Ä–æ–∫—É —Å –º–∞—Å—Å–∏–≤–æ–º
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –º–∞—Å—Å–∏–≤ –∏–∑ —Å—Ç—Ä–æ–∫–∏
        if cleaned.startswith('{') and '"[{' in cleaned:
            # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –º–∞—Å—Å–∏–≤–∞ –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏
            array_start = cleaned.find('"[{')
            if array_start != -1:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–æ–∫—É —Å –º–∞—Å—Å–∏–≤–æ–º
                array_str_start = array_start + 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–≤—ã—á–∫—É
                # –ò—â–µ–º –∫–æ–Ω–µ—Ü —Å—Ç—Ä–æ–∫–∏ (–∑–∞–∫—Ä—ã–≤–∞—é—â–∞—è –∫–∞–≤—ã—á–∫–∞ –ø–µ—Ä–µ–¥ })
                array_str_end = cleaned.find('"', array_str_start + 1)
                if array_str_end != -1:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    array_str = cleaned[array_str_start:array_str_end]
                    # –ó–∞–º–µ–Ω—è–µ–º —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –∏ –¥—Ä—É–≥–∏–µ escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    try:
                        import codecs
                        array_str = codecs.decode(array_str, 'unicode_escape')
                        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                        data = json.loads(array_str)
                        if isinstance(data, list):
                            return KnowledgeBaseBuilder._validate_and_fix_enrichment_data(data)
                    except (json.JSONDecodeError, UnicodeDecodeError, ValueError):
                        pass

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä—è–º–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–∞
        try:
            data = json.loads(cleaned)
            if isinstance(data, list):
                return KnowledgeBaseBuilder._validate_and_fix_enrichment_data(data)
            elif isinstance(data, dict):
                # LLM –≤–µ—Ä–Ω—É–ª –æ–±—ä–µ–∫—Ç –≤–º–µ—Å—Ç–æ –º–∞—Å—Å–∏–≤–∞ - –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –º–∞—Å—Å–∏–≤ –∏–∑ –Ω–µ–≥–æ
                # –ò—â–µ–º –∫–ª—é—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞—Å—Å–∏–≤
                for key in ["chunks", "data", "results", "items", "array"]:
                    if key in data and isinstance(data[key], list):
                        return KnowledgeBaseBuilder._validate_and_fix_enrichment_data(data[key])
                # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç chunk_id –∏–ª–∏ –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è - —ç—Ç–æ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç
                # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –º–∞—Å—Å–∏–≤ (–≤—ã–∑—ã–≤–∞—é—â–∏–π –∫–æ–¥ –¥–æ–ª–∂–µ–Ω –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –≤–µ—Ä–Ω—É–ª—Å—è 1 –æ–±—ä–µ–∫—Ç –≤–º–µ—Å—Ç–æ N)
                if "chunk_id" in data or any(key in data for key in ["context", "geo", "metrics", "years"]):
                    return KnowledgeBaseBuilder._validate_and_fix_enrichment_data([data])
        except json.JSONDecodeError:
            # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            try:
                import codecs
                decoded = codecs.decode(cleaned, 'unicode_escape')
                data = json.loads(decoded)
                if isinstance(data, list):
                    return KnowledgeBaseBuilder._validate_and_fix_enrichment_data(data)
                elif isinstance(data, dict):
                    if "chunk_id" in data or any(key in data for key in ["context", "geo", "metrics", "years"]):
                        return KnowledgeBaseBuilder._validate_and_fix_enrichment_data([data])
            except (json.JSONDecodeError, UnicodeDecodeError, ValueError):
                pass

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ JSON-–º–∞—Å—Å–∏–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        start = cleaned.find("[")
        end = cleaned.rfind("]")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            try:
                data = json.loads(snippet)
                if isinstance(data, list):
                    return KnowledgeBaseBuilder._validate_and_fix_enrichment_data(data)
            except json.JSONDecodeError:
                pass

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: –ü–æ–∏—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö JSON –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ (–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã)
        # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ - LLM —á–∞—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç {"chunk_id":"...",...}{"chunk_id":"...",...} –±–µ–∑ –º–∞—Å—Å–∏–≤–∞
        objects = []
        i = 0
        while i < len(cleaned):
            if cleaned[i] == '{':
                # –ù–∞—Ö–æ–¥–∏–º –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –¥–ª—è —ç—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
                depth = 0
                j = i
                while j < len(cleaned):
                    if cleaned[j] == '{':
                        depth += 1
                    elif cleaned[j] == '}':
                        depth -= 1
                        if depth == 0:
                            # –ù–∞—à–ª–∏ –ø–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç
                            try:
                                obj_str = cleaned[i:j+1]
                                obj = json.loads(obj_str)
                                if isinstance(obj, dict):
                                    # –ü—Ä–∏–Ω–∏–º–∞–µ–º –æ–±—ä–µ–∫—Ç –µ—Å–ª–∏ –µ—Å—Ç—å chunk_id –∏–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∏–∑ –Ω—É–∂–Ω—ã—Ö –ø–æ–ª–µ–π
                                    if "chunk_id" in obj or any(key in obj for key in ["context", "geo", "metrics", "years"]):
                                        objects.append(obj)
                            except json.JSONDecodeError:
                                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON
                                pass
                            i = j + 1  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
                            break
                    j += 1
                else:
                    # –ù–µ –Ω–∞—à–ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —Å–∏–º–≤–æ–ª
                    i += 1
            else:
                i += 1
        
        if objects:
            return KnowledgeBaseBuilder._validate_and_fix_enrichment_data(objects)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –æ–¥–∏–Ω JSON-–æ–±—ä–µ–∫—Ç –∏ –æ–±–µ—Ä–Ω—É—Ç—å –≤ –º–∞—Å—Å–∏–≤
        start = cleaned.find("{")
        end = cleaned.rfind("}")
        if start != -1 and end != -1 and end > start:
            snippet = cleaned[start : end + 1]
            try:
                data = json.loads(snippet)
                if isinstance(data, dict):
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –º–∞—Å—Å–∏–≤ –≤–Ω—É—Ç—Ä–∏ –æ–±—ä–µ–∫—Ç–∞
                    for key in ["chunks", "data", "results", "items", "array"]:
                        if key in data and isinstance(data[key], list):
                            return KnowledgeBaseBuilder._validate_and_fix_enrichment_data(data[key])
                    # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç chunk_id, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –º–∞—Å—Å–∏–≤
                    if "chunk_id" in data or any(key in data for key in ["context", "geo", "metrics", "years"]):
                        return KnowledgeBaseBuilder._validate_and_fix_enrichment_data([data])
            except json.JSONDecodeError:
                pass

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ
        print(f"‚ö†Ô∏è  WARNING: –ù–µ –Ω–∞–π–¥–µ–Ω JSON-–º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ LLM. –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤: {raw[:500]}")
        return []
    
    @staticmethod
    def _validate_and_fix_enrichment_data(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è.
        """
        if not isinstance(data, list):
            print(f"‚ö†Ô∏è  WARNING: –ü–∞—Ä—Å–µ—Ä –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ø–∏—Å–æ–∫, –∞ {type(data).__name__}")
            return []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏
        valid_items = []
        for i, item in enumerate(data):
            if not isinstance(item, dict):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ-—Å–ª–æ–≤–∞—Ä–∏ –±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è (–º–æ–≥—É—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏–∑ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞)
                continue
            valid_items.append(item)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è
        for item in valid_items:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –ø–æ–ª—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç (–¥–∞–∂–µ –µ—Å–ª–∏ null)
            required_fields = ["chunk_id", "context", "geo", "metrics", "years", "time_granularity", "oked"]
            for field in required_fields:
                if field not in item:
                    # –ï—Å–ª–∏ –ø–æ–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫–∞–∫ None
                    item[field] = None
        
        return valid_items

    @staticmethod
    def _normalize_years(value: Any) -> Optional[List[int]]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ years –≤ —Å–ø–∏—Å–æ–∫ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª."""
        if value is None:
            return None
        if isinstance(value, int):
            return [value]
        if isinstance(value, str):
            try:
                return [int(value)]
            except ValueError:
                return None
        if isinstance(value, list):
            years: List[int] = []
            for v in value:
                try:
                    years.append(int(v))
                except (TypeError, ValueError):
                    continue
            return years or None
        return None

    # -------------------- FAISS -------------------- #

    def _build_faiss_index(self, chunks: List[Chunk], index_path: Path) -> None:
        """
        –°—Ç—Ä–æ–∏—Ç FAISS IndexFlatIP –ø–æ embeddings –ø–æ–ª—è `context`.

        –í–ê–ñ–ù–û:
        - HashVectorizer —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–∞, –ø–æ—ç—Ç–æ–º—É IndexFlatIP == cosine similarity.
        """
        if not chunks:
            # —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å –Ω–∞ —Å–ª—É—á–∞–π –ø—É—Å—Ç–æ–π –±–∞–∑—ã (–æ—Ç–ª–∞–¥–∫–∞)
            index = faiss.IndexFlatIP(self._vectorizer.dimension)
            faiss.write_index(index, str(index_path))
            return

        texts = [ch.context for ch in chunks]
        embeddings = self._vectorizer.embed_many(texts).astype("float32")

        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings)
        faiss.write_index(index, str(index_path))

