import argparse
import json
import os
from pathlib import Path
from typing import Any, Dict, List

from src.main.numeric_extractor import FragmentMeta, aggregate_homogeneous, extract_numeric_indicators
from src.main.retriever import SemanticRetriever
from src.main.vectorizer import HashVectorizer


def _build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="RAG CLI: extract numeric indicators (offline).")
    mode = p.add_mutually_exclusive_group()
    mode.add_argument("--strict", action="store_true", help="Strict filtering (default).")
    mode.add_argument("--relaxed", action="store_true", help="Relaxed mode (keep more extractions).")
    p.add_argument("--top-k", type=int, default=10, help="Retriever top_k.")
    p.add_argument("--hybrid-weight", type=float, default=0.3, help="Retriever hybrid weight.")
    p.add_argument("--aggregate", action="store_true", help="Aggregate only homogeneous (metric+unit+entity).")
    p.add_argument("--min-confidence", type=float, default=0.6, help="Min confidence in strict mode.")
    p.add_argument("--query", type=str, default="", help="Single query (if provided).")
    p.add_argument("--log-raw", type=str, default="", help="Write raw hits JSONL for human review.")
    return p


def _extract_from_hits(
    query: str,
    hits: List[Dict[str, Any]],
    *,
    strict: bool,
    aggregate: bool,
    min_confidence: float,
) -> List[Dict[str, Any]]:
    extracted: List[Dict[str, Any]] = []
    for h in hits:
        meta = FragmentMeta(
            source_file=str(h.get("source", "")),
            page=int(h.get("page", 0) or 0),
            bbox=h.get("bbox"),
            title=str(h.get("title", "")),
        )
        extracted.extend(
            extract_numeric_indicators(
                fragment_text=str(h.get("text", "")),
                meta=meta,
                query=query,
                strict=strict,
                min_confidence=min_confidence,
            )
        )
    if aggregate:
        extracted = aggregate_homogeneous(extracted)
    extracted.sort(key=lambda x: float(x.get("confidence", 0.0)), reverse=True)
    return extracted


def main() -> None:
    args = _build_argparser().parse_args()
    strict = True if not args.relaxed else False
    if args.strict:
        strict = True

    base_dir = Path(__file__).resolve().parents[1]
    vector_store_dir = base_dir / "prepare_db" / "vector_store"

    vectorizer = HashVectorizer(dimension=256)
    retriever = SemanticRetriever(vectorizer=vectorizer, data_path=vector_store_dir / "data.json")

    if args.query.strip():
        queries = [args.query.strip()]
    else:
        # small default for local smoke runs
        queries = [
            "–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ –æ–±–ª–∞—Å—Ç—è–º –ë–µ–ª–∞—Ä—É—Å–∏",
            "–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –º–æ–ª–æ–∫–∞",
            "–ß–∏—Å–ª–æ —É—á—Ä–µ–∂–¥–µ–Ω–∏–π –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è",
        ]

    raw_fp = None
    if args.log_raw:
        os.makedirs(str(Path(args.log_raw).parent), exist_ok=True)
        raw_fp = open(args.log_raw, "w", encoding="utf-8")

    try:
        for q in queries:
            hits = retriever.search(q, top_k=args.top_k, hybrid_weight=args.hybrid_weight)
            if raw_fp:
                raw_fp.write(json.dumps({"query": q, "hits": hits}, ensure_ascii=False) + "\n")
            results = _extract_from_hits(
                q,
                hits,
                strict=strict,
                aggregate=bool(args.aggregate),
                min_confidence=float(args.min_confidence),
            )
            print(json.dumps({"query": q, "results": results}, ensure_ascii=False, indent=2))
    finally:
        if raw_fp:
            raw_fp.close()


if __name__ == "__main__":
    main()

import re
from pathlib import Path
from typing import List, Dict, Set, Tuple
from src.main.retriever import SemanticRetriever
from src.main.vectorizer import HashVectorizer

# –§–ª–∞–≥–∏ –∑–∞–ø—É—Å–∫–∞
predefined_queries_flag = False   # –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
user_queries_flag = True        # –¥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–≤–æ–¥–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã

# –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç)
MIN_SCORE_THRESHOLD = 0.1


def _clean_text(text: str, max_length: int = 800) -> str:
    """
    –û—á–∏—â–∞–µ—Ç –∏ –æ–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
    
    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not text:
        return ""
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
    text = " ".join(text.split())
    
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –æ–±—Ä–µ–∑–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    if len(text) > max_length:
        # –ü—ã—Ç–∞–µ–º—Å—è –æ–±—Ä–µ–∑–∞—Ç—å –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–æ—á–∫–µ –∏–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å—É —Å—Ç—Ä–æ–∫–∏
        truncated = text[:max_length]
        last_period = truncated.rfind('.')
        last_newline = truncated.rfind('\n')
        
        cut_point = max(last_period, last_newline)
        if cut_point > max_length * 0.7:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–∑—É–º–Ω–æ–µ –º–µ—Å—Ç–æ –¥–ª—è –æ–±—Ä–µ–∑–∫–∏
            text = truncated[:cut_point + 1] + "..."
        else:
            text = truncated + "..."
    
    return text


# –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –µ–¥–∏–Ω–∏—Ü –∏–∑–º–µ—Ä–µ–Ω–∏—è
UNIT_MULTIPLIERS = {
    '—Ç—ã—Å.': 1000, '—Ç—ã—Å—è—á': 1000, '—Ç—ã—Å': 1000,
    '–º–ª–Ω.': 1000000, '–º–∏–ª–ª–∏–æ–Ω': 1000000, '–º–ª–Ω': 1000000,
    '–º–ª—Ä–¥.': 1000000000, '–º–∏–ª–ª–∏–∞—Ä–¥': 1000000000, '–º–ª—Ä–¥': 1000000000,
    '–≥–∞': 1, '–≥–µ–∫—Ç–∞—Ä': 1, '–≥–µ–∫—Ç–∞—Ä–æ–≤': 1,
    '–∫–º¬≤': 1, '–∫–≤.–∫–º': 1, '–∫–º2': 1,
    '–º¬≤': 1, '–∫–≤.–º': 1, '–º2': 1,
    '—Ç–æ–Ω–Ω': 1, '—Ç': 1, '—Ç–æ–Ω–Ω–∞': 1,
    '–∫–≥': 1, '–∫–∏–ª–æ–≥—Ä–∞–º–º': 1,
    '—Ä—É–±.': 1, '—Ä—É–±–ª–µ–π': 1, '—Ä—É–±': 1,
    '–¥–æ–ª–ª.': 1, '–¥–æ–ª–ª–∞—Ä–æ–≤': 1, 'usd': 1,
    '—á–µ–ª–æ–≤–µ–∫': 1, '—á–µ–ª.': 1, '—á–µ–ª': 1,
    '–ø—Ä–æ—Ü–µ–Ω—Ç': 1, '%': 1, '–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤': 1,
}

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
STATISTICAL_CONTEXT_KEYWORDS = {
    '–Ω–∞—Å–µ–ª–µ–Ω–∏–µ': ['–Ω–∞—Å–µ–ª–µ–Ω', '—á–µ–ª–æ–≤–µ–∫', '–∂–∏—Ç–µ–ª', '—á–µ–ª'],
    '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ': ['–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤', '–≤—ã–ø—É—Å–∫', '–≤—ã—Ä–∞–±–æ—Ç–∫', '–¥–æ–±—ã—á'],
    '–ø–ª–æ—â–∞–¥—å': ['–ø–ª–æ—â–∞–¥', '–≥–∞', '–≥–µ–∫—Ç–∞—Ä', '–∫–º¬≤', '–∫–≤.–∫–º'],
    '—Å—Ç–æ–∏–º–æ—Å—Ç—å': ['—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—Ä—É–±', '–¥–æ–ª–ª', '—Å—Ç–æ–∏–º'],
    '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è': ['–∑–∞—Ä–∞–±–æ—Ç–Ω', '–∑–∞—Ä–ø–ª–∞—Ç', '–¥–æ—Ö–æ–¥', '–æ–ø–ª–∞—Ç'],
    '—ç–∫—Å–ø–æ—Ä—Ç': ['—ç–∫—Å–ø–æ—Ä—Ç', '–≤—ã–≤–æ–∑'],
    '–∏–º–ø–æ—Ä—Ç': ['–∏–º–ø–æ—Ä—Ç', '–≤–≤–æ–∑'],
    '–≤–≤–ø': ['–≤–≤–ø', '–≤–∞–ª–æ–≤–æ–π', '–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π', '–ø—Ä–æ–¥—É–∫—Ç'],
    '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è': ['–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–≤–ª–æ–∂–µ–Ω', '–∫–∞–ø–∏—Ç–∞–ª'],
    '–±—é–¥–∂–µ—Ç': ['–±—é–¥–∂–µ—Ç', '–¥–æ—Ö–æ–¥', '—Ä–∞—Å—Ö–æ–¥'],
    '—É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å': ['—É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç', '—Å–±–æ—Ä', '–≤–∞–ª–æ–≤'],
    '–ø–æ–≥–æ–ª–æ–≤—å–µ': ['–ø–æ–≥–æ–ª–æ–≤—å', '—Å–∫–æ—Ç', '–∂–∏–≤–æ—Ç–Ω'],
}

# –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã
GEOGRAPHIC_ENTITIES = {
    '–æ–±–ª–∞—Å—Ç–∏': ['–±—Ä–µ—Å—Ç—Å–∫', '–≤–∏—Ç–µ–±—Å–∫', '–≥–æ–º–µ–ª—å—Å–∫', '–≥—Ä–æ–¥–Ω–µ–Ω—Å–∫', '–º–∏–Ω—Å–∫', '–º–∏–Ω—Å–∫–∞—è', '–º–æ–≥–∏–ª–µ–≤—Å–∫'],
    '–≥–æ—Ä–æ–¥–∞': ['–º–∏–Ω—Å–∫', '–±—Ä–µ—Å—Ç', '–≤–∏—Ç–µ–±—Å–∫', '–≥–æ–º–µ–ª—å', '–≥—Ä–æ–¥–Ω–æ', '–º–æ–≥–∏–ª–µ–≤'],
    '—Ä–µ–≥–∏–æ–Ω—ã': ['–±–µ–ª–∞—Ä—É—Å—å', '—Ä–µ—Å–ø—É–±–ª–∏–∫–∞', '—Ä–±', '–±–µ–ª–æ—Ä—É—Å'],
}


def _normalize_unit(value_str: str, unit: str) -> Tuple[float, str]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —á–∏—Å–ª–æ —Å –µ–¥–∏–Ω–∏—Ü–µ–π –∏–∑–º–µ—Ä–µ–Ω–∏—è.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ_—á–∏—Å–ª–æ, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è_–µ–¥–∏–Ω–∏—Ü–∞).
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ
    number_match = re.search(r'[\d\s.,]+', value_str)
    if not number_match:
        return None, None
    
    number_str = number_match.group().replace(' ', '').replace(',', '.')
    try:
        number = float(number_str)
    except ValueError:
        return None, None
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å –¥–ª—è –µ–¥–∏–Ω–∏—Ü—ã
    unit_lower = unit.lower().strip('.,;:')
    multiplier = UNIT_MULTIPLIERS.get(unit_lower, 1)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–∏—Å–ª–æ
    normalized_value = number * multiplier
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É
    if unit_lower in ['—Ç—ã—Å.', '—Ç—ã—Å—è—á', '—Ç—ã—Å']:
        normalized_unit = '—Ç—ã—Å.'
    elif unit_lower in ['–º–ª–Ω.', '–º–∏–ª–ª–∏–æ–Ω', '–º–ª–Ω']:
        normalized_unit = '–º–ª–Ω.'
    elif unit_lower in ['–º–ª—Ä–¥.', '–º–∏–ª–ª–∏–∞—Ä–¥', '–º–ª—Ä–¥']:
        normalized_unit = '–º–ª—Ä–¥.'
    else:
        normalized_unit = unit
    
    return normalized_value, normalized_unit


def _extract_entity_from_context(context: str, query: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç –∏–ª–∏ –¥—Ä—É–≥—É—é —Å—É—â–Ω–æ—Å—Ç—å –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    context_lower = context.lower()
    query_lower = query.lower()
    
    # –ò—â–µ–º –æ–±–ª–∞—Å—Ç–∏
    for region_type, keywords in GEOGRAPHIC_ENTITIES.items():
        for keyword in keywords:
            if keyword in context_lower or keyword in query_lower:
                return keyword.capitalize()
    
    # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è "–ë–µ–ª–∞—Ä—É—Å—å", "–†–µ—Å–ø—É–±–ª–∏–∫–∞ –ë–µ–ª–∞—Ä—É—Å—å"
    if '–±–µ–ª–∞—Ä—É—Å—å' in context_lower or '–±–µ–ª–∞—Ä—É—Å—å' in query_lower:
        return '–ë–µ–ª–∞—Ä—É—Å—å'
    
    return None


def _is_relevant_number(context: str, query: str, number_type: str = None) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ –ª–∏ —á–∏—Å–ª–æ –∑–∞–ø—Ä–æ—Å—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    """
    context_lower = context.lower()
    query_lower = query.lower()
    
    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ç–∏–ø —á–∏—Å–ª–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
    if number_type:
        keywords = STATISTICAL_CONTEXT_KEYWORDS.get(number_type, [])
        if keywords:
            if not any(kw in context_lower for kw in keywords):
                return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
    query_words = set(re.findall(r'\b\w+\b', query_lower))
    context_words = set(re.findall(r'\b\w+\b', context_lower))
    
    # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ (–¥–ª–∏–Ω–∞ > 3)
    significant_matches = query_words & context_words
    significant_matches = {w for w in significant_matches if len(w) > 3}
    
    if not significant_matches:
        return False
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
    noise_patterns = [
        r'–ø—Ä–∏–º–µ—á–∞–Ω', r'—Å–Ω–æ—Å–∫', r'–º–µ—Ç–æ–¥–∏–∫', r'—Ä–∞—Å—á–µ—Ç',
        r'–∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω', r'–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω', r'–ø—Ä–∏–≤–µ–¥–µ–Ω',
        r'—Ä–µ–¥–∞–∫—Ü–∏–æ–Ω–Ω', r'—Å–æ–¥–µ—Ä–∂–∞–Ω', r'–æ–≥–ª–∞–≤–ª–µ–Ω'
    ]
    for pattern in noise_patterns:
        if re.search(pattern, context_lower):
            return False
    
    return True


def _extract_numbers_from_text(
    text: str, 
    query: str, 
    title: str = "",
    source: str = "",
    page: int = 0
) -> List[Dict]:
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–µ–ª –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏ –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ –æ–±—ä–µ–∫—Ç–∞–º.
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏:
        - value: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ
        - original: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        - unit: –µ–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è
        - context: –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —á–∏—Å–ª–∞
        - entity: –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç (–æ–±–ª–∞—Å—Ç—å, –≥–æ—Ä–æ–¥ –∏ —Ç.–¥.)
        - title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
        - source: –∏—Å—Ç–æ—á–Ω–∏–∫
        - page: —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    query_lower = query.lower()
    number_type = None
    for stat_type, keywords in STATISTICAL_CONTEXT_KEYWORDS.items():
        if any(kw in query_lower for kw in keywords):
            number_type = stat_type
            break
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —á–∏—Å–µ–ª —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è
    # –ò—â–µ–º: —á–∏—Å–ª–æ + –ø—Ä–æ–±–µ–ª + –µ–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è
    number_unit_pattern = r'(\d{1,3}(?:\s?\d{3})*(?:[.,]\d+)?)\s*([–∞-—è—ë]+\.?|[–∞-—è—ë]+)'
    
    numbers = []
    lines = text.split('\n')
    
    for line_idx, line in enumerate(lines):
        # –ò—â–µ–º —á–∏—Å–ª–∞ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏
        matches = list(re.finditer(number_unit_pattern, line, re.IGNORECASE))
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –µ–¥–∏–Ω–∏—Ü—ã, –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–∞
        if not matches:
            number_pattern = r'\b(\d{1,3}(?:\s?\d{3})*(?:[.,]\d+)?)\b'
            matches = list(re.finditer(number_pattern, line))
        
        for match in matches:
            number_str = match.group(1) if match.lastindex >= 1 else match.group(0)
            unit = match.group(2) if match.lastindex >= 2 else ""
            
            # –ë–µ—Ä–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–¥–æ 50 —Å–∏–º–≤–æ–ª–æ–≤ —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã)
            start = max(0, match.start() - 50)
            end = min(len(line), match.end() + 50)
            context = line[start:end].strip()
            
            # –¢–∞–∫–∂–µ –±–µ—Ä–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –∏ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            full_context = context
            if line_idx > 0:
                prev_line = lines[line_idx - 1][:50]
                full_context = prev_line + " | " + full_context
            if line_idx < len(lines) - 1:
                next_line = lines[line_idx + 1][:50]
                full_context = full_context + " | " + next_line
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            if not _is_relevant_number(full_context, query, number_type):
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–∏—Å–ª–æ –∏ –µ–¥–∏–Ω–∏—Ü—É
            normalized_value, normalized_unit = _normalize_unit(number_str, unit)
            if normalized_value is None:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä–µ–∫—Ç (–æ–±–ª–∞—Å—Ç—å, –≥–æ—Ä–æ–¥ –∏ —Ç.–¥.)
            entity = _extract_entity_from_context(full_context, query)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            numbers.append({
                'value': normalized_value,
                'original': number_str,
                'unit': normalized_unit or unit,
                'context': full_context,
                'entity': entity,
                'title': title,
                'source': source,
                'page': page,
                'line': line_idx
            })
    
    return numbers


def _is_statistical_query(query: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º."""
    statistical_keywords = {
        '—á–∏—Å–ª–æ', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '—á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å', '—Å–∫–æ–ª—å–∫–æ',
        '–Ω–∞—Å–µ–ª–µ–Ω–∏–µ', '—á–µ–ª–æ–≤–µ–∫', '–∂–∏—Ç–µ–ª—å', '–≤–≤–ø', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ',
        '—ç–∫—Å–ø–æ—Ä—Ç', '–∏–º–ø–æ—Ä—Ç', '–æ–±—ä–µ–º', '–¥–æ–±—ã—á–∞', '—Ç–æ–Ω–Ω', '—Ç–æ–Ω–Ω–∞',
        '–ø–æ–≥–æ–ª–æ–≤—å–µ', '—É—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å', '–ø–ª–æ—â–∞–¥—å', '—Å—Ç–æ–∏–º–æ—Å—Ç—å',
        '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è', '–¥–æ—Ö–æ–¥', '—Ä–∞—Å—Ö–æ–¥', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è', '–±—é–¥–∂–µ—Ç'
    }
    query_lower = query.lower()
    return any(kw in query_lower for kw in statistical_keywords)


def _format_table_like_data(text: str) -> str:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏.
    """
    lines = text.split('\n')
    formatted_lines = []
    
    for line in lines:
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ —á–∏—Å–µ–ª, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–∞–º–∏ - —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—É
        numbers = re.findall(r'\d+[\d\s.,]*', line)
        if len(numbers) >= 3:
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –ø—Ä–æ–±–µ–ª–∞–º
            parts = re.split(r'\s{2,}', line)
            if len(parts) >= 2:
                formatted_lines.append(' | '.join(p.strip() for p in parts if p.strip()))
            else:
                formatted_lines.append(line)
        else:
            formatted_lines.append(line)
    
    return '\n'.join(formatted_lines)


def compose_answer(query: str, results: List[Dict], top_k: int = 3) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤.
    
    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        results: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
    
    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    """
    if not results:
        return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É ¬´{query}¬ª –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–æ—Ä–æ–≥—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (—Å–Ω–∏–∂–µ–Ω –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
    filtered_results = [r for r in results if r.get('score', 0) >= MIN_SCORE_THRESHOLD * 0.5]
    
    if not filtered_results:
        return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É ¬´{query}¬ª –Ω–∞–π–¥–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é."
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    filtered_results.sort(key=lambda x: x.get('score', 0), reverse=True)
    
    # –ë–µ—Ä–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    top_results = filtered_results[:top_k]
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
    is_statistical = _is_statistical_query(query)
    
    # –°–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —á–∏—Å–ª–∞ –∏–∑ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    extracted_numbers = []  # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ - –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞
    results_data = []  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    
    for i, result in enumerate(top_results, 1):
        text = result.get('text', '').strip()
        title = result.get('title', '').strip()
        source = result.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
        page = result.get('page', 0)
        
        if not text:
            continue
        
        # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        if is_statistical:
            numbers = _extract_numbers_from_text(text, query, title, source, page)
            if numbers:
                extracted_numbers.extend(numbers)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        results_data.append({
            'text': text,
            'title': title,
            'source': source,
            'page': page,
            'result': result
        })
    
    # –¢–µ–ø–µ—Ä—å —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    context_parts = []
    sources = []
    seen_texts = set()  # –î–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    
    for i, data in enumerate(results_data, 1):
        text = data['text']
        title = data['title']
        source = data['source']
        page = data['page']
        
        # –ü—Ä–æ—Å—Ç–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã
        text_hash = hash(text[:100])  # –•–µ—à –ø–µ—Ä–≤—ã—Ö 100 —Å–∏–º–≤–æ–ª–æ–≤
        if text_hash in seen_texts:
            continue
        seen_texts.add(text_hash)
        
        # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        cleaned_text = _clean_text(text, max_length=1000)
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ —Ç–∞–±–ª–∏—Ü—É, –µ—Å–ª–∏ –µ—Å—Ç—å –º–Ω–æ–≥–æ —á–∏—Å–µ–ª
        if is_statistical and re.search(r'\d+', cleaned_text):
            cleaned_text = _format_table_like_data(cleaned_text)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
        fragment_parts = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
        if title and len(title) > 10:
            clean_title = " ".join(title.split()[:25])  # –ü–µ—Ä–≤—ã–µ 25 —Å–ª–æ–≤
            fragment_parts.append(f"[–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã] {clean_title}")
            fragment_parts.append("")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ö –æ—Ç–¥–µ–ª—å–Ω–æ
        fragment_numbers = [n for n in extracted_numbers if n.get('source') == source and n.get('page') == page]
        if fragment_numbers and is_statistical:
            fragment_parts.append("[–ö–ª—é—á–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Ç–∞–±–ª–∏—Ü—ã]:")
            for num_data in fragment_numbers[:5]:  # –¢–æ–ø-5 —á–∏—Å–µ–ª
                value = num_data['value']
                unit = num_data.get('unit', '')
                entity = num_data.get('entity', '')
                context_short = num_data.get('context', '')[:50]
                
                if entity:
                    fragment_parts.append(f"  {entity}: {value:,.2f} {unit}")
                elif context_short:
                    fragment_parts.append(f"  {value:,.2f} {unit} ({context_short}...)")
                else:
                    fragment_parts.append(f"  {value:,.2f} {unit}")
            fragment_parts.append("")
        
        # –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç–∞–±–ª–∏—Ü—ã
        fragment_parts.append("[–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç–∞–±–ª–∏—Ü—ã]:")
        fragment_parts.append(cleaned_text)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts.append("\n".join(fragment_parts))
        
        # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        source_info = f"{source} (—Å—Ç—Ä. {page})"
        if source_info not in sources:
            sources.append(source_info)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç
    answer_parts = [
        f"[–û–¢–í–ï–¢] –ó–∞–ø—Ä–æ—Å: ¬´{query}¬ª",
        ""
    ]
    
    # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —á–∏—Å–ª–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
    if is_statistical and extracted_numbers:
        answer_parts.append("[–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï]:")
        answer_parts.append("")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∏—Å–ª–∞ –ø–æ –æ–±—ä–µ–∫—Ç–∞–º (–æ–±–ª–∞—Å—Ç–∏, –≥–æ—Ä–æ–¥–∞ –∏ —Ç.–¥.)
        numbers_by_entity = {}
        numbers_without_entity = []
        
        for num_data in extracted_numbers:
            entity = num_data.get('entity')
            if entity:
                if entity not in numbers_by_entity:
                    numbers_by_entity[entity] = []
                numbers_by_entity[entity].append(num_data)
            else:
                numbers_without_entity.append(num_data)
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –æ–±—ä–µ–∫—Ç–∞–º
        if numbers_by_entity:
            answer_parts.append("–ü–æ –æ–±—ä–µ–∫—Ç–∞–º:")
            answer_parts.append("")
            for entity, nums_list in sorted(numbers_by_entity.items()):
                # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º: —Å—Ä–µ–¥–Ω–µ–µ, —Å—É–º–º–∞, –º–∏–Ω, –º–∞–∫—Å
                values = [n['value'] for n in nums_list]
                if values:
                    avg_val = sum(values) / len(values)
                    sum_val = sum(values)
                    min_val = min(values)
                    max_val = max(values)
                    
                    # –ë–µ—Ä–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ (—Å–∞–º—ã–π —á–∞—Å—Ç—ã–π)
                    sources_count = {}
                    for n in nums_list:
                        key = f"{n['source']} (—Å—Ç—Ä. {n['page']})"
                        sources_count[key] = sources_count.get(key, 0) + 1
                    main_source = max(sources_count.items(), key=lambda x: x[1])[0]
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                    unit = nums_list[0].get('unit', '')
                    answer_parts.append(f"  {entity}:")
                    if len(values) == 1:
                        answer_parts.append(f"    –ó–Ω–∞—á–µ–Ω–∏–µ: {values[0]:,.2f} {unit}")
                    else:
                        answer_parts.append(f"    –°—Ä–µ–¥–Ω–µ–µ: {avg_val:,.2f} {unit}")
                        answer_parts.append(f"    –°—É–º–º–∞: {sum_val:,.2f} {unit}")
                        answer_parts.append(f"    –î–∏–∞–ø–∞–∑–æ–Ω: {min_val:,.2f} - {max_val:,.2f} {unit}")
                        answer_parts.append(f"    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–Ω–∞—á–µ–Ω–∏–π: {len(values)}")
                    answer_parts.append(f"    –ò—Å—Ç–æ—á–Ω–∏–∫: {main_source}")
                    answer_parts.append("")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á–∏—Å–ª–∞ –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –æ–±—ä–µ–∫—Ç–∞–º
        if numbers_without_entity:
            answer_parts.append("–û–±—â–∏–µ –¥–∞–Ω–Ω—ã–µ:")
            answer_parts.append("")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            numbers_by_source = {}
            for num_data in numbers_without_entity[:15]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
                key = f"{num_data['source']} (—Å—Ç—Ä. {num_data['page']})"
                if key not in numbers_by_source:
                    numbers_by_source[key] = []
                numbers_by_source[key].append(num_data)
            
            for source_key, nums_list in list(numbers_by_source.items())[:3]:
                answer_parts.append(f"  –ò—Å—Ç–æ—á–Ω–∏–∫: {source_key}")
                for num_data in nums_list[:5]:
                    value = num_data['value']
                    unit = num_data.get('unit', '')
                    title = num_data.get('title', '')
                    context = num_data.get('context', '')[:60]
                    
                    if title:
                        answer_parts.append(f"    ‚Ä¢ {value:,.2f} {unit} (—Ç–∞–±–ª–∏—Ü–∞: {title[:40]}...)")
                    elif context:
                        answer_parts.append(f"    ‚Ä¢ {value:,.2f} {unit} (–∫–æ–Ω—Ç–µ–∫—Å—Ç: {context}...)")
                    else:
                        answer_parts.append(f"    ‚Ä¢ {value:,.2f} {unit}")
                answer_parts.append("")
        
        answer_parts.append("---")
        answer_parts.append("")
    
    answer_parts.append("–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    answer_parts.append("")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    if context_parts:
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–∏–∑—É–∞–ª—å–Ω–æ
        for i, fragment in enumerate(context_parts, 1):
            answer_parts.append(f"{'='*10}")
            answer_parts.append(f"–§—Ä–∞–≥–º–µ–Ω—Ç {i}:")
            answer_parts.append(f"{'='*10}")
            answer_parts.append(fragment)
            answer_parts.append("")
    else:
        answer_parts.append("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
        answer_parts.append("")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    if sources:
        answer_parts.append("[–ò–°–¢–û–ß–ù–ò–ö–ò]:")
        for source in sources:
            answer_parts.append(f"  - {source}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    if top_results:
        avg_score = sum(r.get('score', 0) for r in top_results) / len(top_results)
        max_score = max(r.get('score', 0) for r in top_results)
        answer_parts.append("")
        answer_parts.append(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {avg_score:.3f} (–º–∞–∫—Å: {max_score:.3f}, –Ω–∞–π–¥–µ–Ω–æ: {len(filtered_results)})")
    
    return "\n".join(answer_parts)


def format_detailed_results(query: str, results: List[Dict]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    
    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        results: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    
    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    """
    if not results:
        return "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
    lines = [f"üîç –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: ¬´{query}¬ª", ""]
    
    for i, r in enumerate(results, start=1):
        lines.append(f"{'='*10}")
        lines.append(f"–†–µ–∑—É–ª—å—Ç–∞—Ç #{i} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {r.get('score', 0):.3f})")
        lines.append(f"{'='*10}")
        lines.append(f"üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫: {r.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        lines.append(f"üìë –°—Ç—Ä–∞–Ω–∏—Ü–∞: {r.get('page', '?')}")
        lines.append("")
        lines.append("–¢–µ–∫—Å—Ç:")
        lines.append("-" * 10)
        text = r.get('text', '').strip()
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        if len(text) > 500:
            text = text[:500] + "..."
        lines.append(text)
        lines.append("")
    
    return "\n".join(lines)

def main() -> None:
    # –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è src/
    base_dir = Path(__file__).resolve().parents[1]

    # –ü–∞–ø–∫–∞ —Å —É–∂–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
    vector_store_dir = base_dir / "prepare_db" / "vector_store"

    vectorizer = HashVectorizer(dimension=256)

    retriever = SemanticRetriever(
        vectorizer=vectorizer,
        data_path=vector_store_dir / "data.json",
    )

    print("RAG CLI –∑–∞–ø—É—â–µ–Ω.\n")

    # –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    predefined_queries = [
        # –î–µ–º–æ–≥—Ä–∞—Ñ–∏—è
        # "–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞—Å–µ–ª–µ–Ω–∏—è –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å",
        "–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ –æ–±–ª–∞—Å—Ç—è–º –ë–µ–ª–∞—Ä—É—Å–∏",
        "–ù–∞—Å–µ–ª–µ–Ω–∏–µ –ú–∏–Ω—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –≤ 2024 –≥–æ–¥—É",
        "–ß–∏—Å–ª–æ –≥–æ—Ä–æ–¥—Å–∫–æ–≥–æ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–°–∫–æ–ª—å–∫–æ —Å–µ–ª—å—Å–∫–∏—Ö –∂–∏—Ç–µ–ª–µ–π –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º",
        # "–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞—Å–µ–ª–µ–Ω–∏—è –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–ß–∏—Å–ª–æ —Ä–æ–¥–∏–≤—à–∏—Ö—Å—è –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–ß–∏—Å–ª–æ —É–º–µ—Ä—à–∏—Ö –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–ú–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –Ω–∞—Å–µ–ª–µ–Ω–∏—è",
        # "–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ú–∏–Ω—Å–∫–∞",
        # "–ù–∞—Å–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–Ω—ã—Ö —Ü–µ–Ω—Ç—Ä–æ–≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–ß–∏—Å–ª–æ –º—É–∂—á–∏–Ω –∏ –∂–µ–Ω—â–∏–Ω –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞—Å–µ–ª–µ–Ω–∏—è",
        # "–ß–∏—Å–ª–æ –Ω–∞—Å–µ–ª–µ–Ω–∏—è —Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞",

        # –†–µ–≥–∏–æ–Ω—ã (—Ç–æ–º 1 –∏ —Ç–æ–º 2)
        # "–°–æ—Ü–∏–∞–ª—å–Ω–æ-—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–≤–∏—Ç–∏–µ –ë—Ä–µ—Å—Ç—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
        # "–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –í–∏—Ç–µ–±—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
        # "–≠–∫–æ–Ω–æ–º–∏–∫–∞ –ì–æ–º–µ–ª—å—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
        # "–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å –ì—Ä–æ–¥–Ω–µ–Ω—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
        # "–°–µ–ª—å—Å–∫–æ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ –ú–∏–Ω—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
        # "–ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ú–æ–≥–∏–ª–µ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
        # "–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ø–æ —Ä–∞–π–æ–Ω–∞–º –ú–∏–Ω—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
        # "–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º",
        # "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞–ø–∏—Ç–∞–ª –ø–æ –æ–±–ª–∞—Å—Ç—è–º",
        # "–û–±—ä–µ–º —Ä–æ–∑–Ω–∏—á–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–æ–æ–±–æ—Ä–æ—Ç–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º",
        "–£—Ä–æ–≤–µ–Ω—å –±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü—ã –ø–æ –æ–±–ª–∞—Å—Ç—è–º",
        "–°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º",
        # "–ß–∏—Å–ª–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –ø–æ –æ–±–ª–∞—Å—Ç—è–º",
        "–ß–∏—Å–ª–æ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º",

        # –≠–∫–æ–Ω–æ–º–∏–∫–∞ –∏ –º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏–∫–∞
        "–í–∞–ª–æ–≤–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç –ë–µ–ª–∞—Ä—É—Å–∏",
        "–í–í–ü –Ω–∞ –¥—É—à—É –Ω–∞—Å–µ–ª–µ–Ω–∏—è",
        # "–¢–µ–º–ø—ã —Ä–æ—Å—Ç–∞ –í–í–ü",
        # "–ò–Ω–¥–µ–∫—Å –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Ü–µ–Ω",
        "–ò–Ω—Ñ–ª—è—Ü–∏—è –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–û–±—ä–µ–º –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞",
        # "–î–æ–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –í–í–ü",
        # "–≠–∫—Å–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä–æ–≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–ò–º–ø–æ—Ä—Ç —Ç–æ–≤–∞—Ä–æ–≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–°–∞–ª—å–¥–æ –≤–Ω–µ—à–Ω–µ–π —Ç–æ—Ä–≥–æ–≤–ª–∏",
        # "–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ—Ä–≥–æ–≤—ã–µ –ø–∞—Ä—Ç–Ω–µ—Ä—ã –ë–µ–ª–∞—Ä—É—Å–∏",

        # –ë–µ–ª–∞—Ä—É—Å—å –∏ –†–æ—Å—Å–∏—è
        # "–¢–æ–≤–∞—Ä–æ–æ–±–æ—Ä–æ—Ç –º–µ–∂–¥—É –ë–µ–ª–∞—Ä—É—Å—å—é –∏ –†–æ—Å—Å–∏–µ–π",
        "–≠–∫—Å–ø–æ—Ä—Ç –ë–µ–ª–∞—Ä—É—Å–∏ –≤ –†–æ—Å—Å–∏—é",
        "–ò–º–ø–æ—Ä—Ç –∏–∑ –†–æ—Å—Å–∏–∏ –≤ –ë–µ–ª–∞—Ä—É—Å—å",
        "–î–æ–ª—è –†–æ—Å—Å–∏–∏ –≤–æ –≤–Ω–µ—à–Ω–µ–π —Ç–æ—Ä–≥–æ–≤–ª–µ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –í–í–ü –ë–µ–ª–∞—Ä—É—Å–∏ –∏ –†–æ—Å—Å–∏–∏",
        # "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ë–µ–ª–∞—Ä—É—Å–∏ –∏ –†–æ—Å—Å–∏–∏",
        # "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –∏–Ω—Ñ–ª—è—Ü–∏–∏ –ë–µ–ª–∞—Ä—É—Å—å –†–æ—Å—Å–∏—è",
        # "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–π –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –ë–µ–ª–∞—Ä—É—Å—å –†–æ—Å—Å–∏—è",

        # –°–æ—Ü–∏–∞–ª—å–Ω–∞—è —Å—Ñ–µ—Ä–∞
        "–°—Ä–µ–¥–Ω—è—è –Ω–æ–º–∏–Ω–∞–ª—å–Ω–∞—è –∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞",
        "–†–µ–∞–ª—å–Ω–∞—è –∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞ –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–î–æ—Ö–æ–¥—ã –Ω–∞—Å–µ–ª–µ–Ω–∏—è –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–†–∞—Å—Ö–æ–¥—ã –Ω–∞—Å–µ–ª–µ–Ω–∏—è",
        # "–£—Ä–æ–≤–µ–Ω—å –±–µ–¥–Ω–æ—Å—Ç–∏ –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        "–ß–∏—Å–ª–æ –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä–æ–≤",
        "–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –ø–µ–Ω—Å–∏–∏",
        # "–ß–∏—Å–ª–æ —É—á–∞—â–∏—Ö—Å—è –≤ —à–∫–æ–ª–∞—Ö",
        "–ß–∏—Å–ª–æ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –≤ –≤—É–∑–∞—Ö",
        # "–ß–∏—Å–ª–æ —É—á—Ä–µ–∂–¥–µ–Ω–∏–π –æ–±—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è",
        "–ß–∏—Å–ª–æ —É—á—Ä–µ–∂–¥–µ–Ω–∏–π –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è",
        # "–û–±–µ—Å–ø–µ—á–µ–Ω–Ω–æ—Å—Ç—å –≤—Ä–∞—á–∞–º–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è",

        # –°–µ–ª—å—Å–∫–æ–µ —Ö–æ–∑—è–π—Å—Ç–≤–æ
        "–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∑–µ—Ä–Ω–∞ –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        "–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è",
        "–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –º–æ–ª–æ–∫–∞",
        # "–ü–æ–≥–æ–ª–æ–≤—å–µ –∫—Ä—É–ø–Ω–æ–≥–æ —Ä–æ–≥–∞—Ç–æ–≥–æ —Å–∫–æ—Ç–∞",
        # "–ü–æ–≥–æ–ª–æ–≤—å–µ —Å–≤–∏–Ω–µ–π",
        # "–£—Ä–æ–∂–∞–π–Ω–æ—Å—Ç—å —Å–µ–ª—å—Å–∫–æ—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä",

        # –ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å –∏ —ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞
        "–î–æ–±—ã—á–∞ –Ω–µ—Ñ—Ç–∏ –≤ –ë–µ–ª–∞—Ä—É—Å–∏",
        "–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏–∏",
        # "–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏–∏",
        # "–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –Ω–µ—Ñ—Ç–µ–ø—Ä–æ–¥—É–∫—Ç–æ–≤",
        # "–û–±—ä–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ –º–∞—à–∏–Ω–æ—Å—Ç—Ä–æ–µ–Ω–∏—è",

        # –°—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∏ –∂–∏–ª—å–µ
        "–í–≤–æ–¥ –∂–∏–ª—å—è –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é",
        "–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∂–∏–ª—å—è",
        # "–ñ–∏–ª–∏—â–Ω—ã–π —Ñ–æ–Ω–¥ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–û–±–µ—Å–ø–µ—á–µ–Ω–Ω–æ—Å—Ç—å –∂–∏–ª—å–µ–º –Ω–∞—Å–µ–ª–µ–Ω–∏—è",

        # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç –∏ —Å–≤—è–∑—å
        "–ì—Ä—É–∑–æ–æ–±–æ—Ä–æ—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞",
        "–ü–∞—Å—Å–∞–∂–∏—Ä–æ–æ–±–æ—Ä–æ—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞",
        # "–ü–µ—Ä–µ–≤–æ–∑–∫–∏ –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–º —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–æ–º",
        # "–ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–∑–∫–∏ –≥—Ä—É–∑–æ–≤",

        # –§–∏–Ω–∞–Ω—Å—ã
        # "–î–æ—Ö–æ–¥—ã –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞",
        # "–†–∞—Å—Ö–æ–¥—ã –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞",
        "–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–æ–ª–≥ –ë–µ–ª–∞—Ä—É—Å–∏",
        # "–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞–ø–∏—Ç–∞–ª"
    ]

    if not predefined_queries_flag and not user_queries_flag:
        print("–û–±–∞ —Ä–µ–∂–∏–º–∞ –≤—ã–∫–ª—é—á–µ–Ω—ã. –ó–∞–¥–∞–π—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–µ–∂–∏–º: predefined_queries=True –∏–ª–∏ user_queries=True")
        return

    try:
        # –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        if predefined_queries_flag:
            for query in predefined_queries:
                print(f"\n{'='*20}")
                print(f"–ó–∞–ø—Ä–æ—Å: {query}")
                print('='*20)
                
                results = retriever.search(query, top_k=10, hybrid_weight=0.3)

                if not results:
                    print("[–û–®–ò–ë–ö–ê] –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n")
                    continue

                # –§–æ—Ä–º–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                answer = compose_answer(query, results, top_k=3)
                print(answer)
                
                # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                # print("\n" + format_detailed_results(query, results))
                print()

        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤
        if user_queries_flag:
            print("\n" + "="*70)
            print("–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –≤–≤–æ–¥–∏—Ç—å —Å–≤–æ–∏ –∑–∞–ø—Ä–æ—Å—ã (Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
            print("="*70 + "\n")
            
            while True:
                query = input("> ").strip()
                if not query:
                    continue

                print(f"\n{'='*20}")
                print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {query}")
                print('='*20)
                
                results = retriever.search(query, top_k=10, hybrid_weight=0.3)
                
                if not results:
                    print("[–û–®–ò–ë–ö–ê] –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n")
                    continue

                # –§–æ—Ä–º–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
                answer = compose_answer(query, results, top_k=3)
                print(answer)
                
                # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                # print("\n" + format_detailed_results(query, results))
                print()

    except KeyboardInterrupt:
        print("\n–í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")


if __name__ == "__main__":
    main()
